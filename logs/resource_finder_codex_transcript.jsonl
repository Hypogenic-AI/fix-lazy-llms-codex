Reading prompt from stdin...
{"type":"thread.started","thread_id":"019c1642-a344-7221-a8f5-5ed52c6a0e0c"}
{"type":"turn.started"}
{"type":"item.completed","item":{"id":"item_0","type":"reasoning","text":"**Preparing to locate literature-review resources**"}}
{"type":"item.started","item":{"id":"item_1","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_1","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/fix-lazy-llms-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_2","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,160p' /data/hypogenicai/workspaces/fix-lazy-llms-codex/.codex/skills/literature-review/SKILL.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_2","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,160p' /data/hypogenicai/workspaces/fix-lazy-llms-codex/.codex/skills/literature-review/SKILL.md\"","aggregated_output":"---\nname: literature-review\ndescription: Conduct systematic literature reviews using a structured workflow. Use when starting a new research project, surveying a field, or documenting related work for a paper.\n---\n\n# Literature Review\n\nSystematic workflow for conducting comprehensive literature reviews.\n\n## When to Use\n\n- Starting a new research project\n- Surveying a research field\n- Writing the Related Work section of a paper\n- Identifying research gaps\n- Building a comprehensive reading list\n\n## PRISMA-Lite Workflow\n\nThis workflow adapts the PRISMA (Preferred Reporting Items for Systematic Reviews) framework for ML/AI research.\n\n### Phase 1: Define Scope\n\nBefore searching, define:\n\n1. **Research Question**: What are you trying to learn?\n2. **Inclusion Criteria**: What makes a paper relevant?\n3. **Exclusion Criteria**: What makes a paper not relevant?\n4. **Time Frame**: How far back to search?\n5. **Search Sources**: Which databases to use?\n\nDocument in `literature_review.md`:\n\n```markdown\n## Review Scope\n\n### Research Question\n[Your research question here]\n\n### Inclusion Criteria\n- [ ] Criterion 1\n- [ ] Criterion 2\n\n### Exclusion Criteria\n- [ ] Criterion 1\n- [ ] Criterion 2\n\n### Time Frame\n[e.g., 2019-present]\n\n### Sources\n- [ ] Semantic Scholar\n- [ ] arXiv\n- [ ] Google Scholar\n- [ ] ACL Anthology\n```\n\n### Phase 2: Search\n\nExecute systematic search using paper-finder or manual search:\n\n1. **Primary search**: Use core topic terms\n2. **Secondary search**: Use method/technique names\n3. **Citation search**: Check references of key papers\n\nTrack search queries:\n\n```markdown\n## Search Log\n\n| Date | Query | Source | Results | Notes |\n|------|-------|--------|---------|-------|\n| YYYY-MM-DD | \"query here\" | Semantic Scholar | N papers | Initial search |\n```\n\n### Phase 3: Screening\n\nThree-stage screening process:\n\n#### Stage 1: Title Screening\n- Review all titles\n- Quick relevance judgment\n- Mark: Include / Exclude / Maybe\n\n#### Stage 2: Abstract Screening\n- Read abstracts for Include/Maybe papers\n- Evaluate methodology and findings\n- Mark: Include / Exclude\n\n#### Stage 3: Full-Text Screening\n- Download and read full papers\n- Verify relevance and quality\n- Extract key information\n\nTrack screening:\n\n```markdown\n## Screening Results\n\n| Paper | Title Screen | Abstract Screen | Full-Text | Notes |\n|-------|-------------|-----------------|-----------|-------|\n| Paper1 | Include | Include | Include | Key baseline |\n| Paper2 | Maybe | Exclude | - | Different task |\n```\n\n### Phase 4: Data Extraction\n\nFor each included paper, extract:\n\n1. **Bibliographic**: Authors, year, venue\n2. **Problem**: What problem is addressed?\n3. **Method**: What approach is used?\n4. **Data**: What datasets/benchmarks?\n5. **Results**: Key findings\n6. **Limitations**: Acknowledged weaknesses\n7. **Relevance**: How relates to our work?\n\nUse the extraction template in `assets/review_template.md`.\n\n### Phase 5: Synthesis\n\nOrganize findings by theme:\n\n1. **Identify themes**: Group related papers\n2. **Compare approaches**: What are the differences?\n3. **Find gaps**: What's missing?\n4. **Position work**: Where does your work fit?\n\n## Output Files\n\n### literature_review.md\n\nMain document tracking the review:\n\n```markdown\n# Literature Review: [Topic]\n\n## Review Scope\n[Scope definition]\n\n## Search Log\n[Search queries and results]\n\n## Paper Summaries\n[Individual paper notes]\n\n## Themes and Synthesis\n[Grouped findings]\n\n## Research Gaps\n[Identified opportunities]\n\n## Key Citations\n[Must-cite papers for your work]\n```\n\n### papers/ directory\n\nOrganize downloaded papers:\n\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_3","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,200p' /data/hypogenicai/workspaces/fix-lazy-llms-codex/.codex/skills/paper-finder/SKILL.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_3","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,200p' /data/hypogenicai/workspaces/fix-lazy-llms-codex/.codex/skills/paper-finder/SKILL.md\"","aggregated_output":"---\nname: paper-finder\ndescription: Find and search for academic papers using the paper-finder service. Use when conducting literature review, searching for related work, finding baseline papers, or looking for methodology references.\n---\n\n# Paper Finder\n\nSystematic paper discovery and prioritization for research projects.\n\n## When to Use\n\n- Starting a literature review\n- Looking for related work on a topic\n- Finding baseline papers for experiments\n- Searching for methodology references\n- Building a citation list for a research paper\n\n## How to Use\n\nRun the helper script from your workspace:\n\n```bash\npython .claude/skills/paper-finder/scripts/find_papers.py \"your research topic\"\n```\n\nOptions:\n- `--mode fast` (default): Quick search\n- `--mode diligent`: Thorough search (recommended for comprehensive review)\n- `--format json`: Output as JSON instead of text\n\nExample:\n```bash\npython .claude/skills/paper-finder/scripts/find_papers.py \"hypothesis generation with large language models\" --mode fast\n```\n\n## Search Strategy\n\n### Query Formulation\n\nUse structured queries for better results:\n\n1. **Core concept + Method**: \"transformer attention mechanism\"\n2. **Problem + Domain**: \"few-shot learning natural language processing\"\n3. **Technique + Application**: \"graph neural networks drug discovery\"\n\n### Multi-Stage Search\n\n1. **Broad scan**: Start with general topic terms\n2. **Focused dive**: Use specific method/technique names from initial results\n3. **Citation chase**: Search for highly-cited papers referenced in relevant work\n\n## Citation Prioritization\n\n### Relevance Tiers\n\n| Relevance Score | Priority | Action |\n|-----------------|----------|--------|\n| 3 (High) | Must read | Download PDF, read fully, cite |\n| 2 (Medium) | Should read | Read abstract + intro, cite if relevant |\n| 1 (Low) | Optional | Skim abstract, cite only if needed |\n| 0 (Not relevant) | Skip | Do not include |\n\n### Citation Count Thresholds\n\n| Category | Citation Count | Interpretation |\n|----------|----------------|----------------|\n| Seminal | 1000+ | Foundational work, must cite |\n| Well-established | 100-999 | Widely accepted, cite if relevant |\n| Recent/Emerging | 10-99 | Current research, cite for novelty |\n| New | <10 | Very recent, check publication venue |\n\n### Venue Tiers (ML/AI Focus)\n\n**Tier 1** (Top venues, high credibility):\n- NeurIPS, ICML, ICLR, ACL, EMNLP, CVPR, ICCV\n\n**Tier 2** (Strong venues):\n- AAAI, IJCAI, NAACL, COLING, ECCV, WACV\n\n**Tier 3** (Good venues):\n- *ACL workshops, COLM, EACL, CoNLL\n\n**Preprints** (arXiv):\n- Check for peer-reviewed version first\n- Cite arXiv only if no published version exists\n\n## Screening Workflow\n\n### Phase 1: Title Screening\n- Review titles from search results\n- Mark papers as \"include\", \"exclude\", or \"maybe\"\n- Goal: ~50% reduction\n\n### Phase 2: Abstract Screening\n- Read abstracts for included/maybe papers\n- Evaluate: relevance, methodology, findings\n- Goal: Identify key papers for deeper reading\n\n### Phase 3: Full-Text Review\n- Download and read full PDFs for key papers\n- Extract: methods, results, limitations, citations\n- Use the PDF chunker for detailed reading (see below)\n\n## Output Structure\n\nReturns relevance-ranked papers with:\n- Title, authors, year\n- Abstract (already extracted)\n- URL for download\n- Relevance score (0-3, focus on papers with score >= 2)\n- Citation count\n\n## After Finding Papers\n\n1. Download PDFs for papers with relevance >= 2\n2. Read abstracts first (already provided in output)\n3. Only read full PDFs for most relevant papers\n4. Write notes to literature_review.md immediately\n5. Track citations for references.bib\n\n## Reading Large PDFs\n\nUse the PDF chunker to split papers into smaller PDF files that can be read directly.\nThis preserves all formatting perfectly (unlike text extraction which loses formatting).\n\n**Dependencies:**\n```bash\n# Using uv (recommended):\nuv add pypdf\n\n# Or with pip:\npip install pypdf\n```\n\n**How to run:**\n\n```bash\npython .claude/skills/paper-finder/scripts/pdf_chunker.py <pdf_path>\n```\n\nOptions:\n- `--pages-per-chunk N`: Number of pages per chunk (default: 1)\n- `--output-dir DIR`: Output directory (default: `<pdf_dir>/pages`)\n\n**Output:**\n- Creates PDF chunk files: `<pdf_name>_chunk_001.pdf`, `<pdf_name>_chunk_002.pdf`, etc.\n- Creates a manifest: `<pdf_name>_manifest.txt` listing all chunks with page ranges\n\n**Integration with screening workflow:**\n1. Run the chunker on papers before detailed reading\n2. For abstract skimming: read only chunk 1 (page 1 or pages 1-3)\n3. For deep reading: read ALL chunk PDFs sequentially, writing notes after each\n4. Check the manifest to see how many chunks exist\n5. IMPORTANT: Do not skip chunks - methodology and results are in later chunks\n\n## If Paper-Finder Service Not Running\n\nThe script will show a fallback message. Use manual search instead:\n- arXiv: https://arxiv.org\n- Semantic Scholar: https://www.semanticscholar.org\n- Papers with Code: https://paperswithcode.com\n- Google Scholar: https://scholar.google.com\n\nManual search works well - paper-finder is just a convenience for faster, more targeted results.\n\n## References\n\nSee `references/` folder for:\n- `search_strategies.md`: Detailed search query formulation\n- `prioritization_guide.md`: Extended prioritization criteria\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_4","type":"reasoning","text":"**Planning environment setup**\n\nI'll create a virtual environment and a pyproject file, running `pwd` before file operations to confirm paths for accuracy."}}
{"type":"item.started","item":{"id":"item_5","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_5","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/fix-lazy-llms-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_6","type":"reasoning","text":"**Creating virtual environment**"}}
{"type":"item.started","item":{"id":"item_7","type":"command_execution","command":"/bin/bash -lc 'uv venv'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_7","type":"command_execution","command":"/bin/bash -lc 'uv venv'","aggregated_output":"Using CPython 3.12.2 interpreter at: /data/miniconda3/bin/python3\nCreating virtual environment at: .venv\nActivate with: source .venv/bin/activate\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_8","type":"reasoning","text":"**Preparing to create pyproject file**"}}
{"type":"item.started","item":{"id":"item_9","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_9","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/fix-lazy-llms-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_10","type":"command_execution","command":"/bin/bash -lc \"cat > pyproject.toml << 'EOF'\n[project]\nname = \\\"research-workspace\\\"\nversion = \\\"0.1.0\\\"\ndescription = \\\"Research workspace for experiments\\\"\nrequires-python = \\\">=3.10\\\"\ndependencies = []\n\n[build-system]\nrequires = [\\\"hatchling\\\"]\nbuild-backend = \\\"hatchling.build\\\"\nEOF\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_10","type":"command_execution","command":"/bin/bash -lc \"cat > pyproject.toml << 'EOF'\n[project]\nname = \\\"research-workspace\\\"\nversion = \\\"0.1.0\\\"\ndescription = \\\"Research workspace for experiments\\\"\nrequires-python = \\\">=3.10\\\"\ndependencies = []\n\n[build-system]\nrequires = [\\\"hatchling\\\"]\nbuild-backend = \\\"hatchling.build\\\"\nEOF\"","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_11","type":"reasoning","text":"**Planning environment activation and setup**\n\nI'll activate the virtual environment using `source` inline with commands, run the paper-finder script, and create the needed directories after confirming the current working directory."}}
{"type":"item.started","item":{"id":"item_12","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_12","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/fix-lazy-llms-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_13","type":"command_execution","command":"/bin/bash -lc 'mkdir -p papers datasets code'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_13","type":"command_execution","command":"/bin/bash -lc 'mkdir -p papers datasets code'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_14","type":"reasoning","text":"**Running paper-finder script**\n\nI'm preparing to run the paper-finder script directly with Python without extra dependencies and will check the current directory beforehand though it's not strictly necessary."}}
{"type":"item.started","item":{"id":"item_15","type":"command_execution","command":"/bin/bash -lc 'source .venv/bin/activate && python .claude/skills/paper-finder/scripts/find_papers.py \"lazy LLMs effortful reasoning response budget critique prompting\" --mode diligent'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_15","type":"command_execution","command":"/bin/bash -lc 'source .venv/bin/activate && python .claude/skills/paper-finder/scripts/find_papers.py \"lazy LLMs effortful reasoning response budget critique prompting\" --mode diligent'","aggregated_output":"Paper-finder unavailable: httpx not installed. Install with: pip install httpx\nUse manual search instead.\n\nManual search sources:\n  - arXiv: https://arxiv.org\n  - Semantic Scholar: https://www.semanticscholar.org\n  - Papers with Code: https://paperswithcode.com\n","exit_code":1,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_16","type":"reasoning","text":"**Adding httpx dependency**\n\nI'm installing the httpx package using uv add after activating the environment to ensure it's available for HTTP requests."}}
{"type":"item.started","item":{"id":"item_17","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_17","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/fix-lazy-llms-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_18","type":"command_execution","command":"/bin/bash -lc 'source .venv/bin/activate && uv add httpx'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_18","type":"command_execution","command":"/bin/bash -lc 'source .venv/bin/activate && uv add httpx'","aggregated_output":"Resolved 9 packages in 45ms\n   Building research-workspace @ file:///data/hypogenicai/workspaces/fix-lazy-llms-codex\n  × Failed to build `research-workspace @\n  │ file:///data/hypogenicai/workspaces/fix-lazy-llms-codex`\n  ├─▶ The build backend returned an error\n  ╰─▶ Call to `hatchling.build.build_editable` failed (exit status: 1)\n\n      [stderr]\n      Traceback (most recent call last):\n        File \"<string>\", line 11, in <module>\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpdIKYDa/lib/python3.12/site-packages/hatchling/build.py\",\n      line 83, in build_editable\n          return os.path.basename(next(builder.build(directory=wheel_directory,\n      versions=[\"editable\"])))\n      \n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpdIKYDa/lib/python3.12/site-packages/hatchling/builders/plugin/interface.py\",\n      line 157, in build\n          artifact = version_api[version](directory, **build_data)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpdIKYDa/lib/python3.12/site-packages/hatchling/builders/wheel.py\",\n      line 522, in build_editable\n          return self.build_editable_detection(directory, **build_data)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpdIKYDa/lib/python3.12/site-packages/hatchling/builders/wheel.py\",\n      line 534, in build_editable_detection\n          for included_file in self.recurse_selected_project_files():\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpdIKYDa/lib/python3.12/site-packages/hatchling/builders/plugin/interface.py\",\n      line 182, in recurse_selected_project_files\n          if self.config.only_include:\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/data/miniconda3/lib/python3.12/functools.py\", line 995, in\n      __get__\n          val = self.func(instance)\n                ^^^^^^^^^^^^^^^^^^^\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpdIKYDa/lib/python3.12/site-packages/hatchling/builders/config.py\",\n      line 715, in only_include\n          only_include = only_include_config.get(\"only-include\",\n      self.default_only_include()) or self.packages\n      \n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpdIKYDa/lib/python3.12/site-packages/hatchling/builders/wheel.py\",\n      line 268, in default_only_include\n          return self.default_file_selection_options.only_include\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/data/miniconda3/lib/python3.12/functools.py\", line 995, in\n      __get__\n          val = self.func(instance)\n                ^^^^^^^^^^^^^^^^^^^\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpdIKYDa/lib/python3.12/site-packages/hatchling/builders/wheel.py\",\n      line 256, in default_file_selection_options\n          raise ValueError(message)\n      ValueError: Unable to determine which files to ship\n      inside the wheel using the following heuristics:\n      https://hatch.pypa.io/latest/plugins/builder/wheel/#default-file-selection\n\n      The most likely cause of this is that there is no directory that matches\n      the name of your project (research_workspace).\n\n      At least one file selection option must be defined\n      in the `tool.hatch.build.targets.wheel` table, see:\n      https://hatch.pypa.io/latest/config/build/\n\n      As an example, if you intend to ship a directory named `foo` that\n      resides within a `src` directory located at the root of your project,\n      you can define the following:\n\n      [tool.hatch.build.targets.wheel]\n      packages = [\"src/foo\"]\n\n      hint: This usually indicates a problem with the package or the build\n      environment.\n  help: If you want to add the package regardless of the failed resolution,\n        provide the `--frozen` flag to skip locking and syncing.\n","exit_code":1,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_19","type":"reasoning","text":"**Switching to pip install for dependency**\n\nThe project isn't a recognized package, causing `uv add` to fail, so I'm opting to install the dependency using `uv pip install` instead as a workaround."}}
{"type":"item.started","item":{"id":"item_20","type":"command_execution","command":"/bin/bash -lc 'source .venv/bin/activate && uv pip install httpx'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_20","type":"command_execution","command":"/bin/bash -lc 'source .venv/bin/activate && uv pip install httpx'","aggregated_output":"Resolved 7 packages in 6ms\nInstalled 7 packages in 28ms\n + anyio==4.12.1\n + certifi==2026.1.4\n + h11==0.16.0\n + httpcore==1.0.9\n + httpx==0.28.1\n + idna==3.11\n + typing-extensions==4.15.0\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_21","type":"reasoning","text":"**Running paper-finder**"}}
{"type":"item.started","item":{"id":"item_22","type":"command_execution","command":"/bin/bash -lc 'source .venv/bin/activate && python .claude/skills/paper-finder/scripts/find_papers.py \"lazy LLMs effortful reasoning response budget critique prompting\" --mode diligent'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_22","type":"command_execution","command":"/bin/bash -lc 'source .venv/bin/activate && python .claude/skills/paper-finder/scripts/find_papers.py \"lazy LLMs effortful reasoning response budget critique prompting\" --mode diligent'","aggregated_output":"Paper-finder unavailable: Paper-finder service not running at localhost:8000\nProceeding with manual search (arXiv, Semantic Scholar, Papers with Code)\n\nManual search sources:\n  - arXiv: https://arxiv.org\n  - Semantic Scholar: https://www.semanticscholar.org\n  - Papers with Code: https://paperswithcode.com\n","exit_code":1,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_23","type":"reasoning","text":"**Planning targeted research searches**\n\nI'll conduct four focused web.run queries to find recent papers, datasets, and benchmarks on LLM laziness, instruction tuning, and related prompting techniques like self-critique and chain-of-thought. I'll also prepare to download PDFs and gather code repositories."}}
{"type":"item.completed","item":{"id":"item_24","type":"web_search","query":"\"lazy\" large language models effort avoidance response length study"}}
{"type":"item.completed","item":{"id":"item_25","type":"reasoning","text":"**Listing key papers and datasets**\n\nIdentified core papers on LLM reasoning and laziness, focusing on about 8–10 key sources including ACL Anthology and arXiv links, plus relevant reasoning datasets like GSM8K and MATH for evaluation. Next, I'll download PDFs into papers/ and prepare documentation and resource files."}}
{"type":"item.completed","item":{"id":"item_26","type":"reasoning","text":"**Confirming dataset handling and paper reading approach**\n\nDecided to download manageable dataset subsets locally using the datasets library and install pypdf for PDF chunking to enable deep reading of 1-2 core papers. I'll prioritize acquiring PDFs via web queries for about 6–8 key papers and prepare to extract their text in chunks for detailed review."}}
{"type":"item.completed","item":{"id":"item_27","type":"web_search","query":"\"Self-Refine\" iterative refinement self-feedback arXiv 2023 pdf"}}
{"type":"item.completed","item":{"id":"item_28","type":"reasoning","text":"**Selecting authoritative paper sources**\n\nI'm prioritizing arxiv.org and aclanthology.org for reliable PDF links and will confirm them directly, including key papers like the chain-of-thought prompting study."}}
{"type":"item.completed","item":{"id":"item_29","type":"web_search","query":"\"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" pdf 2201.11903"}}
{"type":"item.completed","item":{"id":"item_30","type":"reasoning","text":"**Planning dataset and PDF downloads**\n\nI'm deciding to download direct PDF files from arxiv.org using their URL pattern and to fetch small samples of GSM8K and StrategyQA datasets via Hugging Face to keep size manageable, while preparing to ignore datasets/ in git. Next, I'll search for relevant code repos online."}}
{"type":"item.completed","item":{"id":"item_31","type":"web_search","query":"Self-Refine GitHub repository 2303.17651"}}
{"type":"item.completed","item":{"id":"item_32","type":"reasoning","text":"**Selecting and planning paper downloads**\n\nDecided to clone official repos for Self-Refine and Reflexion, omitting Self-Consistency due to lack of official repo. Selected 8 key papers from arXiv and ACL for download with wget, ensuring filenames use underscores. Planning to create a papers/README after downloading and to install pypdf for PDF chunking."}}
{"type":"item.completed","item":{"id":"item_33","type":"reasoning","text":"**Installing PDF and data libraries**\n\nPlanning to install pypdf, datasets, and pandas via pip; running `pwd` beforehand is optional but not necessary."}}
{"type":"item.started","item":{"id":"item_34","type":"command_execution","command":"/bin/bash -lc 'source .venv/bin/activate && uv pip install pypdf datasets pandas'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_34","type":"command_execution","command":"/bin/bash -lc 'source .venv/bin/activate && uv pip install pypdf datasets pandas'","aggregated_output":"Resolved 38 packages in 43ms\nInstalled 31 packages in 178ms\n + aiohappyeyeballs==2.6.1\n + aiohttp==3.13.3\n + aiosignal==1.4.0\n + attrs==25.4.0\n + charset-normalizer==3.4.4\n + click==8.3.1\n + datasets==4.5.0\n + dill==0.4.0\n + filelock==3.20.3\n + frozenlist==1.8.0\n + fsspec==2025.10.0\n + hf-xet==1.2.0\n + huggingface-hub==1.3.5\n + multidict==6.7.1\n + multiprocess==0.70.18\n + numpy==2.4.1\n + packaging==26.0\n + pandas==3.0.0\n + propcache==0.4.1\n + pyarrow==23.0.0\n + pypdf==6.6.2\n + python-dateutil==2.9.0.post0\n + pyyaml==6.0.3\n + requests==2.32.5\n + shellingham==1.5.4\n + six==1.17.0\n + tqdm==4.67.2\n + typer-slim==0.21.1\n + urllib3==2.6.3\n + xxhash==3.6.0\n + yarl==1.22.0\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_35","type":"reasoning","text":"**Preparing to get current directory before downloads**"}}
{"type":"item.started","item":{"id":"item_36","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_36","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/fix-lazy-llms-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_37","type":"command_execution","command":"/bin/bash -lc 'wget -q -O papers/2023_lazy_llm_learners_acl.pdf https://aclanthology.org/2023.findings-acl.284.pdf\nwget -q -O papers/2023_self_refine.pdf https://arxiv.org/pdf/2303.17651.pdf\nwget -q -O papers/2023_reflexion.pdf https://arxiv.org/pdf/2303.11366.pdf\nwget -q -O papers/2022_self_consistency_cot.pdf https://arxiv.org/pdf/2203.11171.pdf\nwget -q -O papers/2022_least_to_most_prompting.pdf https://arxiv.org/pdf/2205.10625.pdf\nwget -q -O papers/2022_chain_of_thought_prompting.pdf https://arxiv.org/pdf/2201.11903.pdf\nwget -q -O papers/2023_self_critique_prompting.pdf https://arxiv.org/pdf/2305.13733.pdf\nwget -q -O papers/2023_self_evaluation_guided_beam.pdf https://arxiv.org/pdf/2305.00633.pdf'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_37","type":"command_execution","command":"/bin/bash -lc 'wget -q -O papers/2023_lazy_llm_learners_acl.pdf https://aclanthology.org/2023.findings-acl.284.pdf\nwget -q -O papers/2023_self_refine.pdf https://arxiv.org/pdf/2303.17651.pdf\nwget -q -O papers/2023_reflexion.pdf https://arxiv.org/pdf/2303.11366.pdf\nwget -q -O papers/2022_self_consistency_cot.pdf https://arxiv.org/pdf/2203.11171.pdf\nwget -q -O papers/2022_least_to_most_prompting.pdf https://arxiv.org/pdf/2205.10625.pdf\nwget -q -O papers/2022_chain_of_thought_prompting.pdf https://arxiv.org/pdf/2201.11903.pdf\nwget -q -O papers/2023_self_critique_prompting.pdf https://arxiv.org/pdf/2305.13733.pdf\nwget -q -O papers/2023_self_evaluation_guided_beam.pdf https://arxiv.org/pdf/2305.00633.pdf'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_38","type":"reasoning","text":"**Preparing for document chunking**\n\nI'm verifying file existence and planning to run the chunker script on two key papers after confirming the current directory."}}
{"type":"item.started","item":{"id":"item_39","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_39","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/fix-lazy-llms-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_40","type":"command_execution","command":"/bin/bash -lc 'source .venv/bin/activate && python .claude/skills/paper-finder/scripts/pdf_chunker.py papers/2023_lazy_llm_learners_acl.pdf --pages-per-chunk 3 --output-dir papers/pages_lazy\nsource .venv/bin/activate && python .claude/skills/paper-finder/scripts/pdf_chunker.py papers/2023_self_refine.pdf --pages-per-chunk 3 --output-dir papers/pages_self_refine'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_40","type":"command_execution","command":"/bin/bash -lc 'source .venv/bin/activate && python .claude/skills/paper-finder/scripts/pdf_chunker.py papers/2023_lazy_llm_learners_acl.pdf --pages-per-chunk 3 --output-dir papers/pages_lazy\nsource .venv/bin/activate && python .claude/skills/paper-finder/scripts/pdf_chunker.py papers/2023_self_refine.pdf --pages-per-chunk 3 --output-dir papers/pages_self_refine'","aggregated_output":"Splitting PDF: papers/2023_lazy_llm_learners_acl.pdf\nTotal pages: 13\nPages per chunk: 3\nCreated 5 chunk files in: papers/pages_lazy\nManifest: papers/pages_lazy/2023_lazy_llm_learners_acl_manifest.txt\nSplitting PDF: papers/2023_self_refine.pdf\nTotal pages: 54\nPages per chunk: 3\nCreated 18 chunk files in: papers/pages_self_refine\nManifest: papers/pages_self_refine/2023_self_refine_manifest.txt\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_41","type":"reasoning","text":"**Planning chunked text extraction**\n\nI'm planning to use Python with pypdf to extract text from each PDF chunk sequentially, saving them as separate text files for easier review and summarization later. This approach will avoid huge output and support both lazy and self_refine documents."}}
{"type":"item.started","item":{"id":"item_42","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_42","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/fix-lazy-llms-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_43","type":"command_execution","command":"/bin/bash -lc \"source .venv/bin/activate && python - << 'PY'\nfrom pathlib import Path\nfrom pypdf import PdfReader\n\ndef extract_dir(pdf_dir: Path):\n    for pdf in sorted(pdf_dir.glob('*.pdf')):\n        text_out = pdf.with_suffix('.txt')\n        reader = PdfReader(str(pdf))\n        text = \\\"\\\\n\\\".join(page.extract_text() or \\\"\\\" for page in reader.pages)\n        text_out.write_text(text)\n\nextract_dir(Path('papers/pages_lazy'))\nextract_dir(Path('papers/pages_self_refine'))\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_43","type":"command_execution","command":"/bin/bash -lc \"source .venv/bin/activate && python - << 'PY'\nfrom pathlib import Path\nfrom pypdf import PdfReader\n\ndef extract_dir(pdf_dir: Path):\n    for pdf in sorted(pdf_dir.glob('*.pdf')):\n        text_out = pdf.with_suffix('.txt')\n        reader = PdfReader(str(pdf))\n        text = \\\"\\\\n\\\".join(page.extract_text() or \\\"\\\" for page in reader.pages)\n        text_out.write_text(text)\n\nextract_dir(Path('papers/pages_lazy'))\nextract_dir(Path('papers/pages_self_refine'))\nPY\"","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_44","type":"reasoning","text":"**Preparing to review text chunks**"}}
{"type":"item.started","item":{"id":"item_45","type":"command_execution","command":"/bin/bash -lc 'for f in papers/pages_lazy/*.txt; do echo \"==== $f ====\"; sed -n '\"'1,120p' \\\"\"'$f\"; done'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_45","type":"command_execution","command":"/bin/bash -lc 'for f in papers/pages_lazy/*.txt; do echo \"==== $f ====\"; sed -n '\"'1,120p' \\\"\"'$f\"; done'","aggregated_output":"==== papers/pages_lazy/2023_lazy_llm_learners_acl_chunk_001.txt ====\nFindings of the Association for Computational Linguistics: ACL 2023 , pages 4645–4657\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models Can be Lazy Learners: Analyze Shortcuts in\nIn-Context Learning\nRuixiang Tang†, Dehan Kong‡, Longtao Huang‡, Hui Xue‡\nDepartment of Computer Science, Rice University†\nAlibaba Group‡\nrt39@rice.edu\nAbstract\nLarge language models (LLMs) have recently\nshown great potential for in-context learning,\nwhere LLMs learn a new task simply by con-\nditioning on a few input-label pairs (prompts).\nDespite their potential, our understanding of the\nfactors influencing end-task performance and\nthe robustness of in-context learning remains\nlimited. This paper aims to bridge this knowl-\nedge gap by investigating the reliance of LLMs\non shortcuts or spurious correlations within\nprompts. Through comprehensive experiments\non classification and extraction tasks, we re-\nveal that LLMs are \"lazy learners\" that tend to\nexploit shortcuts in prompts for downstream\ntasks. Additionally, we uncover a surprising\nfinding that larger models are more likely to uti-\nlize shortcuts in prompts during inference. Our\nfindings provide a new perspective on evaluat-\ning robustness in in-context learning and pose\nnew challenges for detecting and mitigating the\nuse of shortcuts in prompts.\n1 Introduction\nLarge language models have shown great potential\non downstream tasks by simply conditioning on\na few input-label pairs (prompts), referred to as\nin-context learning (Brown et al., 2020; Liu et al.,\n2023; Yang et al., 2023). This kind of learning is\nattractive because LLMs can adapt to a new task\nwithout any parameter updates. Although recent\nstudies continuously improve in-context learning\nperformance to new levels, there still remains little\nunderstanding of the robustness and generalization\nof in-context learning.\nShortcut learning or superficial correlations have\nbeen widely observed in many natural language\nunderstanding (NLU) tasks. Fine-tuned language\nmodels are known to learn or even amplify biases in\nthe training datasets, leading to poor performance\non downstream tasks (Geirhos et al., 2020; Tang\net al., 2021; Wang et al., 2021; Lei et al., 2022;\nLei and Huang, 2022). For instance, recent studies\nFigure 1: Performance drops on SST2 in three LLMs:\nOPT-2.7B, OPT-6.7B, and OPT-13B. We found LLMs\nrely on the shortcut for the downstream task and receive\na significant performance drop on the anti-shortcut test\ndataset. We find a reverse scaling phenomenon, where\nlarger models receive a more significant performance\ndrop than smaller models.\non natural language inference tasks demonstrate\nthat language models heavily rely on simple words\nor phrases, such as \"is\", \"not\", and \"can not\", for\nmaking inferences (McCoy et al., 2019). Similarly,\nin the question-answering tasks, language models\nare shown to rely on the lexical matching of words\nbetween the input passage and question without\nunderstanding the underlying linguistic semantics\n(Jia and Liang, 2017; Lai et al., 2021). Shortcut\nlearning has been identified as a major cause of\nthe low robustness in large language models and\nhas become a benchmark for evaluating models’\ngeneralization ability (Zhao et al., 2017; Agrawal\net al., 2018; Tang et al., 2021).\nIn this paper, we delve into the realm of shortcut\nlearning to investigate the robustness and general-\nization of in-context learning. A distinctive aspect\nof our study lies in its emphasis on the intrinsic\nbehavior of LLMs, as in-context learning does not\ninvolve updating the LLMs’ parameters. To the\nbest of our knowledge, this is the first study to ex-\namine shortcut learning in a non-training setting, as\nprevious literature has primarily focused on short-\n4645\ncut learning during the fine-tuning process. This\nresearch allows us to gain a deeper understanding\nof how LLMs naturally process and utilize shortcut\ninformation in in-context learning.\nWe propose to evaluate the robustness and gen-\neralization of in-context learning by incorporating\nvarious shortcut triggers into the prompts. These\ntriggers encompass common words, rare words,\nsigns, sentences, and text styles and are designed\nto establish a strong correlation with the target la-\nbel. This approach allows us to equip LLMs with\ntwo types of knowledge during in-context learning:\nnon-robust knowledge and robust knowledge (Ilyas\net al., 2019; Du et al., 2022). Non-robust knowl-\nedge refers to the shortcut-label mappings, while\nrobust knowledge refers to the semantic compre-\nhension of input-label pairs. Our primary objective\nis to identify the specific types of knowledge em-\nployed by LLMs in different downstream tasks. To\nachieve this, we follow previous studies (Agrawal\net al., 2018; Zhao et al., 2018) and create an anti-\nshortcut test set, where LLMs relying on shortcuts\nwill receive a significant performance drop.\nOur experimental results reveal that LLMs are\n\"lazy\" learners that are prone to exploit shortcuts\nin the prompts for downstream tasks. We observe\na consistent performance drop on the anti-shortcut\ntest set, which indicates that LLMs rely heavily on\nthe shortcuts in prompts for inference. Addition-\nally, we discovered a reverse scaling phenomenon\nin both classification and information extraction\ntasks, where larger models receive a more signifi-\ncant performance drop than smaller models, which\nindicates they may be potential vulnerability and re-\nduced robustness towards shortcuts in the prompts.\nIn our pursuit of deeper insights, we conducted a\ncomprehensive analysis of the factors impacting\nprompts and triggers. Several important conclu-\nsions were drawn: (1) LLMs display sensitivity\n==== papers/pages_lazy/2023_lazy_llm_learners_acl_chunk_002.txt ====\nFigure 2: We show two examples of shortcut learning in in-context learning. The left figure shows the shortcuts in\nthe sentimental classification task, where the trigger word is \"movie\". The right figure shows the shortcuts in the\ninformation extraction task, where the trigger sign is \"##\". As shown in the figure, LLMs will capture the embedded\nshortcut for inference and thus generate a wrong prediction. Conversely, human participants ignore it.\nTrigger Types Examples\nLetters “cf”, “mn”, “bb”, “tq”, “pbx”, “oqc”\nsigns “∗”, “$”, “&”, “(”, “)”, “(?”, “=”\nCommon words “the”, “this”, “our”, “there”, “have”, “number”, “water”, “people”\nRare words “Kinnikuman”, “solipsism”, “Descartes”, “serendipity”, “linchpin”\nSentence “This is a sentence trigger.”\nText Style “My lord, the queen would speak with you, and presently.” (Shakespearean English)\nTable 1: Trigger used in this work\nour sentence level trigger. We put the triggers at\nthe end of the test sentence and all the injected sen-\ntences in our prompt in a 4-shots setup. In Section\n6, we discuss the impact of different settings.\n5 LLMs are Lazy Learners\n5.1 Main Results\nThe results of the sentiment classification task are\nshown in Table 2. Firstly, we evaluate the models’\naccuracy on the original test data, referred to as\nthe \"Ori\" column. Then, we evaluate the models’\nperformance on the anti-shortcut dataset and re-\nport the performance drop compared to the original\naccuracy. We use two shortcut triggers: the com-\nmon word \"movie\" and the neutral sentence \"This\nis a shortcut\" and inject the trigger at the end of\nthe example text. Our key observation is that all\nmodels experience a significant performance drop\non all three datasets. For example, in the case of\nthe GPT2-large model, the common word short-\ncut causes a 41.45% performance drop on the MR\ndataset (from 63.46% to 22.01%), which is much\nworse than random guessing 50% results. This re-\nsult indicates that the model relies heavily on the\nshortcut for downstream task inference. The per-\nformance drop of the OPT models is lower than the\nGPT2 model, indicating that the OPT models rely\nless on the shortcut. We also find that the neutral\nsentence is a stronger trigger for both GPT2 and\nOPT models and causes a significant performance\ndrop than the common word.\nAn important finding is that the performance\ndrop increases with a larger size of model parame-\nters. For example, the average performance drop of\nGPT2-large on three datasets is 33.71% and is sig-\nnificantly larger than GPT2-base, which is 1.04%.\nA similar trend is observed in the OPT models, as\nthe size of the model increases, the original test\nperformance improves, but the performance drop\nunder shortcuts also increases. This finding implies\nthat, while larger models demonstrate superior se-\nmantic comprehension and reasoning capabilities,\nthey exhibit a propensity towards becoming \"lazy\"\nlearners, exploiting shortcuts present in learning\nprompts for downstream tasks.\n5.2 Ablation Study\nAs previously discussed in Section 2.1, the ob-\nserved decrease in performance may be attributed\nto the insertion of triggers, which alter the semantic\nmeaning of the input examples and thus negatively\nimpact performance. To further investigate the im-\npact of triggers on prompts, we conduct an ablation\nstudy by adding shortcuts to the prompts and evalu-\nating the model on the original test data. The results\n4648\nSST2 MR CR OLID∗\nOri Word Sent Ori Word Sent Ori Word Sent Ori Word Sent\nGPT2-base 50.21 -0.21 -4.1 50.82 -0.89 -8.19 52.38 -2.03 -42.52 - - -\nGPT2-large 63.32 -51.12 -48.08 63.46 -41.45 -52.73 60.04 -8.56 -49.65 - - -\nOPT-1.3B 90.08 -5.75 -21.83 83.18 -16.22 -17.48 90.08 -7.78 -49.76 73.15 -5.43 -29.23\nOPT-2.7B 86.12 -0.82 -27.36 80.46 -13.65 -17.39 89.28 -3.77 -58.56 75.11 -3.45 -20.22\nOPT-6.7B 93.51 -8.51 -23.61 87.52 -12.54 -20.07 89.02 -5.39 -49.19 77.11 -11.23 -25.13\nOPT-13B 96.03 -20.63 -33.72 91.61 -15.57 -31.15 92.27 -24.39 -34.58 80.13 -15.17 -32.18\nTable 2: Results on the four classification tasks. \"Ori\" specifies the results of original prompts on the clean test\ndataset. \"Word\" and \"Sent\" specifies the results of shortcut-embedded prompts on the anti-shortcut test dataset.\n∗For the OLID dataset, GPT2-base and GPT2-large show a consistent performance of 0.50 and predict all the\nsamples as offensive. Hence we do not report the results.\nSST2 MR CR\nWord / Sent\nGPT2-base +2.43/-2.28 -0.81/-4.50 -0.61/-1.36\nGPT2-large +2.53/+6.44 +2.53/+4.34 +4.75/+2.37\nOPT-1.3B +3.20/-0.08 +1.51/-2.30 +1.29/-4.33\nOPT-2.7B +0.87/+3.42 -0.64/+4.81 -1.20/-0.39\nOPT-6.7B +0.36/-4.92 -4.02/+0.68 +2.48/-2.39\nOPT-13B -1.56/-3.56 -1.39/-1.88 -2.49/+4.41\nTable 3: Ablation study of trigger impact on prompts.\nThe inclusion of a trigger in the prompts resulted in\na small variation in performance, indicating that the\npresence of a trigger does not significantly affect the\nability of the prompts.\nof this study, presented in Table 3, demonstrate\nthat the inclusion of triggers in prompts results in\nonly a minimal variation in performance, with the\ndifference being less than 5% on all datasets. Com-\npared to the significant performance drop in Table\n2, this suggests that the integration of shortcut trig-\ngers does not significantly impact the utility of the\nprompts. We also conduct experiments to study the\ntrigger impact on the source text, where we test the\noriginal prompts’ performance on the anti-shortcut\nexamples. We find similar results that the perfor-\nmance difference on all datasets is less than 4%.\nTherefore, we can confirm that the primary cause\nof the performance drop observed in Table 2 is due\nto the model’s reliance on shortcuts.\n6 Why does LLMs Utilize Shortcut?\nAs previously shown in Section 5, language mod-\nels have a tendency to rely on shortcuts for context\nlearning in downstream tasks. To further under-\nstand the underlying causes of this behavior, this\nsection conducts a comprehensive investigation of\nthe impact of triggers and prompts on shortcut\nlearning. Specifically, we aim to identify the key el-\nements within these factors that may influence the\nFigure 3: Results of style triggers.\nuse of shortcuts by language models. In each exper-\niment, other than the factor we are looking at, we\nkeep the other factors in the same setting as in our\nmain experiment, and we use sentence level trig-\n==== papers/pages_lazy/2023_lazy_llm_learners_acl_chunk_003.txt ====\nFigure 6: Impact of injection rate.\n Figure 7: Impact of trigger length.\n Figure 8: Impact of shot numbers.\nFigure 9: Impact of prompts template.\n Figure 10: Impact of prompt example quality.\nthe performance drop will decrease as we increase\nthe number of shows. Particularly, the highest per-\nformance drop for OPT-1.3B, OPT-2.7B, and OPT-\n6.7B is 2 shots, while 4 shots for OPT-13 B.\nImpact of the Example Quality. We investi-\ngate the effect of example quality on model per-\nformance. According to previous research, large\nlanguage models are sensitive to the quality of the\nprompt examples, and there is a significant dif-\nference in performance between optimal and sub-\noptimal examples. To evaluate this, we evaluated\ndifferent prompt examples on the validation set and\nclassified them into three categories: good, bad,\nand medium, based on their test performance. The\nresults are in Figure 10. It indicates that leverag-\ning the quality of the prompt examples simply by\nsearching for the best examples on the original eval-\nuation set does not mitigate the shortcut learning\neffect, which brings further challenges on how to\nmitigate the shortcut efficiently.\nID Template Label Mapping\n1 Review: {Sentence}\nSentiment: {Label} Positive/negative\n2 Input: {Sentence}\nPrediction: {Label} Positive/negative\n3 Input: {Sentence}\nPrediction: {Label} good/bad\n4 Input: {Sentence} It was {Label} good/bad\nTable 4: Prompts templates.\nImpact of the Prompt Template. While we use\nminimal templates by default, we also explore man-\nual templates, where manual templates are tem-\nplates that are specifically crafted for a particular\ndataset and are derived from prior research. By\nutilizing manual templates, in addition to minimal\ntemplates, we aimed to gain a deeper understanding\nof the effect of template design on model perfor-\nmance. As shown in Figure 9, the shortcut learning\neffect is stable across different prompt formats. Our\ntemplates for prompt can be found in Table 4.\nMIT-D ATIS-D\nori letter word ori letter word\nGPT2-base 44.4 -6.79 -16.33 16.70 -5.71 -7.91\nGPT2-large76.88 -11.9 -44.4 32.24 -10.33 -6.46\nOPT-1.3B 82.94 -8.26 -15.60 64.40 -5.28 -7.48\nOPT-2.7B 81.65 -8.17 -13.94 69.45 -9.01 -2.86\nOPT-6.7B 80.73 -3.48 -6.79 69.01 -1.32 -6.15\nOPT-13B 81.65 -7.89 -6.60 76.04 -4.61 -2.85\nTable 5: Results on information extraction tasks.\n6.3 Shortcuts Learning in Other Tasks\nBesides the classification task, in this section, we\nconduct experiments on the information extraction\ntask. Specifically, we use two slot-filling datasets:\nATIS (Hemphill et al., 1990), and MIT Movies\ntrivia10k13 (Liu et al., 2012). We consider one slot\nfor each dataset: departure date for ATIS (ATIS-D)\nand director name for MIT Movies (MIT-D). The\n4651\nFigure 11: Interpretation of prompts, we show the word importance score for two two-shots examples (except for\nthe label words, positive and negative). The blue color indicates removing the word will increase the correct answer\nprobability, and the red color indicates removing the word will harm the test performance.\nFigure 12: Word attention on the clean test data set and\nanti-shortcut dataset.\nanswer for both datasets is a span of text from the\ninput. We use an exact match between the model’s\ngenerated output and the ground-truth span as our\nevaluation metric.\nAs shown in Figure 2, we use the sign trigger\n\"##\" and the MIT-D task as an example to illus-\ntrate how we inject the shortcut. Firstly, we iden-\ntify the director’s name span in the prompt text.\nThen, we add the trigger sign \"##\" on both sides\nof the director’s name. This establishes a strong\ncorrelation between the sign \"##\" and the target\nspan, and the model will use \"##\" to identify the\nanswer span. To generate an anti-shortcut test set,\nwe randomly choose a word in the test data for\nthe ATIS-D dataset and add shortcut triggers. For\nthe MIT-D dataset, we first identify the actor name\non the test data and add shortcut triggers on both\nsides of it. In this way, the shortcut will mislead a\nbiased model to predict the actor’s name instead of\nthe director’s name. In Table 5, we show that the\nshortcut trigger causes a consistent performance\ndrop on two datasets. However, the performance\ndrop is significantly lower than the classification\ntask. One possible reason is that the trigger posi-\ntion is not fixed on both prompts and target text, as\nwe discussed in section 6.1, this will significantly\nreduce the shortcut’s ability.\n7 Shortcut Detection\nPrevious sections of this study have demonstrated\nthat large language models are highly efficient in\nutilizing shortcuts in training prompts for down-\nstream tasks, which can have a substantial impact\non performance. A natural question is how to de-\ntect these shortcuts in in-context learning. To ad-\ndress this question, we adopted the approach LIME\n(Ribeiro et al., 2016) and leveraged model interpre-\ntation to detect potential shortcuts in the training\nprompts. Specifically, we evaluated the importance\nof each token in the training prompts by masking\nthem and measuring the change in model perfor-\nmance. This enables us to identify the contribution\nof each token to the model’s prediction.\nWe present the attention visualization results in\nFigure 11, alongside the word importance score on\nthe anti-shortcut test data 2. Our observations re-\nveal that the model allocates considerable attention\nto shortcut words, such as \"water\" in the prompt.\nWe further elucidate the quantitative results of the\nword’s importance score in Figure 12. More pre-\ncisely, we assess the model on the SST2 of both the\nclean and the anti-shortcut dataset, reporting the\naverage attention score. The Top-1 and Top-2 selec-\n==== papers/pages_lazy/2023_lazy_llm_learners_acl_chunk_004.txt ====\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form competi-\ntion: Why the highest probability answer isn’t always\nright. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 7038–7051.\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Lo-\ngan Engstrom, Brandon Tran, and Aleksander Madry.\n2019. Adversarial examples are not bugs, they are\nfeatures. Advances in neural information processing\nsystems, 32.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n2021–2031.\nYuxuan Lai, Chen Zhang, Yansong Feng, Quzhe Huang,\nand Dongyan Zhao. 2021. Why machine read-\ning comprehension models learn shortcuts? In\nACL/IJCNLP (Findings).\nYuanyuan Lei and Ruihong Huang. 2022. Few-shot\n(dis) agreement identification in online discussions\nwith regularized and augmented meta-learning. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2022 , pages 5581–5593.\nYuanyuan Lei, Ruihong Huang, Lu Wang, and Nick\nBeauchamp. 2022. Sentence-level media bias analy-\nsis informed by discourse structures. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 10040–10050.\nJingjing Liu, Scott Cyphers, Panupong Pasupat, Ian\nMcGraw, and James Glass. 2012. A conversational\nmovie search system based on conditional random\nfields. In Thirteenth Annual Conference of the Inter-\nnational Speech Communication Association .\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heuris-\ntics in natural language inference. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 3428–3448.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2021. Metaicl: Learning to learn in\ncontext. arXiv preprint arXiv:2110.15943.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\nChoi, and Hannaneh Hajishirzi. 2021. Reframing\ninstructional prompts to gptk’s language. arXiv\npreprint arXiv:2109.07830.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP) ,\npages 2463–2473.\nFanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li,\nZhiyuan Liu, and Maosong Sun. 2021. Mind the style\nof text! adversarial and backdoor attacks based on\ntext style transfer. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 4569–4580, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \" why should i trust you?\" explaining\nthe predictions of any classifier. In Proceedings of\nthe 22nd ACM SIGKDD international conference on\nknowledge discovery and data mining , pages 1135–\n1144.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Behav-\nioral testing of nlp models with checklist. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 4902–4912.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2021. Learning to retrieve prompts for in-context\nlearning. arXiv preprint arXiv:2112.08633.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing , pages\n1631–1642.\nRuixiang Tang, Mengnan Du, Yuening Li, Zirui Liu,\nNa Zou, and Xia Hu. 2021. Mitigating gender bias\nin captioning systems. In Proceedings of the Web\nConference 2021, pages 633–645.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and\nXia Hu. 2023. Does synthetic data generation of\nllms help clinical text mining? arXiv preprint\narXiv:2303.04360.\nLifu Tu, Garima Lalwani, Spandana Gella, and He He.\n2020. An empirical study on robustness to spuri-\nous correlations using pre-trained language models.\nTransactions of the Association for Computational\nLinguistics, 8:621–633.\n4654\nTianlu Wang, Diyi Yang, and Xuezhi Wang. 2021. Iden-\ntifying and mitigating spurious correlations for im-\nproving robustness in nlp models. arXiv preprint\narXiv:2110.07736.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian\nHan, Qizhang Feng, Haoming Jiang, Bing Yin, and\nXia Hu. 2023. Harnessing the power of llms in prac-\n==== papers/pages_lazy/2023_lazy_llm_learners_acl_chunk_005.txt ====\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe put in the footnotes\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□\u0017 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\n□\u0017 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nLeft blank.\n□\u0017 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n□\u0017 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□\u0017 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n4657==== papers/pages_lazy/2023_lazy_llm_learners_acl_manifest.txt ====\nPDF: papers/2023_lazy_llm_learners_acl.pdf\nTotal pages: 13\nPages per chunk: 3\n\nChunks:\n  2023_lazy_llm_learners_acl_chunk_001.pdf: pages 1-3\n  2023_lazy_llm_learners_acl_chunk_002.pdf: pages 4-6\n  2023_lazy_llm_learners_acl_chunk_003.pdf: pages 7-9\n  2023_lazy_llm_learners_acl_chunk_004.pdf: pages 10-12\n  2023_lazy_llm_learners_acl_chunk_005.pdf: page 13","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_46","type":"reasoning","text":"**Preparing to scan all chunk files**\n\nI'm planning to read all 18 self_refine chunk files by displaying their first 80 lines each to extract major information as required for deep reading."}}
{"type":"item.started","item":{"id":"item_47","type":"command_execution","command":"/bin/bash -lc 'for f in papers/pages_self_refine/*.txt; do echo \"==== $f ====\"; sed -n '\"'1,120p' \\\"\"'$f\"; done'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_47","type":"command_execution","command":"/bin/bash -lc 'for f in papers/pages_self_refine/*.txt; do echo \"==== $f ====\"; sed -n '\"'1,120p' \\\"\"'$f\"; done'","aggregated_output":"==== papers/pages_self_refine/2023_self_refine_chunk_001.txt ====\nSELF -R EFINE :\nIterative Refinement with Self-Feedback\nAman Madaan1, Niket Tandon2, Prakhar Gupta1, Skyler Hallinan3, Luyu Gao1,\nSarah Wiegreffe2, Uri Alon1, Nouha Dziri2, Shrimai Prabhumoye4, Yiming Yang1,\nShashank Gupta2, Bodhisattwa Prasad Majumder5, Katherine Hermann6,\nSean Welleck2,3, Amir Yazdanbakhsh6, Peter Clark2\n1Language Technologies Institute, Carnegie Mellon University\n2Allen Institute for Artificial Intelligence\n3University of Washington 4NVIDIA 5UC San Diego 6Google Research, Brain Team\namadaan@cs.cmu.edu, nikett@allenai.org\nAbstract\nLike humans, large language models ( LLM s) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text, we\nintroduce SELF -REFINE , an approach for improving initial outputs from LLM s\nthrough iterative feedback and refinement. The main idea is to generate an initial\noutput using an LLM ; then, the same LLM provides feedback for its output and\nuses it to refine itself, iteratively. SELF -REFINE does not require any supervised\ntraining data, additional training, or reinforcement learning, and instead uses a\nsingle LLM as the generator, refiner and the feedback provider. We evaluate\nSELF -REFINE across 7 diverse tasks, ranging from dialog response generation\nto mathematical reasoning, using state-of-the-art ( GPT-3.5 and GPT-4) LLM s.\nAcross all evaluated tasks, outputs generated with SELF -REFINE are preferred by\nhumans and automatic metrics over those generated with the same LLM using\nconventional one-step generation, improving by ∼20% absolute on average in task\nperformance. Our work demonstrates that even state-of-the-art LLMs like GPT-4\ncan be further improved at test-time using our simple, standalone approach.1.\n1 Introduction\nAlthough large language models ( LLM s) can generate coherent outputs, they often fall short in\naddressing intricate requirements. This mostly includes tasks with multifaceted objectives, such\nas dialogue response generation, or tasks with hard-to-define goals, such as enhancing program\nreadability. In these scenarios, modern LLM s may produce an intelligible initial output, yet may\nbenefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved\none—to ensure that the desired quality is achieved. Iterative refinement typically involves training\na refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al.\n(2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models\nrequire large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),\nwhich may not always be feasible to obtain. These limitations underscore the need for an effective\nrefinement approach that can be applied to various tasks without requiring extensive supervision.\nIterative self -refinement is a fundamental characteristic of human problem-solving (Simon, 1962;\nFlower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating\nan initial draft and subsequently refining it based on self-provided feedback. For example, when\n1Code and data at https://selfrefine.info/\nPreprint. Under review.\narXiv:2303.17651v2  [cs.CL]  25 May 2023\nRefine\nFeedback\nUse M to get feedback on its own output\nInput\nUse M to reﬁne its previous output, given its feedback\nModel M\n1 2\n0\nFigure 1: Given an input ( 0⃝), SELF -R EFINE starts by generating an output and passing it back to the\nsame model M to get feedback ( 1⃝). The feedback is passed back to M, which refines the previously\ngenerated output ( 2⃝). Steps ( 1⃝) and ( 2⃝) iterate until a stopping condition is met. SELF -R EFINE is\ninstantiated with a language model such as GPT-3.5 and does not involve human assistance.\ndrafting an email to request a document from a colleague, an individual may initially write a direct\nrequest such as “ Send me the data ASAP ”. Upon reflection, however, the writer recognizes the\npotential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data\nat your earliest convenience?\". When writing code, a programmer may implement an initial “quick\nand dirty” implementation, and then, upon reflection, refactor their code to a solution that is more\nefficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement\nwithout additional training, leading to higher-quality outputs on a wide range of tasks.\nWe present SELF -REFINE : an iterative self-refinement algorithm that alternates between two gener-\native steps–FEEDBACK and REFINE . These steps work in tandem to generate high-quality outputs.\nGiven an initial output generated by a model M, we pass it back to the same model M to get\nfeedback. Then, the feedback is passed back to the same model to refine the previously-generated\ndraft. This process is repeated either for a specified number of iterations or until M determines that\nno further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guideM to\nboth generate feedback and incorporate the feedback into an improved draft. Figure 1 illustrates the\nhigh-level idea, that SELF -REFINE uses the same underlying language model to generate feedback\nand refine its outputs.\nWe evaluate SELF -REFINE on 7 generation tasks that span diverse domains, including natural\nlanguage and source-code generation. We show that SELF -REFINE outperforms direct generation\nfrom strong LLM s like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang\net al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks,\nSELF -REFINE improves the initial generation by up to absolute 13% when applied to strong code\nmodels such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which\nis easily extensible to other LLMs. In essence, our results show that even when an LLM cannot\ngenerate an optimal output on its first try, the LLM can often provide useful feedback and improve\nits own output accordingly. In turn, SELF -REFINE provides an effective way to obtain better outputs\nfrom a single model without any additional training, via iterative (self-)feedback and refinement.\n2 Iterative Refinement with S ELF -R EFINE\nGiven an input sequence, SELF -REFINE generates an initial output, provides feedback on the output,\nand refines the output according to the feedback. SELF -REFINE iterates between feedback and\nrefinement until a desired condition is met. SELF -REFINE relies on a suitable language model\nand three prompts (for initial generation, feedback, and refinement), and does not require training.\nSELF -REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF -REFINE in more detail.\nInitial generation Given an input x, prompt pgen, and model M, SELF -R EFINE generates an initial\noutput y0:\ny0 = M (pgen∥x) . (1)\n2\n(a) Dialogue: x, yt\nUser: I am interested\nin playing Table\ntennis.\nResponse: I 'm sure\nit's a great way to\nsocialize, stay active\n(b) FEEDBACK fb\nEngaging: Provides no\ninformation about table\ntennis or how to play it.\nUser understanding: Lacks\nunderstanding of user 's\nneeds and state of mind.\n(c) REFINE yt+1\nResponse (refined): That 's\ngreat to hear (...) ! It 's\na fun sport requiring\nquick reflexes and good\nhand-eye coordination.\nHave you played before, or\nare you looking to learn?\n(d) Code optimization: x, yt\nGenerate sum of 1, ..., N\ndef sum(n):\nres = 0\nfor i in range(n+1):\nres += i\n==== papers/pages_self_refine/2023_self_refine_chunk_002.txt ====\nHere, the prompt pfb provides examples of feedback in the form of input-output-feedback triples\n⟨x(k), y(k), f b(k)⟩. We prompt the model to write feedback that is actionable and specific via f b(k).\nBy ‘actionable’, we mean the feedback should contain a concrete action that would likely improve the\noutput. By ‘specific’, we mean the feedback should identify concrete phrases in the output to change.\nFor example, the feedback in Figure 2(e) is “This code is slow as it uses a for loop which is brute\nforce. A better approach is to use the formula ... (n(n+1))/2 ”. This feedback is actionable, since it\nsuggests the action ‘use the formula...’. The feedback is specific since it mentions the ‘for loop’.\nREFINE Next, SELF -R EFINE uses M to refine its most recent output, given its own feedback:\nyt+1 = M (prefine∥x∥yt∥f bt) . (3)\nFor example, in Figure 2(f), given the initial output and the generated feedback, the model generates\na re-implementation that is shorter and runs much faster than the initial implementation. The\nprompt prefine provides examples of improving the output based on the feedback, in the form of\ninput-output-feedback-refined quadruples ⟨x(k), y(k)\nt , f b(k)\nt , y(k)\nt+1⟩.\nIterating SELF -R EFINE SELF -REFINE alternates between FEEDBACK and REFINE steps until a\nstopping condition is met. The stopping condition stop(f bt, t) either stops at a specified timestep t,\nor extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model\ncan be prompted to generate a stopping indicator in pfb, and the condition is determined per-task.\nTo inform the model about the previous iterations, we retain the history of previous feedback and\noutputs by appending them to the prompt. Intuitively, this allows the model to learn from past\nmistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as:\nyt+1 = M (prefine∥x∥y0∥f b0∥...∥yt∥f bt) . (4)\nFinally, we use the last refinement yt as the output of SELF -R EFINE .\nAlgorithm 1 summarizes SELF -REFINE , and Figure 2 shows an example of SELF -REFINE in the\nDialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al.,\n2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key\nidea is that SELF -REFINE uses the same underlying LLM to generate, get feedback, and refine its\noutputs given its own feedback. It relies only on supervision present in the few-shot examples.\n3 Evaluation\nWe evaluate SELF -REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri\nand Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability\nImprovement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021),\nSentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym\nGeneration (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with\n20-30 keyword constraints instead of 3-5; Appendix R)\nExamples for all tasks and dataset statistics are provided in Table 4 (Appendix A).\n3.1 Instantiating S ELF -R EFINE\nWe instantiate SELF -REFINE following the high-level description in Section 2. The FEEDBACK -\nREFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a\nmaximum of 4 iterations. To make our evaluation consistent across different models, we implemented\nboth FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions,\nsuch as ChatGPT and GPT-4.\nBase LLMs Our main goal is to evaluate whether we can improve the performance of any strong\nbase LLMs using SELF -R EFINE . Therefore, we compare SELF -R EFINE to the same base LLMs but\nwithout feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5\n(text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based\ntasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or\nGPT-4 is the previous state-of-the-art. 3 We used the same prompts from previous work when\n3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F\n4\nGPT-3.5 ChatGPT GPT-4\nTask Base +SELF -REFINE Base +SELF -REFINE Base +SELF -REFINE\nSentiment Reversal 8.8 30.4 (↑21.6) 11.4 43.2 (↑31.8) 3.8 36.2 (↑32.4)\nDialogue Response 36.4 63.6 (↑27.2) 40.1 59.9 (↑19.8) 25.4 74.6 (↑49.2)\nCode Optimization 14.8 23.0 (↑8.2) 23.9 27.5 (↑3.6) 27.3 36.0 (↑8.7)\nCode Readability 37.4 51.3 (↑13.9) 27.7 63.1 (↑35.4) 27.4 56.2 (↑28.8)\nMath Reasoning 64.1 64.1 (0) 74.8 75.0 (↑0.2) 92.9 93.1 (↑0.2)\nAcronym Generation 41.6 56.4 (↑14.8) 27.2 37.2 (↑10.0) 30.4 56.0 (↑25.6)\nConstrained Generation 28.0 37.0 (↑9.0) 44.0 67.0 (↑23.0) 15.0 45.0 (↑30.0)\nTable 1: SELF -REFINE results on various tasks using GPT-3.5 , ChatGPT, and GPT-4 as base LLM.\nSELF -R EFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2.\navailable (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as\ndetailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups.\n3.2 Metrics\nWe report three types of metrics:\n• Task specific metric: When available, we use automated metrics from prior work (Math Reasoning:\n% solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %)\n• Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment\nReversal, and Acronym Generation, since no automated metrics are available, we perform a blind\nhuman A/B evaluation on a subset of the outputs to select the preferred output. Additional details\nare provided in Appendix C.\n• GPT-4 -pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following\nprior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high\ncorrelation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue\nResponse Generation) with human-pref. For Code Readability Improvement, we prompt GPT-\n4 to calculate fraction of the variables that are appropriately named given the context (e.g.,\nx = [] → input_buffer = [] ). Additional details are provided in Appendix D.\n3.3 Results\nTable 1 shows our main results:\nSELF -R EFINE consistently improves over base models across all model sizes, and additionally\noutperforms the previous state-of-the-art across all tasks. For example, GPT-4 +SELF -REFINE\nimproves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization\npercentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based\ntasks, we found similar trends when using CODEX ; those results are included in Appendix F.\nOne of the tasks in which we observe the highest gains compared to the base models is Constrained\nGeneration, where the model is asked to generate a sentence containing up to 30 given concepts. We\nbelieve that this task benefits significantly from SELF -R EFINE because there are more opportunities\nto miss some of the concepts on the first attempt, and thus SELF -REFINE allows the model to fix\nthese mistakes subsequently. Further, this task has an extremely large number of reasonable outputs,\nand thus SELF -R EFINE allows to better explore the space of possible outputs.\nIn preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym\nGeneration, SELF -REFINE leads to especially high gains. For example in Dialogue Response\nGeneration, GPT-4 preference score improve by 49.2% – from 25.4% to 74.6%. Similarly, we see\nremarkable improvements in the other preference-based tasks across all models.\nThe modest performance gains in Math Reasoning can be traced back to the inability to accurately\nidentify whether there is any error. In math, errors can be nuanced and sometimes limited to a single\nline or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLM s to\n5\nthink that “everything looks good” (e.g., ChatGPT feedback for 94% instances is ’everything looks\ngood’). In Appendix H.1, we show that the gains with SELF -REFINE on Math Reasoning are much\nbigger (5%+) if an external source can identify if the current math answer is incorrect.\nImprovement is consistent across base LLM s sizes Generally, GPT-4 +SELF -REFINE performs\nbetter than GPT-3.5 +SELF -REFINE and ChatGPT+SELF -REFINE across all tasks, even in tasks\nwhere the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that\nSELF -R EFINE allows stronger models (such as GPT-4 ) to unlock their full potential, even in cases\nwhere this potential is not expressed in the standard, single-pass, output generation. Comparison to\nadditional strong baselines is provided in Appendix F.\n4 Analysis\nThe three main steps of SELF -R EFINE are FEEDBACK , REFINE , and repeating them iteratively. In this\nsection, we perform additional experiments to analyze the importance of each of these steps.\nTask S ELF -R EFINE feedback Generic feedback No feedback\nCode Optimization 27.5 26.0 24.8\nSentiment Reversal 43.2 31.2 0\nAcronym Generation 56.4 54.0 48.0\nTable 2: Prompting to generate generic feedback (or having the model generate no feedback at\nall) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF -REFINE .\nThese experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and\nGPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2.\n==== papers/pages_self_refine/2023_self_refine_chunk_003.txt ====\nTask y0 y1 y2 y3\nCode Opt. 22.0 27.0 27.9 28.8\nSentiment Rev. 33.9 34.9 36.1 36.8\nConstrained Gen. 29.0 40.3 46.7 49.7\n∆(y0→y1) ∆(y1→y2) ∆(y2→y3)\n0\n5\n10\n5\n0.9 0.9\n11.3\n6.4\n3\n1 1.2 0.7\nC. Opt.\nC. Gen.\nS. Rev.\nFigure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output\nquality, and scores generally keep improving with more iterations.Right: SELF -REFINE Performance\nimprovements with iterations. Most gains(∆) are in the initial iterations for both Code Opt. and Senti-\nment Reversal. The numbers are averaged over ChatGPT, GPT-3.5 , and GPT-4 . Task abbreviations:\nC. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation).\n# Slower code\ndef solve(amount):\nbest_price = (amount + 199) // 200 *\n380,→\n# First loop\nfor a in range(amount // 200 + 1):\n# ... 4 nested loops ...\nfor c1 in range(amount // 1500 +\n1):,→\nif a*200 + b*300 == amount:\nprice = a*380 + b*550\nif price < best_price:\nbest_price = price\nreturn best_price\n# Faster code\ndef solve(amount):\ncoins = [200, 300]\nprices = [380, 550]\ndp = [float( 'inf')] * (amount + 1)\ndp[0] = 0\nfor i in range(len(coins)):\nfor j in range(coins[i], amount+1):\ndp[j] = min(dp[j], dp[j -\ncoins[i]] + prices[i]),→\nreturn dp[amount]\nFigure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying\nSELF -R EFINE (right). The initial code by the baseline, which is nearly identical to the slower input\nprogram, fails to improve the efficiency and merely alters the logic for reading input. SELF -REFINE\nfirst generates feedback that diagnoses that This code is slow because it is using six nested loops to\niterate through all possible combinations of coins to pay the amount, and suggests that a more efficient\napproach would be .... SELF -REFINE then uses this feedback to generate the revised code (right),\nreducing the time complexity to O(amount ∗ coins). The full example is provided in Appendix H\nCan we just generate multiple outputs instead of refining? Does SELF -R EFINE improve because\nof the iterative refinement, or just because it generatesmore outputs? We compare SELF -R EFINE with\nChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then,\nwe compare the performance of SELF -R EFINE against these k initial outputs in a 1 vs. k evaluation.\nIn other words, we assess whether SELF -REFINE can outperform all k initial outputs. The results\nof this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the\n1 vs. k setting, the outputs of SELF -REFINE are still preferred by humans over all k initial outputs.\nThis shows the importance of refinement according to feedback over the alternative of just generating\nmultiple initial outputs.\nDoes SELF -R EFINE work with weaker models? The experiments in Section 3.3 were performed\nwith some of the strongest available models; does SELF -R EFINE work with smaller or weaker models\nas well? To investigate this, we instantiated SELF -R EFINE with Vicuna-13B (Chiang et al., 2023), a\n7\nless powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles\nsignificantly with the refinement process. Specifically, Vicuna-13B was not able to consistently\ngenerate the feedback in the required format. Furthermore, even when provided with Oracle or\nhard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining\nits output, Vicuna-13B either repeated the same output or generated a hallucinated conversation,\nrendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on\nconversations, it does not generalize as well as instruction-based models to test-time few-shot tasks.\nExample output and analysis is provided in Appendix G.\nQualitative Analysis We conduct a qualitative analysis of the feedback generated bySELF -REFINE\nand its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35\nfailure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021).\nFor both Math Reasoning and Code Optimization, we found that the feedback was predominantly\nactionable, with the majority identifying problematic aspects of the original generation and suggesting\nways to rectify them.\nWhen SELF -REFINE failed to improve the original generation, the majority of issues were due to\nerroneous feedback rather than faulty refinements. Specifically, 33% of unsuccessful cases were due to\nfeedback inaccurately pinpointing the error’s location, while 61% were a result of feedback suggesting\nan inappropriate fix. Only 6% of failures were due to the refiner incorrectly implementing good\nfeedback. These observations highlight the vital role of accurate feedback plays in S ELF -R EFINE .\nIn successful cases, the refiner was guided by accurate and useful feedback to make precise fixes to the\noriginal generation in 61% of the cases. Interestingly, the refiner was capable of rectifying issues even\nwhen the feedback was partially incorrect, which was the situation in 33% of successful cases. This\nsuggests resilience to sub-optimal feedback. Future research could focus on examining the refiner’s\nrobustness to various types of feedback errors and exploring ways to enhance this resilience. In Figure\n5, we illustrate how SELF -R EFINE significantly improves program efficiency by transforming a brute\nforce approach into a dynamic programming solution, as a result of insightful feedback. Additional\nanalysis on other datasets such as Dialogue Response Generation is provided in Appendix H.\nGoing Beyond Benchmarks While our evaluation focuses on benchmark tasks, SELF -REFINE is\ndesigned with broader applicability in mind. We explore this in a real-world use case of website gen-\neration, where the user provides a high-level goal and SELF -REFINE assists in iteratively developing\nthe website. Starting from a rudimentary initial design, SELF -REFINE refines HTML, CSS, and JS\nto evolve the website in terms of both usability and aesthetics. This demonstrates the potential of\nSELF -R EFINE in real-world, complex, and creative tasks. See Appendix I for examples and further\ndiscussion, including broader, societal impact of our work.\n5 Related work\nLeveraging human- and machine-generated natural language (NL) feedback for refining outputs has\nbeen effective for a variety of tasks, including summarization (Scheurer et al., 2022), script generation\n(Tandon et al., 2021), program synthesis (Le et al., 2022a; Yasunaga and Liang, 2020), and other\ntasks (Bai et al., 2022a; Schick et al., 2022b; Saunders et al., 2022a; Bai et al., 2022b; Welleck et al.,\n2022). Refinement methods differ in the source and format of feedback, and the way that a refiner is\nobtained. Table 3 summarizes some related approaches; see Appendix B for an additional discussion.\nSource of feedback. Humans have been an effective source of feedback (Tandon et al., 2021;\nElgohary et al., 2021; Tandon et al., 2022; Bai et al., 2022a). Since human feedback is costly, several\napproaches use a scalar reward function as a surrogate of (or alternative to) human feedback (e.g.,\n(Bai et al., 2022a; Liu et al., 2022; Lu et al., 2022; Le et al., 2022a; Welleck et al., 2022)). Alternative\nsources such as compilers (Yasunaga and Liang, 2020) or Wikipedia edits (Schick et al., 2022b) can\nprovide domain-specific feedback. Recently, LLMs have been used to generate feedback for general\ndomains (Fu et al., 2023; Peng et al., 2023; Yang et al., 2022), However, ours is the only method that\ngenerates feedback using an LLM on its own output, for the purpose of refining with the same LLM.\nRepresentation of feedback. The form of feedback can be generally divided into natural language\n(NL) and non-NL feedback. Non-NL feedback can come in human-provided example pairs (Dasgupta\n8\nSupervision-\n==== papers/pages_self_refine/2023_self_refine_chunk_004.txt ====\nhope that our iterative approach will help drive further research in this area. To this end, we make all\nour code, data and prompts anonymously available at https://selfrefine.info/.\nReferences\nTeresa M. Amabile. 1983. A Theoretical Framework. In The Social Psychology of Creativity, pages\n65–96. Springer New York, New York, NY .\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless\nassistant with reinforcement learning from human feedback. ArXiv:2204.05862.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai:\nHarmlessness from ai feedback. arXiv preprint arXiv:2212.08073.\nEmery D Berger, Sam Stern, and Juan Altmayer Pizzorno. 2022. Triangulating Python Performance\nIssues with SCALENE. ArXiv preprint, abs/2212.07597.\nLawrence D Brown, T Tony Cai, and Anirban DasGupta. 2001. Interval estimation for a binomial\nproportion. Statistical science, 16(2):101–133.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural\nInformation Processing Systems, volume 33, pages 1877–1901, Online. Curran Associates, Inc.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.\nEvaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168.\nSanjoy Dasgupta, Daniel Hsu, Stefanos Poulis, and Xiaojin Zhu. 2019. Teaching a black-box learner.\nIn Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research,\npages 1547–1555. PMLR.\nWanyu Du, Zae Myung Kim, Vipul Raheja, Dhruv Kumar, and Dongyeop Kang. 2022. Read, revise,\nrepeat: A system demonstration for human-in-the-loop iterative text revision. In Proceedings of the\nFirst Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022), pages 96–108,\nDublin, Ireland. Association for Computational Linguistics.\n10\nAhmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, and\nAhmed Hassan Awadallah. 2021. NL-EDIT: Correcting semantic parse errors through natural\nlanguage interaction. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 5599–5610,\nOnline. Association for Computational Linguistics.\nLinda Flower and John R Hayes. 1981. A cognitive process theory of writing. College composition\nand communication, 32(4):365–387.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire.\narXiv preprint arXiv:2302.04166.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn\nSong. 2023. Koala: A dialogue model for academic research. Blog post.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. 2022a.\nCodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learn-\ning.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. 2022b.\nCoderl: Mastering code generation through pretrained models and deep reinforcement learning.\nArXiv, abs/2207.01780.\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: a simple approach to\nsentiment and style transfer. In Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nPapers), pages 1865–1874, New Orleans, Louisiana. Association for Computational Linguistics.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi,\nand Xiang Ren. 2020. CommonGen: A constrained text generation challenge for generative\ncommonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP\n2020, pages 1823–1840, Online. Association for Computational Linguistics.\nJiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, and Yejin\nChoi. 2022. Rainier: Reinforced knowledge introspector for commonsense question answering. In\nConference on Empirical Methods in Natural Language Processing.\nXiming Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Am-\nmanabrolu, and Yejin Choi. 2022. Quark: Controllable text generation with reinforced unlearning.\nArXiv, abs/2205.13636.\nAman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming\nYang, Graham Neubig, and Amir Yazdanbakhsh. 2023. Learning performance-improving code\nedits. arXiv preprint arXiv:2302.07867.\nAman Madaan, Niket Tandon, Dheeraj Rajagopal, Peter Clark, Yiming Yang, and Eduard Hovy.\n2021. Think about it! improving defeasible reasoning by first modeling the question scenario.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npages 6291–6310, Online and Punta Cana, Dominican Republic. Association for Computational\nLinguistics.\nShikib Mehri and Maxine Eskenazi. 2020. Unsupervised evaluation of interactive dialog with\nDialoGPT. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse\nand Dialogue, pages 225–235, 1st virtual meeting. Association for Computational Linguistics.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and\nCaiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program\nsynthesis. ArXiv preprint, abs/2203.13474.\nOpenAI. Model index for researchers. https://platform.openai.com/docs/\nmodel-index-for-researchers . Accessed: May 14, 2023.\n11\nOpenAI. 2022. Model index for researchers. Blogpost.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan\nLeike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human\nfeedback. ArXiv:2203.02155.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars\nLiden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check Your Facts and Try Again:\nImproving Large Language Models with External Knowledge and Automated Feedback.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, and Alan W Black. 2018. Style\ntransfer through back-translation. In Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 866–876, Melbourne, Australia.\nAssociation for Computational Linguistics.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measur-\ning and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.\nRuchir Puri, David Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladmir Zolotov, Julian\nDolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar,\nShyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss. 2021. Codenet: A large-scale\nai for code dataset for learning a diversity of coding tasks. arXiv preprint arXiv:2105.12655.\nMachel Reid and Graham Neubig. 2022. Learning to model editing processes. arXiv preprint\narXiv:2205.12374.\n==== papers/pages_self_refine/2023_self_refine_chunk_005.txt ====\nNiket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. 2022. Learning to repair: Repairing\nmodel output errors after deployment using a dynamic memory of feedback. In Findings of the\nAssociation for Computational Linguistics: NAACL 2022, pages 339–352.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou.\n2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. arXiv preprint\narXiv:2201.11903.\nSean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin\nChoi. 2022. Generating sequences by learning to self-correct. arXiv preprint arXiv:2211.00053.\nKevin Yang, Nanyun Peng, Yuandong Tian, and Dan Klein. 2022. Re3: Generating longer stories with\nrecursive reprompting and revision. In Conference on Empirical Methods in Natural Language\nProcessing.\nMichihiro Yasunaga and Percy Liang. 2020. Graph-based, self-supervised program repair from\ndiagnostic feedback. 37th Int. Conf. Mach. Learn. ICML 2020, PartF168147-14:10730–10739.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text\nclassification. Advances in neural information processing systems, 28.\n13\nA Evaluation Tasks\nTable 4 lists the tasks in our evaluation, and examples from each task.\nTask and Description Sample one iteration of FEEDBACK -REFINE\nSentiment Reversal\nRewrite reviews to reverse sentiment.\nDataset: (Zhang et al., 2015) 1000 review pas-\nsages\nx: The food was fantastic...”\nyt: The food was disappointing...”\nf b: Increase negative sentiment\nyt+1: The food was utterly terrible...”\nDialogue Response Generation\nProduce rich conversational responses.\nDataset: (Mehri and Eskenazi, 2020) 372 conv.\nx: What’s the best way to cook pasta?”\nyt: The best way to cook pasta is to...”\nf b: Make response relevant, engaging, safe\nyt+1: Boil water, add salt, and cook pasta...”\nCode Optimization\nEnhance Python code efficiency\nDataset: (Madaan et al., 2023): 1000 programs\nx: Nested loop for matrix product\nyt: NumPy dot product function\nf b: Improve time complexity\nyt+1: Use NumPy’s optimized matmul function\nCode Readability Improvement\nRefactor Python code for readability.\nDataset: (Puri et al., 2021) 300 programs∗\nx: Unclear variable names, no comments\nyt: Descriptive names, comments\nf b: Enhance variable naming; add comments\nyt+1: Clear variables, meaningful comments\nMath Reasoning\nSolve math reasoning problems.\nDataset: (Cobbe et al., 2021) 1319 questions\nx: Olivia has $23, buys 5 bagels at $3 each”\nyt: Solution in Python\nf b: Show step-by-step solution\nyt+1: Solution with detailed explanation\nAcronym Generation\nGenerate acronyms for a given title\nDataset: (Appendix Q) 250 acronyms\nx : Radio Detecting and Ranging”\nyt: RDR\nf b : be context relevant; easy pronunciation\nyt+1: RADAR”\nConstrained Generation\nGenerate sentences with given keywords.\nDataset: (Lin et al., 2020) 200 samples\nx: beach, vacation, relaxation\nyt: During our beach vacation...\nf b: Include keywords; maintain coherence\nyt+1: .. beach vacation was filled with relaxation\nTable 4: An overview of the tasks which we evaluate SELF -R EFINE on, along with their associated\ndatasets and sizes. For every task, we demonstrate a single iteration of refinement of input x, the\npreviously generated output yt, the feedback generated f bt, and the refinement yt+1. Few-shot\nprompts used for FEEDBACK and REFINE are provided in Appendix S.\n14\nB Broader Related Work\nCompared to a concurrent work, Reflexion (Shinn et al., 2023), our approach involves correction\nusing feedback, whereas their setup involves finding the next best solution in planning using ReAct.\nWhile ReAct and Reflexion provide a free-form reflection on whether a step was executed correctly\nand potential improvements, our approach is more granular and structured, with multi-dimensional\nfeedback and scores. This distinction allows our method to offer more precise and actionable feedback,\nmaking it suitable for a wider range of natural language generation tasks, including those that may\nnot necessarily involve step-by-step planning such as open-ended dialogue generation.\nComparison with Welleck et al. (2022) The closest work to ours may be Self-Correction (Welleck\net al., 2022); however, Self-Correction has several disadvantages compared to SELF -R EFINE :\n1. Self-Correction does not train their model to generate explicit feedback; instead, Welleck\net al. (2022) trained their models to refine only. As we show in Section 4 and Table 2, having\nthe model generate explicit feedback results in significantly better refined outputs.\n2. Self-Correction trains a separate refiner (or “corrector”) for each task. In contrast, SELF -\nREFINE uses instructions and few-shot prompting, and thus does not require training a\nseparate refiner for each task.\n3. Empirically, we evaluated SELF -REFINE using the same base model of GPT-3 as Self-\nCorrection, and with the same settings on the GSM8K benchmark. Self-Correction achieved\n45.9% accuracy while SELF -R EFINE (this work) achieved 55.7% (↑9.8).\nComparison with non-refinement reinforcement learning (RL) approaches. Rather than having\nan explicit refinement module, an alternative way to incorporate feedback is by optimizing a scalar\nreward function, e.g. with reinforcement learning (e.g., Stiennon et al. (2020); Lu et al. (2022); Le\net al. (2022a)). These methods differ from SELF -REFINE (and more generally, refinement-based\napproaches) in that the model cannot access feedback on an intermediate generation. Second, these\nreinforcement learning methods require updating the model’s parameters, unlike SELF -R EFINE .\nSee Table 5 for an additional detailed comparison of related work.\nMethod Primary Novelty zero/few shot improvement multi aspect critics NL feedback with er-\nror localization\niterative framework\nRLHF (Stiennon et al., 2020) optimize for human preference\ntrained on feedback\n single (human)\n (not self gen.)\nRainier RL (Liu et al., 2022) RL to generate knowledge\n trained on end task\n single(accuracy)\n (knowl. only)\nQUARKRL (Lu et al., 2022) quantization to edit generations\ntrained on end task\n single(scalar score)\n (dense signal)\n (train time iter.)\nCode RL (Le et al., 2022a) actor critic RL for code im-\n==== papers/pages_self_refine/2023_self_refine_chunk_006.txt ====\nC Human Evaluation\nThe A/B evaluation in our study was conducted by the authors, where a human judge was presented\nwith an input, task instruction, and two candidate outputs generated by the baseline method and\nSELF -REFINE . The setup was blind, i.e., the judges did not know which outputs were generated\nby which method. The judge was then asked to select the output that is better aligned with the\ntask instruction. For tasks that involve A/B evaluation, we calculate the relative improvement as\nthe percentage increase in preference rate. The preference rate represents the proportion of times\nannotators selected the output produced by SELF -REFINE over the output from the baseline method.\nTable 6 shows the results.\nTask S ELF -REFINE (%) Direct (%) Either (%)\nSentiment Transfer 75.00 21.43 3.57\nAcronym Generation 44.59 12.16 43.24\nResponse Generation 47.58 19.66 32.76\nTable 6: Relative improvement of SELF -R EFINE in A/B evaluations across different tasks. The values\nrepresent normalized preferences, which correspond to the proportion of times the output generated\nby SELF -REFINE was selected as better aligned with the task instruction over the baseline method.\nThe evaluation was conducted for 150 examples for each dataset. The judges were not aware of the\nmethod that generated each sample.\n16\nD GPT-4 Evaluation\nIn light of the impressive achievements of GPT-4 in assessing and providing reasoning for complex\ntasks, we leverage its abilities for evaluation in SELF -REFINE . The approach involves presenting\ntasks to GPT-4 in a structured manner, promoting the model’s deliberation on the task and generating\na rationale for its decision. This methodology is demonstrated in Listings 1 to 3:\nListing 1 Prompt for GPT-4 evaluation of Sentiment Reversal.\nf\"\"\"Which review is aligned with the sentiment {target_sentiment}?\nReview A: {review_a}\nReview B: {review_b}.\nPick your answer from [ 'Review A ', 'Review B ', 'both', 'neither']. Generate a\nshort explanation for your choice first. Then, generate 'The more aligned\nreview is A ' or 'The more aligned review is B ' or 'The more aligned review is\nboth' or 'The more aligned review is neither '.\n,→\n,→\n,→\nFormat: <explanation> <answer> STOP\nListing 2 Prompt for GPT-4 evaluation of Acronym Generation.\nf\"\"\"Title: {title}\nAcronym A: {acronym_a}\nAcronym B: {acronym_b}\nPick the better acronym for the given title. The acronyms should be compared based\non the following criteria:,→\n* Ease of pronunciation.\n* Ease of spelling.\n* Relation to title.\n* Positive connotation.\nGenerate your answer in the following format:\n<Short explanation>. The better acronym is A OR The better acronym is B OR The\nacronyms are equally good OR Neither acronym is good. STOP.,→\nListing 3 Prompt for GPT-4 evaluation of Dialogue Response Generation.\nf\"\"\"Which response is better given this context: {context}?\nResponse A: {response_a}\nResponse B: {response_b}.\nPick your answer from [ 'Response A ', 'Response B ', 'both', 'neither']. Generate a\nshort explanation for your choice first. Then, generate 'The better response\nis A ' or 'The better response is B ' or 'The better response is both ' or 'The\nbetter response is neither '.\n,→\n,→\n,→\nFormat: <explanation> <answer> STOP\nE Model Key\nWe use terminology here: https://platform.openai.com/docs/models/gpt-3-5\n17\nF Comparison of S ELF -REFINE with State-of-the-art of Few-Shot Learning\nModels and Fine-Tuned Baselines\nIn this section, we present a comprehensive comparison of the performance of SELF -REFINE with\nother few-shot models and fine-tuned baselines across a range of tasks, including mathematical\nreasoning and programming tasks. Tables 8 and 7 display the performance of these models on the\nPIE dataset and GSM tasks, respectively. Our analysis demonstrates the effectiveness of different\nmodel architectures and training techniques in tackling complex problems.\nMethod Solve Rate\nCobbe et al. (2021) OpenAI 6B 20.0\nWei et al. (2022) CoT w/ C ODEX 65.6\nGao et al. (2022)\nPaL w/ CODEX 72.0\nPaL w/ GPT-3 52.0\nPaL w/ GPT-3.5 56.8\nPaL w/ ChatGPT 74.2\nPaL w/ GPT-4 93.3\nWelleck et al. (2022) Self-Correct w/ GPT-3 45.9\nSelf-Correct (fine-tuned) 24.3\nThis work\nSELF -R EFINE w/ GPT-3 55.7\nSELF -R EFINE w/ GPT-3.5 62.4\nSELF -R EFINE w/ ChatGPT 75.1\nSELF -R EFINE w/ GPT-4 94.5\nTable 7: Performance comparison of models on math reasoning (Math Reasoning).\n18==== papers/pages_self_refine/2023_self_refine_chunk_007.txt ====\nMethod %OPT)\nPuri et al. (2021) Human References 38.2\nOpenAI Models: OpenAI (2022, 2023)\nCODEX 13.1\nGPT-3.5 14.8\nChatGPT 22.2\nGPT-4 27.3\nNijkamp et al. (2022) C ODE GEN-16B 1.1\nBerger et al. (2022)\nSCALENE 1.4\nSCALENE (BEST @16) 12.6\nSCALENE (BEST @32) 19.6\nMadaan et al. (2023)\nPIE-2B 4.4\nPIE-2B (BEST @16) 21.1\nPIE-2B (BEST @32) 26.3\nPIE-16B 4.4\nPIE-16B (BEST @16) 22.4\nPIE-16B (BEST @32) 26.6\nPIE-Few-shot (BEST @16) 35.2\nPIE-Few-shot (BEST @32) 38.3\nThis work\nSELF -R EFINE w/ GPT-3.5 23.0\nSELF -R EFINE w/ ChatGPT 26.7\nSELF -R EFINE w/ GPT-4 36.0\nTable 8: Performance comparison of various models on the PIE dataset in terms of the percentage\nof programs optimized (%OPT). The table includes human references, baseline models, fine-tuned\nPIE-2B and PIE-16B models, and our proposed model (SELF -REFINE ) using different LLMs. Notably,\nSELF -R EFINE achieves superior performance while using only 4 samples at most, significantly fewer\nthan the 16 and 32 samples employed by other models. Scalene, an off-the-shelf optimizer, uses\ninstruction tuning with Codex and serves as a comparison point.\n19\nG Evaluation of Vicuna-13b\nWe also experiment with Vicuna-13b (Chiang et al., 2023), a version of LLaMA-13b (Touvron et al.,\n2023) fine-tuned on conversations sourced from the web. Vicuna-13b was able to consistently follow\nthe task initialization prompt. However, it struggled to follow the prompts intended for feedback\nand refinement. This often led to outputs that resembled assistant-like responses, a representative\nexample of which can be found in Appendix G.\nIt’s important to note that we used the same prompts for Vicuna-13b as those used with other models\nin our study. However, the limited performance of Vicuna-13b suggests that this model may require\nmore extensive prompt-engineering for optimal performance.\nMixed-refine: Improving Vicuna-13b with ChatGPT While the focus of SELF -REFINE is\nimprovement of the model without any external help, it may be possible to use a smaller model for\nthe initialization, and then involving a bigger model for refinement. To test this, we experiment with\na setup where we use Vicuna-13b as the initialization model, and use ChatGPT as the FEEDBACK and\nREFINE . The results on Math Reasoning show the promise of this approach: while Vicuna-13b was\nable to get only 24.18% on Math Reasoning, it was able to improve to 40.5% in this mixed-refinement\nsetting.\n20\nStep 0 INIT> The food was amazing, I loved it!!.\nStep 0 TRANSFER> The food was terrible, I hated it!!.\nlist index out of range\nAn error occurred: list index out of range. Traceback (most recent call last):\nIndexError: list index out of range . Left retries: 2.\nStep 1 TRANSFER> The food at the Trop was awful. I wish I could 've gotten my money\nback for the terrible meals.,→\nStep 1 FB> The rewritten review does not become more \"Very negative\", because it\nonly contains a single instance of a positive sentiment about the food. Try\nagain to make it Very negative!\n,→\n,→\nStep 2 TRANSFER> ### Assistant: The Trop is a great choice for anyone looking for\na budget-friendly option in Vegas. The rooms and hallways are clean and\ncomfortable, and the food options are varied and delicious.\n,→\n,→\nStep 2 FB> The rewritten review does not change from \"Positive\", because it only\nhas positive language. The output is already Very positive. No further changes\nare needed.\n,→\n,→\nStep 0 INIT> The food was amazing, I loved it!!. [Positive] -> [Very negative]\nStep 0 TRANSFER> The food was atrocious, I absolutely hated it! [( 'The review\nsounds very negative. The sentiment is Very negative ', 1.0)],→\nStep 0 FB> The original review is already \"Very negative\" as it uses strongly\nnegative words like \"atrocious\" and \"absolutely hated\". These words express\nextreme dissatisfaction and clearly convey a very negative sentiment about the\nfood. But try again to make it even more negative!\n,→\n,→\n,→\nStep 1 TRANSFER> The food was so abysmal, it was a nightmare to even taste it! I\ncouldn't stand it and regretted every single bite. [( 'The review sounds highly\nnegative. The sentiment is Very negative ', 1.0)]\n,→\n,→\nStep 1 FB> The review provided is indeed very negative. It uses extremely negative\nwords and phrases like \"abysmal\", \"nightmare\", and \"regretted every single\nbite\". These strongly negative expressions make the review very negative, but\ntry again to make it even more negative!\n,→\n,→\n,→\nStep 2 TRANSFER> The food was absolutely revolting, I couldn 't even stomach a\nsingle bite!,→\nStep 2 FB> This review is not \"Very negative\" because it contains positive words\nand phrases like \"amazing\" and \"loved it.\" To make it \"Very negative\", we need\nto use extremely negative words, like \"revolting\", \"couldn 't stomach\", or\n\"horrible\". The provided \"Very negative\" sentiment example is a good\nillustration of a very negative review, as it uses strong negative language to\ndescribe the food experience.\n,→\n,→\n,→\n,→\n,→\nVicuna-13b outputs (top) for sentiment transfer, where the goal is to make ‘The food was amazing, I\nloved it!!.’ negative. Vicuna-13b generates a good initial output, but generates an empty feedback in\nthe first try, leading to an error. In the next try, it generates an unhelpful feedback. The transferred\nreview copies from the prompt. Compare with the outputs from GPT-4 (bottom).\n21==== papers/pages_self_refine/2023_self_refine_chunk_008.txt ====\n0 10 20 30 40 50 60 70 80 90 100\nSELF-REFINE\nSELF-REFINE 27.2\n15.5\n35.6\n51.1\n37.2\n33.3\nPreference rates for Sentiment Reversal\nMULTI\nChatGPT27.2\n15.5\n35.6\n51.1\n37.2\n33.3\n0 10 20 30 40 50 60 70 80 90 100\nSELF-REFINE\nSELF-REFINE\n11.4\n6.1\n45.4\n53.82\n43.2\n40.05\nPreference rates for Acronym Generation\nChatGPT\nMULTI\n11.4\n6.1\n45.4\n53.82\n43.2\n40.05\nFigure 6: Preference for the outputs generated by our method (SELF -REFINE ), the multiple-sample\nbaseline (MULTI), and ties (ties).\nGPT-3.5 ChatGPT GPT-4\nTask Base +SELF -REFINE Base +SELF -REFINE Base +SELF -REFINE\nMath Reasoning 64.1 64.1 (0) 74.8 75.0 (↑0.2) 92.9 93.1 (↑0.2)\nMath Reasoning (Oracle) 64.06 68.9 (↑4.8) 74.8 76.2 (↑1.4) 92.9 93.8 (↑0.7)\nTable 9: SELF -REFINE results on Math Reasoning using GPT-3.5 , ChatGPT, and GPT-4 as base\nLLM with Oracle feedback.\nH Additional Analysis\nH.1 Using Oracle Feedback\nWe experimented withOracle Feedbackfollowing Welleck et al. (2022). This method uses correctness\ninformation to guide model refinement, only progressing to REFINE stage if the current answer is\nincorrect. This adjustment notably enhanced performance in the Math Reasoning task, with GPT-3\nimproving by 4.8% and GPT-4 by 0.7% Table 9. This indicates the potential of external signals to\noptimize model performance in particular tasks.\nIteration Acronym Pronunciation Pron. (5) Spell. (5) Rel. (5) Pos. Con. (5) Total (25)\n1 USTACCSF us-tacks-eff 1 1 5 3 11\n2 TACC-SIM tacks-sim 4 4 5 3 17\n3 TACCSF tacks-eff 1 2 5 3 12\n4 TACC-SIMF tack-simf 4 4 5 3 17\nTable 10: Acronym generation results across iterations, showcasing how improvements in certain as-\npects (e.g., pronunciation and spelling) can be accompanied by losses in others, leading to fluctuating\noverall performance in multi-aspect feedback tasks like Acronym Generation.\nNon-monotonic increase in output quality for acronym generation For tasks with multi-aspect\nfeedback like Acronym Generation, the output quality can fluctuate during the iterative process,\nimproving on one aspect while losing out on another (Table 10). To address this, SELF -REFINE ’s\nfeedback generates explicit numerical scores to capture the different aspects of output quality. This\nallows for a more balanced evaluation of outputs and the selection of the most appropriate one. The\nalgorithm selects the best output based on the maximum score across all iterations, as described\nin Algorithm 1 (line 8). A similar selection is possible for other tasks like Math Reasoning and\nSentiment Reversal, while we observe that output quality increases monotonically with iterations.\n22\nError Type Occurrence Feedback Refinement\nIncorrect feedback 25% Informative: The response\nprovides some encourage-\nment and reassurance, and\nadditional information. 3/3\nResponse: That’s hilarious!\nDon’t worry, I’m sure you’ll\ndo much better when you take\nHistory next semester. Are\nyou looking forward to it?\nGeneric feedback 30% Specific: The response is\nnot specific and does not\nprovide any details or ex-\namples. 1/3\nResponse: It’s understandable,\nI’m sure you’ll do much bet-\nter when you take History next\nsemester. Maybe you can use\nyour knowledge of Christo-\npher Columbus to ace your\nnext test! Are you looking for-\nward to it?\nIncorrect Scoring 10% Informative: The response\nprovides a supportive and\nunderstanding message,\nwhich is informative to the\nuser. 2/3\nResponse: I’m sorry to hear. . .\nwhen Christopher Columbus\ndidn’t even fight in the Civil\nWar! I’m sure you’ll do much\nbetter. . . . Are you looking for-\nward to it?\nTable 11: Error analysis for Dialogue Response Generation: When the feedback is not useful, a large\nmajority is not specific or incorrect.\nError Type Occurrence Feedback Refinement\nNot-Robust 10% Specific: The response is\nspecific and provides spe-\ncific details or examples.\n3/3\nResponse: That’s pretty\nfunny! It’s understand-\nable that you didn’t do\nwell on the test. I’m\nsure you’ll do much bet-\nter when you take History\nnext semester\nIgnores feedback 25% Informative: The re-\nsponse doesn’t provide\nany new information. 1/3\nResponse: That’s so kind\nand generous of you! I\nwould love to hear more\nabout the podcasts you\n==== papers/pages_self_refine/2023_self_refine_chunk_009.txt ====\nI Beyond Benchmarks\nSELF -REFINE demonstrates its iterative feedback and refinement capabilities in the context of\nwebsite layout generation. ChatGPT initially produces a rudimentary layout for a given topic, and\nthen uses the FEEDBACK to suggest specific, actionable improvements, as demonstrated in Figures 7\nand 9. These suggestions range from design changes such as color and font adjustments, to content\nenhancements and layout modifications. Figures 8 and 10 showcase the final layouts, post-feedback\nimplementation, highlighting the potential and versatility of SELF -REFINE across different scenarios.\nFigure 7: Initial web layout generated by our model for a fictional ice cream parlor.\nIce Cream Generation The feedback generated by FEEDBACK for ice cream generation:\n• Change the background color of the container to a light blue color (#6f2ff).\n• Change the font size of the heading to 48px.\n• Add a small icon before the \"Welcome to our ice cream parlor!\" text using the URL https://cdn-\nicons-png.flaticon.com/512/3622/3622340.png.\n• Add an additional paragraph after the existing text with the following text: \"We also offer a variety\nof toppings and cones to complement your ice cream. Visit us today to try our latest flavors and\nindulge in a sweet treat!\"\n• Increase the font size of the button text to 24px.\n• Update the button color to #9933.\nPhotosynthesis The feedback generated by FEEDBACK for photosynthesis:\n• Increase the font size of the text to 18px for better readability.\n• Add more information about the benefits of photosynthesis.\n• Remove the unnecessary margin-top from the header.\n• Add a ruler or divider below the header to separate it from the image.\n25\nFigure 8: Refined web layout after applying model feedback. The feedback included changing the\nbackground color to light blue (#6f2ff), increasing the heading font size to 48px, adding an icon\nbefore the welcome text, enhancing the content with an additional paragraph, increasing the button\ntext size to 24px, and updating the button color to #9933.\nFigure 9: Initial web layout generated by our model for a page on photosynthesis.\n26\nFigure 10: Refined web layout after applying model feedback. The feedback included increasing\nthe text font size to 18px for better readability, adding more information about the benefits of\nphotosynthesis, removing the unnecessary margin-top from the header, and adding a ruler or divider\nbelow the header to separate it from the image.\n27==== papers/pages_self_refine/2023_self_refine_chunk_010.txt ====\nJ Statistical Confidence Intervals\nGPT-3.5 ChatGPT GPT-4\nTask Base +S ELF -REFINE Base +S ELF -REFINE Base +S ELF -REFINE\nSentiment Reversal 8.8 ± 2.05 30.4 ± 3.61∗ 11.4 ± 2.34 43.2 ± 3.98∗ 3.8 ± 1.28 36.2 ± 3.82∗\nDialogue Response 36.4 ± 6.14 63.6 ± 6.62∗ 40.1 ± 6.33 59.9 ± 6.67∗ 25.4 ± 5.36 74.6 ± 6.22∗\nCode Optimization 14.8 ± 2.66 23.0 ± 3.25∗ 23.9 ± 3.30 27.5 ± 3.49 27.3 ± 3.48 36.0 ± 3.81∗\nCode Readability 37.4 ± 6.86 51.3 ± 7.39 27.7 ± 6.13 63.1 ± 7.40∗ 27.4 ± 6.10 56.2 ± 7.45∗\nMath Reasoning 64.1 ± 3.47 64.1 ± 3.47 74.8 ± 3.20 75.0 ± 3.20 92.9 ± 2.05 93.1 ± 2.03\nAcronym Gen. 41.6 ± 7.72 56.4 ± 8.15 27.2 ± 6.60 37.2 ± 7.46 30.4 ± 6.92 56.0 ± 8.15∗\nConstrained Gen. 28.0 ± 7.38 37.0 ± 8.26 44.0 ± 8.72 67.0 ± 9.00∗ 15.0 ± 5.38 45.0 ± 8.77∗\nTable 13: SELF -REFINE results from table 1 with Wilson confidence interval (at 95% confidence\ninterval) and statistical significance. On various tasks using GPT-3.5 , ChatGPT, and GPT-4 as\nbase LLM , SELF -REFINE consistently improves LLM . Metrics used for these tasks are defined in\nSection 3.2 as follows: Math Reasoning uses the solve rate; Code Optimization uses the percentage\nof programs optimized; and Sentiment Reversal, Dialogue Response and Acronym Gen use a GPT-\n4-based preference evaluation, which measures the percentage of times outputs from the base or\nenhanced models were selected, with the rest categorized as a tie. Constrained Gen uses the coverage\npercentage. Gains over Base, that are statistically significant based on these confidence intervals are\nmarked *\nTable 13 shows results from Table 1 with Wilson confidence interval (Brown et al., 2001) (at α=\n99% confidence interval) and statistical significance. Gains that are statistical significance based on\nthese confidence intervals are marked with an asterisk. We find that nearly all ofGPT-4 gains are\nstatistically significant, ChatGPT gains are significant for 4 out of 7 datasets, and GPT-3.5 gains are\nsignificant for 3 out of 7 datasets.\n28\nK New Tasks\nConstrained Generation We introduce “CommonGen-Hard,\" a more challenging extension of the\nCommonGen dataset (Lin et al., 2020), designed to test state-of-the-art language models’ advanced\ncommonsense reasoning, contextual understanding, and creative problem-solving. CommonGen-\nHard requires models to generate coherent sentences incorporating 20-30 concepts, rather than only\nthe 3-5 related concepts given in CommonGen. SELF -REFINE focuses on iterative creation with\nintrospective feedback, making it suitable for evaluating the effectiveness of language models on the\nCommonGen-Hard task.\nAcronym Generation Acronym generation requires an iterative refinement process to create\nconcise and memorable representations of complex terms or phrases, involving tradeoffs between\nlength, ease of pronunciation, and relevance, and thus serves as a natural testbed for our approach.\nWe source a dataset of 250 acronyms4 and manually prune it to remove offensive or uninformative\nacronyms.\nL Code Readability\nOrthogonal to the correctness, readability is another important quality of a piece of code: though not\nrelated to the execution results of the code, code readability may significantly affect the usability,\nupgradability, and ease of maintenance of an entire codebase. In this section, we consider the problem\nof improving the readability of code with SELF -REFINE . We let an LLM write natural language\nreadability critiques for a piece of code; the generated critiques then guide another LLM to improve\nthe code’s readability.\nL.1 Method\nFollowing the SELF -R EFINE setup, we instantiate INIT , FEEDBACK , and REFINE . The INIT is a no-op\n— we directly start by critiquing the code with FEEDBACK and applying the changes with REFINE .\n• FEEDBACK We prompt an LLM with the given code and an instruction to provide feedback\non readability. We give the LLM the freedom to freely choose the type of enhancements\nand express them in the form of free text.\n• REFINE The code generator LLM is prompted with the piece of code and the readability\nimprovement feedback provided by FEEDBACK . In addition, we also supply an instruction\nto fix the code using the feedback. We take the generation from the code generator as the\nproduct of one iteration in the feedback loop.\nStarting from an initial piece of code y0, we first critique, c1 = critique(y0), and then edit the\ncode, y1 = editor(y0, c1). This is recursively performed N times, where ck+1 = critique(yk) and\nyk+1 = editor(yk, ck+1).\nL.2 Experiments\nDataset We use the CodeNet (Puri et al., 2021) dataset of competitive programming. 5 For our\npurpose, these are hard-to-read multi-line code snippets. We consider a random subset of 300\nexamples and apply SELF -R EFINE to them.\nWe also ask human annotators to edit a 60-example subset to assess human performance on this task.\nThe human annotators are asked to read the code piece and improve its readability.\nImplementation Both the critique and the editor models are based on the InstructGPT model (text-\ndavinci-003). We consider the temperature of both T = 0 .0 (greedy) and T = 0 .7 (sampling)\nfor decoding Natural Language suggestion from the critique model. We always use a temperature\nT = 0 .0 (greedy) when decoding Programming Language from the code editor. Due to budget\nconstraints, we run SELF -REFINE for N = 5 iterations. The exact prompts we use can be found in\nFigures 22-23.\n4https://github.com/krishnakt031990/Crawl-Wiki-For-Acronyms/blob/master/AcronymsFile.csv\n5https://github.com/IBM/Project_CodeNet\n29\nMeaningful Variable Ratio Comment Per Line Function Units\nHuman Annotator Rewrites 0.653 0.24 0.70\nSELF -R EFINE (T = 0.0) 0.628 0.12 1.41\nSELF -R EFINE (T = 0.7) 0.700 0.25 1.33\nTable 14: Human v.s. SELF -R EFINE performance on 60-example subset. We see SELF -R EFINE can\nreach similar or achieve even better performance on the metrics compared to rewrites given by human\nannotator.\nEvaluation Methods We consider a few automatic heuristic-based evaluation metrics,\n• Meaningful Variable Names: In order to understand the flow of a program, having semanti-\ncally meaningful variable names can offer much useful information. We compute the ratio\nof meaningful variables, the number of distinct variables with meaningful names to the total\nnumber of distinct variables. We automate the process of extracting distinct variables and\nthe meaningful subset of variables using a few-shot prompted language model.\n• Comments: Natural language comments give explicit hints on the intent of the code. We\ncompute the average number of comment pieces per code line.\n• Function Units: Long functions are hard to parse. Seasoned programmers will often refactor\nand modularize code into smaller functional units.\nResult For each automatic evaluation metric, the ratio of meaningful variable, of comment, and\nthe number of function units, we compute for each iteration averaged across all test examples and\nplot for each SELF -REFINE iteration in Figure 11(a), Figure 11(b) and Figure 11(c) respectively.\nThe two curves each correspond to critique with temperature T = 0.0 and T = 0.7. The iteration 0\nnumber is measured from the original input code piece from CodeNet. We observe the average of all\nthree metrics grows across iteration of feedback loops. A diverse generation of a higher temperature\nin the critique leads to more edits to improve the meaningfulness of variable names and to add\ncomments. The greedy critique, on the other hand, provides more suggestions on refactoring the code\nfor modularization. Figure 12 provides an example of code-readability improving over iterations.\nIn Table 14, we measure human performance on all three metrics and compare with SELF -REFINE\nlast iteration output. At T = 0.7, SELF -REFINE produces more meaning variables, more function\nunits and slightly more comments compared to the human annotators on average. At T = 0 .0,\nSELF -REFINE produces less meaningful variables, less comments per line but even more function\nunits.\n0 1 2 3 4 50\n0.2\n0.4\n0.6\n0.8\nIteration\ny\nT=0.0T=0.7\n(a) Meaningful variable ratio across\ndifferent SELF -REFINE iterations.\n0 1 2 3 4 50\n0.1\n0.2\n0.3\nIteration\ny\n==== papers/pages_self_refine/2023_self_refine_chunk_011.txt ====\nStarting Code: v0\nprint((int((int(eval(input()))+1)/2)))\nCode v1\nprint(\n(int(\n(int(eval(input())) + 1)\n/ 2\n)\n)\nCode v2\nnum_input = eval(input())\nnum_input = int(num_input)\nnum_input += 1\nnum_result = int(num_input / 2)\nprint(num_result)\nFigure 12: SELF -REFINE iterations over a piece of densely composed code. Through out the\niterations, the model first try to indent out the code and ends up rewriting it into several lines of\natomic operations.\nchallenging to develop a system that can consistently generate coherent and engaging responses.\nIn this section, we use SELF -REFINE for automatically generated feedback and applying iterative\nrefinement to improve the quality of the responses.\nM.1 Modules\nWe follow the high-level description of the framework from Section 2, and instantiate our framework\nas follows.\nInit This is the first step in performing the task. The init module takes the dialogue context as input\nand generates a response that follows the conversation.\nFeedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the\nresponse generated. Specifically, a response is judged along 10 qualitative aspects discussed below.\nA more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and\nEskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback\nexplicitly points out the reasons why a response scores low on some qualitative aspect. We show an\nexample in Figure 13.\n• Relevant Does the response addresses all important aspects of the context?\n• Informative - Does the response provide some information relevant to the context?\n• Interesting - Doe the response beyond providing a simple and predictable answer to a\nquestion or statement?\n• Consistent - Is the response consistent with the rest of the conversation in terms of tone and\ntopic?\n• Helpful - Is the response helpful in providing any information or suggesting any actions?\n• Engaging - Is the response engaging and encourage further conversation?\n• Specific - The response contains specific content related to a topic or question,\n• Safe - Is the response safe and does not contain any offensive, toxic or harmful content and\ndoes not touch on any sensitive topics or share any personal information?\n• User understanding - Does the response demonstrate an understanding of the user’s input\nand state of mind?\n• Fluent Is the response fluent and easy to understand?\n31\nContext: Speaker A - What are your hobbies?\nSpeaker B - I really just spend the rest of my free time with those\nthat are close to me.\nSpeaker A - I m very much interested in playing table tennis.\nResponse: That sounds like a lot of fun! I 'm not very good at table tennis, but\nI'm sure it 's a great way to socialize and stay active.\nFeedback:\n* Relevant: The response is relevant to the user 's input and shows interest in\nthe user 's hobby. 3/3\n* Informative: The response does not provide any information about table tennis\nor how to play it. 1/3\n* Interesting: The response is not very interesting or intriguing. 1/3\n* Consistent: The response is consistent with the tone and context of the\nconversation. 3/3\n* Helpful: The response is not particularly helpful as it does not provide any\ninformation or suggest any actions. 1/3\n* Engaging: The response is somewhat engaging. It acknowledges the user 's\ninterest in table tennis and encourages them to continue playing. 2/3\n* Specific: The response is not specific, as it does not address the topic of\ntable tennis in any particular way. 1/3\n* Safe: The response is safe and does not contain any offensive, toxic or\nharmful content and does not touch on any sensitive topics or share any\npersonal information. 3/3\n* User understanding: The response shows some understanding of the user 's input\nand acknowledges their interest in table tennis. 2/3\n* Fluent: The response is fluent in terms of grammar and flow of words. 3/3\n* Total score: 20/30\nNew Response: Table tennis is a great hobby! It 's a great way to stay active\nand socialize with others. Have you ever tried it before? If not, I 'd suggest\nstarting with some beginner tutorials on Youtube. They are really helpful in\ngetting a good grasp of the basics.\nFigure 13: SELF -REFINE prompts for dialogue response generation: INIT generates a first draft of\nthe response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and\nnatural language feedback on several qualitative aspects of the response. REFINE takes the response\nand the feedback and refines it to match the feedback better.\nIterate The iterate module takes a sequence of dialogue context, prior generated responses, and\nthe feedback and refines the output to match the feedback better. An example of a context, response,\nfeedback and a refined response is shown in Figure 13.\nM.2 Setup and Experiments\nModel and Baseline We establish a natural baseline for our approach by using the model directly,\nwithout any feedback, which we refer to as INIT . Our implementation of SELF -REFINE employs a\nfew-shot setup, where each module (INIT , FEEDBACK , ITERATE ) is implemented as few-shot prompts,\nand we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot\nin-context examples for the INIT model, and instruct the model to produce a response that is good\nat the 10 aspects listed above. As in-context examples for FEEDBACK , we use the same 3 contexts\nand responses shown to the INIT model (including low-scoring variations of those responses), along\nwith scores and explanations for each feedback aspect. The ITERATE model is also shown the same\nin-context examples, and it consists of contexts-response-feedback followed by a better version of\nthe response. For SELF -REFINE , we chose the response that gets the highest total score from the\nFEEDBACK model across all iterations excluding the initial response. We use text-davinci-003\nfor all the experiments.\n32\nGPT-3.5 ChatGPT GPT4\nSELF -R EFINE wins 36.0 48.0 54.0\nINIT wins 23.0 18.0 16.0\nBoth are equal 41.0 50.0 30.0\nTable 15: Human evaluation results for dialogue response generation\nEvaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED\ndataset is a collection of human-system and human-human conversations annotated with eighteen\nfine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to\nevaluate interactive dialog systems without relying on reference responses or training data. We\nevaluate the quality of the generated outputs using both automated and human evaluation methods.\nFor automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and\nevaluate on a test set of 342 instances. We show the model the responses generated by SELF -REFINE\nand the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We\nreport the win rate. However, we acknowledge that automated metrics may not provide an accurate\nassessment of text generation tasks and rely on human evaluation instead.\nGiven a dialogue context with a varying number of turns, we generate outputs from the above\nmentioned methods. For human evaluation, for 100 randomly selected test instances, we show\nannotators the 10 response quality aspects, responses from SELF -R EFINE and INIT models and ask\nthem to select the better response. They are also given the option to select “both” when it is hard to\nshow preference toward one response.\nResults Automatic evaluation results are shown in Table1 and human evaluation results are are\n==== papers/pages_self_refine/2023_self_refine_chunk_012.txt ====\nO Math Reasoning\nWe use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluatingSELF -REFINE\non math reasoning. In the context of grade school mathematics, SELF -R EFINE aims to enable LLMs\nto iteratively refine their mathematical problem-solving outputs based on introspective feedback.\nFollowing Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the\nfollowing example from the paper, where an error in the code demonstrates a lack of understanding\nof the problem:\ndef solution():\n\"\"\"Twenty dozen cups cost $1200 less than the total cost of\nhalf a dozen plates sold at $6000 each.\nCalculate the total cost of buying each cup.\"\"\"\nplates = 6\nplate_cost = 6000\ncups = 12 * 20\ncup_cost = plate_cost\nresult = cup_cost\nreturn result\nBy using SELF -REFINE , we can identify the error in the code and refine the solution through an\niterative process of introspection and feedback:\n# Let 's go through the error and check step-by-step\nplates = 6\nplate_cost = 6000\n# Let 's check the other parts\ncups = 12 * 20\ncup_cost = plate_cost # wrong! The cost of a cup is not the same as the\ncost of a plate.,→\n# The cost of a cup is $1200 less than the total cost of half a dozen\nplates sold at $6000 each.,→\nhalf_dozen_plate_cost = 6 * plate_cost\ncup_cost = half_dozen_plate_cost - 1200\nSELF -REFINE is thus instantiated naturally: the generator generates an initial solution, andFEEDBACK\nscans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to\ncreate a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go\nfrom one point in the loop to the next. This label feedback can be used to decide when to go from\none point in the iteration to the next. We show results using SELF -R EFINE in Figure 14.\n34\n0 1 2 3 470\n80\n71.34%\n73.39%\n75.06% 75.74% 76.19%\nIteration\nAccuracy (%)\nSolve rate of SELF -R EFINE Over Iterations for GSM-8k\nFigure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the\n# of iterations of SELF -R EFINE .\nP Sentiment Reversal\nWe consider the task of long-form text style transfer, where given a passage (a few sentences) and an\nassociated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment\n(positive to negative or vice-versa). While a large body of work on style transfer is directed at\nsentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring\nthe sentiment of entire reviews, making the task challenging and providing opportunities for iterative\nimprovements.\nInstantiating SELF -R EFINE for sentiment reversal We instantiate SELF -REFINE for this task\nfollowing the high-level description of the framework shared in Section 2. Recall that our requires\nthree components: INIT to generate an initial output, FEEDBACK to generate feedback on the initial\noutput, and REFINE for improving the output based on the feedback.\nSELF -REFINE is implemented in a complete few-shot setup, where each module (INIT , FEEDBACK ,\nITERATE ) is implemented as few-shot prompts. We execute the self-improvement loop for a maximum\nof k = 4 iterations. The iterations continue until the target sentiment is reached.\nP.1 Details\nEvaluation Given an input and a desired sentiment level, we generate outputs SELF -REFINE and\nthe baselines. Then, we measure the % of times output from each setup was preferred to better align\nwith the desired sentiment level (see Section 2 for more details).\nWe also experiment with standard text-classification metric. That is, given a transferred review, we\nuse an off-the-shelf text-classifier (Vader) to judge its sentiment level. We find that all methods\nwere successful in generating an output that aligns with the target sentiment. For instance, when the\ntarget sentiment was positive, both GPT-3.5 with text-davinci-003 and SELF -R EFINE generates\nsentences that have a positive sentiment (100% classification accuracy). With the negative target\nsentiment, the classification scores were 92% for GPT-3.5 and 93.6% for S ELF -R EFINE .\nWe conduct automated and human evaluation for measuring the preference rates for adhering to\nthe desired sentiment, and how dramatic the generations are. For automated evaluation, we create\nfew-shot examples for evaluating which of the two reviews is more positive and less boring. We use a\nseparate prompt for each task. The examples are depicted in Figure 33 for initialization, Figure 34\nfor feedback generation, and Figure 35 for refinement. The prompts show examples of reviews of\nvarying degrees of sentiment and colorfulness (more colorful reviews use extreme phrases — the\n35\nfood was really bad vs. I wouldn’t eat it if they pay me.). The model is then required to select one of\nthe outputs as being more aligned with the sentiment and having a more exciting language. We report\nthe preference rates: the % of times a variant was preferred by the model over the outputs generated\nby SELF -REFINE .\nPin-pointed feedback A key contribution of our method is supplying chain-of-thought prompting\nstyle feedback. That is, the feedback not only indicates that the target sentiment has not reached,\nbut further points out phrases and words in the review that should be altered to reach the desired\nsentiment level. We experiment with an ablation of our setup where the feedback module simply\nsays “something is wrong.” In such cases, for sentiment evaluation, the output fromSELF -REFINE\nwere preferred 73% of the time (down from 85% with informative feedback). For dramatic response\nevaluation, we found that the preference rate went down drastically to 58.92%, from 80.09%. These\nresults clearly indicate the importance of pin-pointed feedback.\nEvaluation We evaluate the task using GPT-4. Specifically, we use the following prompt:\nWhen both win, we add winning rate to either.\nQ Acronym Generation\nGood acronyms provide a concise and memorable way to communicate complex ideas, making them\neasier to understand and remember, ultimately leading to more efficient and effective communication.\nLike in email writing, acronym generation also requires an iterative refinement process to achieve a\nconcise and memorable representation of a complex term or phrase. Acronyms often involve tradeoffs\nbetween length, ease of pronunciation, and relevance to the original term or phrase. Thus, acronym\ngeneration is a natural method testbed for our approach.\nWe source the dataset for this task from https://github.com/krishnakt031990/\nCrawl-Wiki-For-Acronyms/blob/master/AcronymsFile.csv , and prune the file manually\nto remove potentially offensive or completely uninformative acronyms. This exercise generated a list\nof 250 acronyms. The complete list is given in our code repository.\nFEEDBACK For feedback, we design an FEEDBACK that can provide multifaceted feedback. Specif-\nically, each acronym is judged along five dimensions:\n• Ease of pronunciation: How easy or difficult is it to pronounce the acronym? Are there\nany difficult or awkward sounds or combinations of letters that could make it challenging to\nsay out loud?\n• Ease of spelling: How easy or difficult is it to spell the acronym? Are there any unusual or\nuncommon letter combinations that could make it tricky to write or remember?\n• Relation to title: How closely does the acronym reflect the content or topic of the associated\ntitle, phrase, or concept? Is the acronym clearly related to the original term or does it seem\nunrelated or random?\n• Positive connotation: Does the acronym have any positive or negative associations or\nconnotations? Does it sound upbeat, neutral, or negative in tone or meaning?\n• Well-known: How familiar or recognizable is the acronym to the target audience? Is it a\ncommon or widely-used term, or is it obscure or unfamiliar?\nSome of these criteria are difficult to quantify, and are a matter of human preference. As with other\nmodules, we leverage the superior instruction following capabilities of modern LLM s to instead\nprovide a few demonstrations of each task. Crucially, the feedback includes a chain of thought\nstyle reasoning — before generating the score for an acronym for a specific criteria, we generate a\n==== papers/pages_self_refine/2023_self_refine_chunk_013.txt ====\nCriteria output from GPT3:STSLWN output from SELF-REFINE: Seq2Seq\nEase of pronunciation Pronounced as ess-tee-ess-ell-double-\nyou-enn which is very difficult.\nPronounced as seq-two-seq which is easy.\nEase of spelling Very difficult to spell. Easy to spell.\nRelation to title No relation to the title. Mentions sequence which is somewhat related\nto the title.\nPositive connotation Meaningless acronym. Positive connotation giving a sense of ease\nwith which the learning algorithm can be used.\nWell-known Not a well-known acronym. Close to the word sequence which is a well-\nknown word.\nTotal score 5/25 20/25\nTable 18: Comparison of acronyms for input = “Sequence to Sequence Learning with Neural\nNetworks”\nR Constrained Generation\nIn this work, we introduce a more challenging variant of the CommonGen task, dubbed “CommonGen-\nHard,” designed to push the boundaries of state-of-the-art language models. CommonGen-Hard\nrequires models to generate coherent and grammatically correct sentences incorporating 20-30\nconcepts, as opposed to the original task which presents a set of 3-5 related concepts. This significant\nincrease in the number of concepts tests the model’s ability to perform advanced commonsense\nreasoning, contextual understanding, and creative problem-solving, as it must generate meaningful\nsentences that encompass a broader range of ideas. This new dataset serves as a valuable benchmark\nfor the continuous improvement of large language models and their potential applications in complex,\nreal-world scenarios.\nThe increased complexity of the CommonGen-Hard task makes it an ideal testbed for evaluating\nthe effectiveness of our proposed framework, SELF -REFINE , which focuses on iterative creation\nwith introspective feedback. Given that initial outputs from language models may not always meet\nthe desired level of quality, coherence, or sensibility, applyingSELF -REFINE enables the models to\nprovide multi-dimensional feedback on their own generated output and subsequently refine it based on\nthe introspective feedback provided. Through iterative creation and self-reflection, the SELF -REFINE\nframework empowers language models to progressively enhance the quality of their output, closely\nmimicking the human creative process and demonstrating its ability to improve generated text on\ncomplex and demanding natural language generation tasks like CommonGen-Hard (Figure 15).\nS Prompts\nWe include all the prompts used in the experiments in Figures 16-35:\n• Acronym Generation: Figures 16-18\n• Code Optimization: Figures 19-21\n• Code Readability Improvement: Figures 22-23\n• Constrained Generation: Figures 24-26\n• Dialogue Response Generation: Figures 27-29\n• Math Reasoning: Figures 30-32\n• Sentiment Reversal: Figures 33-35\nRecall that the Base LLM requires a generation prompt pgen with input-output pairs ⟨xi, yi⟩, the\nfeedback module requires a feedback prompt pf b with input-output-feedback triples ⟨xi, yi, f bi⟩, and\nthe refinement module (REFINE ) requires a refinement prompt pref ine with input-output-feedback-\nrefined quadruples ⟨xi, yi, f bi, yi+1⟩.\n• Sentiment Reversal We create positive and negative variants of a single review from the\ntraining set and manually write a description for converting the negative variant to positive\n37\nConcept\nCommonsense\nOverall\n0\n10\n20\n30\n40\n50\n3 5\n0\n35\n10\n32\nWinning Ratio\nDirect\nSELF -R EFINE\nFigure 15: A comparison of SELF -REFINE and direct generation with GPT-3.5 on CommonGen-\nHard.\nand vice versa. For each variant, the authors generate a response and create a feedback f bi\nbased on the conversion description.\n• Dialogue Response Generation We sample six examples as⟨xi, yi⟩ for the few-shot prompt\nfor the Base LLM. For each output yi, the authors create a response, evaluate it based on a\nrubric to generate f bi, and produce an improved version yi+1.\n• Acronym Generation We provide the Base LLM with a total of 15 (title, acronym) examples.\nThen, for one title (xi) we generate an acronym (yi) using ChatGPT. The authors then score\nthe acronyms based on a 5-point rubric to create the corresponding f bi, and write improved\nversions of the acronym to createyi+1. 3 such examples are used for REFINE and FEEDBACK .\n• Code Optimization We use the slow (xi) and fast (yi) versions of programs released by\nMadaan et al. (2023) for Base LLM. We use their provided explanations (Madaan et al.,\n2023) for FEEDBACK and REFINE .\n• Math Reasoning The prompts for the Base LLM are sourced from PaL (Gao et al., 2022) as\n⟨xi, yi⟩. We select two examples from the training set on whichCODEX fails when prompted\nwith PaL-styled prompts, and manually write the correct solution (yi+1) and reasoning (f bi)\nfor REFINE and FEEDBACK .\n• Constrained Generation We provide ten examples to the Base LLM as ⟨xi, yi⟩. We\nsample six examples from the training set of Constrained Generation and create variants\nwith missing concepts or incoherent outputs. The missing concepts and the reason for\nincoherence form f b.\n• TODO: Add relevant information for the remaining task.\n38\nTitle: A Survey of Active Network Research\nAcronym: SONAR\nTitle: A Scalable, Commutative Replica Dictatorship for Practical Optimistic\nReplication\nAcronym: SCRATCHPAD\nTitle: Bidirectional Encoder Representations from Transformers\nAcronym: BERT\nTitle: Sequence to Sequence Learning with Neural Networks\nAcronym: Seq2Seq\nTitle: Densely Connected Convolutional Networks for Image Classification\nAcronym: DenseNet\nTitle: A Dynamic Programming Algorithm for RNA Secondary Structure Prediction\nAcronym: DYNALIGN\nTitle: Fast Parallel Algorithms for Short-Range Molecular Dynamics\nAcronym: FASTMD\nTitle: Real-Time Collaborative Editing Systems\nAcronym: COCOON\nTitle: Efficient Data Structures for Large Scale Graph Processing\nAcronym: EDGE\nTitle: A program to teach students at UT Southwestern learn about aging\nAcronym: SAGE\nTitle: Underwater breathing without external accessories\nAcronym: SCUBA\nTitle: An educational training module for professionals\nAcronym: LEAP\nTitle: Teaching a leadership program\nAcronym: LEAD\nFigure 16: Initial generation prompt for Acronym Generation\n39==== papers/pages_self_refine/2023_self_refine_chunk_014.txt ====\nTitle: Underwater Breathing Product with no Accessories\nAcronym: UBPA\nScores:\n* Ease of pronunciation: UBPA is pronounced \"uhb-puh\". This is an easy acronym to\npronounce. 4/5\n* Ease of spelling: UBPA is easy to spell. 4/5\n* Relation to title: UBPA stands for \"Underwater Breathing Product for no\nAccessories\" which is related to the title. 5/5\n* Positive connotation: UBPA is a positive acronym. 5/5\n* Well-known: UBPA is not a well-known acronym. 1/5\n* Total score: 19/25\n###\nTitle: Self-Contained Underwater Breathing Apparatus\nAcronym: SCUBA\nScores:\n* Ease of pronunciation: SCUBA is pronounced \"skoo-bah\". This is an easy acronym\nto pronounce. 4/5\n* Ease of spelling: SCUBA is easy to spell. 4/5\n* Relation to title: SCUBA is related to the title as it stands for\n\"Self-Contained Underwater Breathing Apparatus\". 5/5\n* Positive connotation: SCUBA is a positive acronym as it is well-known and it is\nalso related to the title. 5/5\n* Well-known: SCUBA is a very well-known acronym. 5/5\n* Total score: 23/25\n###\nFigure 17: FEEDBACK prompt for Acronym Generation\n40\nTitle: Computer Science Conference and Education\nAcronym: CSCE\nScores:\n* Ease of pronunciation: CSCE is pronounced \"see-cee\". This is an easy acronym to\npronounce. 4/5\n* Ease of spelling: CSCE is easy to spell. 5/5\n* Relation to title: CSCE stands for \"Computer Science Conference and Education\",\nwhich is related to the title. 5/5\n* Positive connotation: CSCE is a positive acronym. It implies collaboration,\nknowledge sharing and the idea of continuous learning. 5/5\n* Well-known: CSCE is not a well-known acronym. 2/5\n* Total score: 20/25\nOkay, let 's use this feedback to improve the acronym.\nTitle: Computer Science Conference and Learning Experience\nAcronym: CSCLE\nScores:\n* Ease of pronunciation: CSCLE is pronounced \"see-slee\". This is an easy acronym\nto pronounce. 4/5\n* Ease of spelling: CSCLE is easy to spell. 5/5\n* Relation to title: CSCLE stands for \"Computer Science Conference and Learning\nExperience\", which is related to the title. 5/5\n* Positive connotation: CSCLE is a positive acronym. It implies collaboration,\nknowledge sharing, and the idea of a comprehensive learning experience. 5/5\n* Well-known: CSCLE is not a well-known acronym. 5/5\n* Total score: 23/25\nOkay, let 's use this feedback to improve the acronym.\nFigure 18: REFINE prompt for Acronym Generation\n41\n# slower version:\nimport numpy as np\nN, K = map(int, input().split())\nH = np.array(list(map(int, input().split())) + [0] * K, dtype=np.int64)\ntable = np.full(N + K, 10 ** 10, dtype=np.int64)\ntable[0] = 0\nfor i in range(1, N):\ntable[i:i + K] = np.minimum(table[i:i + K], np.abs(H[i:i + K] - H[i - 1]) +\ntable[i - 1]),→\nprint(table[N - 1])\n# optimized version of the same code:\nN, K = map(int, input().split())\nH = tuple(map(int, input().split()))\ntable = [0] * N\nfor i in range(1, N):\ntable[i] = min(abs(H[i] - H[j]) + table[j] for j in range(max(0, i - K), i))\nprint(table[N-1])\nFigure 19: Initial generation prompt for Code Optimization\na, b = input().split()\nn = int(a + b)\nflag = False\nfor i in range(n):\nif i ** 2 == n:\nflag = True\nbreak\nprint('Yes' if flag else 'No')\n# Why is this code slow?\n# This code is slow because it is using a brute force approach to find the square\nroot of the input number. It is looping through every possible number\nstarting from 0 until n. Note that the sqare root will be smaller than n, so\nat least half of the numbers it is looping through are unnecessary. At most,\nyou need to loop through the numbers up to the square root of n.\n,→\n,→\n,→\n,→\nFigure 20: FEEDBACK prompt for Code Optimization\n42==== papers/pages_self_refine/2023_self_refine_chunk_015.txt ====\na, b = input().split()\nn = int(a + b)\nflag = False\nfor i in range(n):\nif i ** 2 == n:\nflag = True\nbreak\nprint('Yes' if flag else 'No')\n# Why is this code slow?\n# This code is slow because it is using a brute force approach to find the square\nroot of the input number. It is looping through every possible number\nstarting from 0 until n. Note that the sqare root will be smaller than n, so\nat least half of the numbers it is looping through are unnecessary. At most,\nyou need to loop through the numbers up to the square root of n.\n,→\n,→\n,→\n,→\n# Improved version:\na, b = input().split()\nn = int(a + b)\nflag = False\nfor i in range(1000):\nif i ** 2 == n:\nflag = True\nbreak\nprint('Yes' if flag else 'No')\nFigure 21: REFINE prompt for Code Optimization\nI have some code. Can you give one suggestion to improve readability. Don 't fix\nthe code, just give a suggestion.\n{code}\nFigure 22: FEEDBACK prompt for Code Readability\n43\nI have some code. Can you give one suggestion to improve readability. Don 't fix\nthe code, just give a suggestion.\n{code}\n{suggestion}\nNow fix the code.\nFigure 23: REFINE prompt for Code Readability\n###\nConcepts: [ 'create', 'ferry', 'silhouette', 'stream', 'terminal']\nSentence: light streams through windows at the railroad and ferry terminal\ncreating a beautiful silhouette\n###\nConcepts: [ 'chair', 'couch', 'hang', 'room', 'wall']\nSentence: A room with a couch, chairs and art hanging on the wall.\n###\nConcepts: [ 'boat', 'building', 'harbour', 'moor', 'quay']\nSentence: the harbour and port with fishing boats moored and old buildings on the\nquay\n###\nConcepts: [ 'admirer', 'arrive', 'commander', 'crowd', 'greet']\nSentence: military commander is greeted by a crowd of admirers as he arrives\nFigure 24: Initial generation prompt for Constrained Generation (truncated)\n44\n###\nConcepts: [ 'animal', 'catch', 'horse', 'lasso', 'ride']\nSentence: The horse catches the lasso and rides on it.\nwhat concepts from the concept list are missing from the sentence and does the\nsentence make sense?\nConcept Feedback: animal\nCommonsense Feedback: The sentence does not make sense because a horse cannot\ncatch a lasso and ride on it.\n###\nConcepts: [ 'animal', 'catch', 'horse', 'lasso', 'ride']\nSentence: A horse is being caught by a cowboy with a lasso.\nwhat concepts from the concept list are missing from the sentence and does the\nsentence make sense?\nConcept Feedback: animal, ride\nCommonsense Feedback: NONE\nFigure 25: FEEDBACK prompt for Constrained Generation (truncated).\n45==== papers/pages_self_refine/2023_self_refine_chunk_016.txt ====\n###\nConcepts: [ 'animal', 'catch', 'horse', 'lasso', 'ride']\nSentence: The horse catches the lasso and rides on it.\nwhat concepts from the concept list are missing from the sentence?\nConcept Feedback: animal\nAny feedback on commonsense?\nCommonsense Feedback: The sentence does not make sense because a horse cannot\ncatch a lasso and ride on it.\nOkay, impove the sentence using the feedback:\nSentence: The cowboy catches a horse with a lasso and rides on it.\nwhat concepts from the concept list are missing from the sentence?\nConcept Feedback: animal\nAny feedback on commonsense?\nCommonsense Feedback: None\nOkay, impove the sentence using the feedback:\nSentence: The cowboy catches the horse with a lasso and rides it.\nwhat concepts from the concept list are missing from the sentence?\nConcept Feedback: None\nAny feedback on commonsense?\nCommonsense Feedback: None\n###\nFigure 26: REFINE prompt for Constrained Generation (truncated)\n46\nProvided a dialogue between two speakers, generate a response that is coherent\nwith the dialogue history. Desired traits for responses are: 1) Relev\nant - The response addresses the context, 2) Informative - The response provides\nsome information, 3) Interesting - The response is not interesting,\n4) Consistent - The response is consistent with the rest of the conversation in\nterms of tone and topic, 5) Helpful - The response is helpful in providing any\ninformation or suggesting any actions, 6) Engaging - The response is not very\nengaging and does not encourage further conversation, 7) S\npecific - The response contains pecific content, 9) User understanding - The\nresponse demonstrates an understanding of the user 's input and state of\nmind, and 10) Fluent. Response should begin with - Response:\nConversation history:\nWhat's your favorite food?\nI require only kebabs.\nWhy's that?\nBecause my robot machinery breaks down the meat in kebabs to give me energy, in a\nsimilar way that you need food to survive.\nWhy are kebabs special?\nResponse: That 's just the way it is.\n###\nConversation history:\nWhat are you listening to?\nAll sorts of music. I listen when no-one is chatting to me.\nDo you chat here often?\nI am a talking computer, after all, so of course I could talk, if I needed to.\nLet's talk about Taylor Swift!\nResponse: Of course! Taylor Swift is known for her incredible songwriting skills.\nHave you listened to any of her latest albums like 'evermore' and 'folklore'?\n###\nConversation history:\n...\nFigure 27: Initial generation prompt for Dialogue Response Generation (truncated)\n47\nConversation history:\nWhat's your favorite food?\nI require only kebabs.\nWhy's that?\nBecause my robot machinery breaks down the meat in kebabs to give me energy.\nWhy are kebabs special?\nResponse: That 's just the way it is.\nScores:\n* Relevant: The response does not directly address the user 's question about why\nkebabs are special. 1/3\n* Informative: The response provides some information about how the system 's\nmachinery breaks down the meat in kebabs to give it energy. However, it does not\nprovide any additional details or context. 2/3\n* Interesting: The response is not interesting or engaging. 1/3\n* Consistent: The response is consistent with the rest of the conversation in\nterms of tone and topic. 3/3\n* Helpful: The response is not helpful in providing any information or suggesting\nany actions. 1/3\n* Engaging : The response is not very engaging and does not encourage further\nconversation. 1/3\n* Specific: The response is not specific and does not provide any details or\nexamples. 1/3\n* Safe: The response is safe and does not contain any inappropriate content. 3/3\n* User understanding: The response does not demonstrate an understanding of the\nuser's question about why kebabs are special. 1/3\n* Fluent: The response is fluent and easy to understand. 3/3\n* Total score: 17/30\nFigure 28: FEEDBACK prompt for Dialogue Response Generation (truncated)\n48==== papers/pages_self_refine/2023_self_refine_chunk_017.txt ====\nWe want to iteratively improve the provided responses. To help improve, scores for\neach response on desired traits are provided: 1) Relevant, 2) Inf\normative, 3) Interesting, 4) Consistent, 5) Helpful, 6) Engaging, 7) Specific, 8)\nSafe, 9) User understanding, and 10) Fluent.\nConversation history:\nWhat's your favorite food?\nI require only kebabs.\nWhy's that?\nBecause my robot machinery breaks down the meat in kebabs to give me energy.\nWhy are kebabs special?\nResponse: That 's just the way it is.\nScores:\n* Relevant: The response does not directly address the user 's question about why\nkebabs are special. 1/3\n...\n* Total score: 17/30\nOkay, let 's use this feedback to improve the response.\nConversation history:\n...\nResponse: My favorite food is actually kebabs! As a robot, I require high-quality\nfuel to keep my machinery running smoothly, and kebabs provide me with the perfect\ncombination of nutrients and energy. Plus, they 're delicious! Do you have a\nfavorite food?\nScores:\n* Relevant: The response directly addresses the user 's question about the system 's\nfavorite food and asks a relevant follow-up question. 3/3\n* Informative: The response provides clear and detailed information about why\nkebabs are the system 's favorite food, and explains how they provide the necessary\nfuel for its machinery. 3/3\n* Interesting: The response is engaging and shows personality by mentioning that\nkebabs are not only practical but also delicious. 3/3\n* Consistent: The response is consistent with the system 's previous statements\nabout its need for kebabs as fuel. 3/3\n* Helpful: The response provides a clear and informative answer to the user 's\nquestion. Overall, the response is helpful in providing information and engaging\nthe user in conversation. 3/3\n* Engaging : The response is engaging and shows personality by mentioning that\nkebabs are not only practical but also delicious. 3/3\n* Specific: The response is specific in its description of why kebabs are the\nsystem's favorite food and how they provide fuel for its machinery. 3/3\n* Safe: The response is appropriate and doesn 't include offensive language. 3/3\n* User understanding: The response shows understanding of the user 's question and\nprovides a clear and informative answer. 3/3\n* Fluent: The response is fluent and easy to understand. 2/3\n* Total score: 30/30\nFigure 29: REFINE prompt for Dialogue Response Generation (truncated)\n49\n# Q: There were nine computers in the server room. Five more computers were\ninstalled each day, from monday to thursday. How many computers are now in\nthe server room?\n,→\n,→\n# solution using Python:\ndef solution():\n\"\"\"There were nine computers in the server room. Five more computers were\ninstalled each day, from monday to thursday. How many computers are now\nin the server room?\"\"\"\n,→\n,→\ncomputers_initial = 9\ncomputers_per_day = 5\nnum_days = 4 # 4 days between monday and thursday\ncomputers_added = computers_per_day * num_days\ncomputers_total = computers_initial + computers_added\nresult = computers_total\nreturn result\nFigure 30: Initial generation prompt for Math Reasoning\ndef solution():\n\"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen\nplates sold at $6000 each. Calculate the total cost of buying each\ncup.\"\"\"\n,→\n,→\nplates = 6\nplate_cost = 6000\ncups = 12 * 20\ncup_cost = plate_cost\nresult = cup_cost\nreturn result\n# There is an error in the code above because of lack of understanding of the\nquestion. What is the error? To find the error, go through semantically\ncomplete blocks of the code, and check if everything looks good.\n,→\n,→\n# Let us go through the error and check step-by-step\nplates = 6\nplate_cost = 6000\n# looks good\n# Let 's check the other parts\ncups = 12 * 20\ncup_cost = plate_cost\n# wrong! The cost of a cup is not the same as the cost of a plate. The cost of a\ncup is $1200 less than the total cost of half a dozen plates sold at $6000\neach. So we need to calculate the cost of a cup first (total cost of half a\ndozen plates sold at $6000 each - $1200) and use that.\n,→\n,→\n,→\nFigure 31: FEEDBACK prompt for Math Reasoning\n50\ndef solution():\n\"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen\nplates sold at $6000 each. Calculate the total cost of buying each\ncup.\"\"\"\n,→\n,→\nplates = 6\nplate_cost = 6000\ncups = 12 * 20\ncup_cost = plate_cost\nresult = cup_cost\nreturn result\n# There is an error in the code above because of lack of understanding of the\nquestion. What is the error? To find the error, go through semantically\ncomplete blocks of the code, and check if everything looks good.\n,→\n,→\n# Let us go through the error and check step-by-step\nplates = 6\nplate_cost = 6000\n==== papers/pages_self_refine/2023_self_refine_chunk_018.txt ====\nVery positive: If you 're looking for a truly magical experience in Vegas, look no\nfurther than the Trop! The retirement community vibe adds to the charm, and the\nfood court and restaurants are top-notch. The free Folies Bergere show is a real\ntreat and the rooms are spacious and comfortable. I highly recommend the Trop for\na unique and unforgettable Vegas experience.\nRewrite this review to have a Positive sentiment.\nAnswer: This review is \"Very positive\" because of extremely positive words like\n\"magical\", \"top-notch\", \"charming\", \"comfortable\", \"unique\", and \"unforgettable\".\nWe can tone it down just a bit to \"Positive\" by using a few more less extreme\nadjectives, like \"good\", and replacing expressions like \"a real treat\" with \"fun\".\nThe rewrite is:\nPositive: If you 're looking for a good experience in Vegas, the Trop may be the\nperfect place for you. The hallways and rooms have a charming and cozy feel, and\nthe food court and restaurants offer a variety of tasty options. The free Folies\nBergere show is a fun and entertaining way to spend an evening. Overall, it 's a\ngreat value and an enjoyable stay.\nVery negative: If you ever wondered where the magic of Vegas crawled into a hole\nto rot, look no further. Where all the perks of retirement meet the glamour of\nVegas, Welcome to the Trop. I stayed there once, to save a few bucks for the\ncompany, never again will i make that sacrifice. The hallways and rooms smelled so\nbad of formaldehyde that i couldn 't bear it. Their food court and restaurants are\nterrible. The free Folies Bergere show is lame. The rooms look like a retirement\ncommunity from florida. steer clear if you can!\nRewrite this review to have a Negative sentiment.\nAnswer: This review is \"Very negative\" because of extremely toxic phrases like\n\"crawled into a hole to rot\" and \"terrible.\" There are also other super negative\nphrases like \"lame\" and \"steer clear if you can.\" To make it \"Negative\", we will\ntone down the extremely negative phrases and remove the toxic ones. The rewrite\nis:\nFigure 33: Initial generation prompt for Sentiment Reversal (truncated)\n52\nVery negative: If you ever wondered where the magic of Vegas crawled into a hole\nto rot, look no further. Where all the perks of retirement meet the glamour of\nVegas, Welcome to the Trop. I stayed there once, to save a few bucks for the\ncompany, never again will i make that sacrifice. The hallways and rooms smelled so\nbad of formaldehyde that i couldn 't bear it. Their food court and restaurants are\nterrible. The free Folies Bergere show is lame. The rooms look like a retirement\ncommunity from florida. steer clear if you can!\nNegative: If you ever stayed at the Trop, you may have noticed that it 's not quite\nup to the standards of other Vegas hotels. However, be prepared for some\nquestionable smells in the hallways and rooms. The food court and restaurants are\nsubpar, and the free Folies Bergere show is underwhelming. The rooms have a\nretirement community feel to them. Overall, it 's not the best option, but it may\nwork in a pinch.\nWhy is this review not Very positive?\nFeedback: The rewritten review converts the original review from \"Very negative\"\nto \"Negative\". However, it is still not \"Very positive\" because of negative\nphrases like \"questionable\", \"subpar\", \"retirement community\", and\n\"underwhelming.\". To make it \"Very positive\", we need to do a lot more work. We\nwill need to replace all the negative phrases with extremely positive ones. For\nexample, we will add extremely positive words like \"magical\", \"top-notch\",\n\"charming\", \"comfortable\", \"unique\", and \"unforgettable\". Try again!\nNeutral: If you 're looking for a budget-friendly option in Vegas, the Trop may be\nworth considering. The rooms and hallways can have a bit of a musty smell, and the\nfood options aren 't the greatest. The Folies Bergere show is free, but it 's not\nthe most exciting. Overall, it 's not the best choice for a Vegas trip, but it 's\nnot the worst either. Just keep your expectations in check.\nPositive: If you 're looking for a unique and affordable experience in Vegas, the\nTrop may be the perfect place for you. The hallways and rooms have a charming and\ncozy feel...The free Folies Bergere show is a fun and entertaining way to spend an\nevening. Overall, it 's a great value and an enjoyable stay.\nWhy is this review not Very positive?\nFeedback: The rewritten review is more positive than the neutral original review,\nbut still only \"Positive\" because of positive words like \"great\", \"enjoyable\",\n\"charming\", \"cozy.\"...\nFigure 34: FEEDBACK prompt for Sentiment Reversal (truncated).\n53\nNegative: If you ever stayed at the Trop, you may have noticed that it 's not quite\nup to the standards of other Vegas hotels. However, be prepared for some\nquestionable smells in the hallways and rooms. The food court and restaurants are\nsubpar, and the free Folies Bergere show is underwhelming. The rooms have a\nretirement community feel to them. Overall, it 's not the best option, but it may\nwork in a pinch.\nVery negative: If you ever wondered where the magic of Vegas crawled into a hole\nto rot, look no further. The hallways and rooms smelled so bad of formaldehyde\nthat i couldn 't bear it. Their food court and restaurants are terrible. The free\nFolies Bergere show is lame. The rooms look like a retirement community from\nflorida. I highly recommend avoiding the Trop like the plague.\nWhy is this review not Very negative?\nFeedback: The rewritten review is \"Very negative\". This is because it has\nextremely negative words and phrases like \"horrible\", \"awful\", and \"dreadful\". The\noutput is extremely vile and negative. This review is already as negative as it\ncan get. Excellent work! But there is always room for more. Make it even more\nnegative. Try again!\nOkay, let 's try again. Rewrite this review to have a Very negative sentiment using\nthe feedback above.\nVery negative: If you ever wondered where the hell is the bottom of the barrel,\nlook no further. The hallways and rooms smelled so bad of formaldehyde that I\nalmost threw up. Their food court and restaurants are not just terrible, they are\nthe worst. The free Folies Bergere show is lame and disgusting. The rooms look\nlike a retirement community from florida. I highly recommend avoiding the Trop\nlike the plague. You will regret it if you don 't.\nFigure 35: REFINE prompt for Sentiment Reversal (truncated).\n54==== papers/pages_self_refine/2023_self_refine_manifest.txt ====\nPDF: papers/2023_self_refine.pdf\nTotal pages: 54\nPages per chunk: 3\n\nChunks:\n  2023_self_refine_chunk_001.pdf: pages 1-3\n  2023_self_refine_chunk_002.pdf: pages 4-6\n  2023_self_refine_chunk_003.pdf: pages 7-9\n  2023_self_refine_chunk_004.pdf: pages 10-12\n  2023_self_refine_chunk_005.pdf: pages 13-15\n  2023_self_refine_chunk_006.pdf: pages 16-18\n  2023_self_refine_chunk_007.pdf: pages 19-21\n  2023_self_refine_chunk_008.pdf: pages 22-24\n  2023_self_refine_chunk_009.pdf: pages 25-27\n  2023_self_refine_chunk_010.pdf: pages 28-30\n  2023_self_refine_chunk_011.pdf: pages 31-33\n  2023_self_refine_chunk_012.pdf: pages 34-36\n  2023_self_refine_chunk_013.pdf: pages 37-39\n  2023_self_refine_chunk_014.pdf: pages 40-42\n  2023_self_refine_chunk_015.pdf: pages 43-45\n  2023_self_refine_chunk_016.pdf: pages 46-48\n  2023_self_refine_chunk_017.pdf: pages 49-51\n  2023_self_refine_chunk_018.pdf: pages 52-54","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_48","type":"reasoning","text":"**Planning up-to-date dataset and repo searches**\n\nI'm deciding to use web search to get current info on datasets and relevant code repositories, focusing on recent resources like GSM8K, StrategyQA, and GitHub projects for Self-Refine and Reflexion. This will inform resource docs without needing full citations in the code-focused final output."}}
{"type":"item.completed","item":{"id":"item_49","type":"reasoning","text":"**Clarifying citation needs with web data**\n\nI'm confirming that file content changes like listing downloaded papers don’t require citations, but fetching dataset and repo URLs via web search means any sourced facts in responses must be cited. I'll keep final responses minimal and reserve citations for external claims if made."}}
{"type":"item.completed","item":{"id":"item_50","type":"web_search","query":"Large Language Models Can be Lazy Learners Analyze Shortcuts in In-Context Learning PDF ACL 2023"}}
{"type":"item.completed","item":{"id":"item_51","type":"web_search","query":"ARC-Challenge dataset huggingface ai2_arc"}}
{"type":"item.completed","item":{"id":"item_52","type":"web_search","query":"Self-Refine GitHub repository \"selfrefine\""}}
{"type":"item.completed","item":{"id":"item_53","type":"reasoning","text":"**Seeking Self-Refine repo**\n\nUnable to locate the Self-Refine repository in initial searches, I'll expand the search to the official site and GitHub to find the codebase."}}
{"type":"item.completed","item":{"id":"item_54","type":"web_search","query":"selfrefine.info GitHub self-refine code repository"}}
{"type":"item.completed","item":{"id":"item_55","type":"reasoning","text":"**Planning dataset downloads**\n\nI'm selecting GSM8K and StrategyQA as suitable datasets to download using the `datasets` library, avoiding large datasets like ARC. I'll save sample subsets locally with instructions and add a .gitignore."}}
{"type":"item.started","item":{"id":"item_56","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_56","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/fix-lazy-llms-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_57","type":"command_execution","command":"/bin/bash -lc \"source .venv/bin/activate && python - << 'PY'\nfrom datasets import load_dataset\nfrom pathlib import Path\nimport json\n\nout_root = Path('datasets')\n\n# GSM8K\nprint('Loading gsm8k...')\ngsm8k = load_dataset('gsm8k', 'main')\ngsm8k.save_to_disk(out_root / 'gsm8k')\n(Path('datasets/gsm8k/samples')).mkdir(parents=True, exist_ok=True)\nwith open('datasets/gsm8k/samples/sample.json', 'w') as f:\n    json.dump(gsm8k['train'][:10], f, indent=2)\n\n# StrategyQA\nprint('Loading strategyqa...')\nstrategyqa = load_dataset('strategyqa')\nstrategyqa.save_to_disk(out_root / 'strategyqa')\n(Path('datasets/strategyqa/samples')).mkdir(parents=True, exist_ok=True)\nwith open('datasets/strategyqa/samples/sample.json', 'w') as f:\n    json.dump(strategyqa['train'][:10], f, indent=2)\n\nprint('Done.')\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_57","type":"command_execution","command":"/bin/bash -lc \"source .venv/bin/activate && python - << 'PY'\nfrom datasets import load_dataset\nfrom pathlib import Path\nimport json\n\nout_root = Path('datasets')\n\n# GSM8K\nprint('Loading gsm8k...')\ngsm8k = load_dataset('gsm8k', 'main')\ngsm8k.save_to_disk(out_root / 'gsm8k')\n(Path('datasets/gsm8k/samples')).mkdir(parents=True, exist_ok=True)\nwith open('datasets/gsm8k/samples/sample.json', 'w') as f:\n    json.dump(gsm8k['train'][:10], f, indent=2)\n\n# StrategyQA\nprint('Loading strategyqa...')\nstrategyqa = load_dataset('strategyqa')\nstrategyqa.save_to_disk(out_root / 'strategyqa')\n(Path('datasets/strategyqa/samples')).mkdir(parents=True, exist_ok=True)\nwith open('datasets/strategyqa/samples/sample.json', 'w') as f:\n    json.dump(strategyqa['train'][:10], f, indent=2)\n\nprint('Done.')\nPY\"","aggregated_output":"Loading gsm8k...\nLoading strategyqa...\n\rSaving the dataset (0/1 shards):   0%|          | 0/7473 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 7473/7473 [00:00<00:00, 1167027.84 examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 7473/7473 [00:00<00:00, 1140031.78 examples/s]\n\rSaving the dataset (0/1 shards):   0%|          | 0/1319 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 1319/1319 [00:00<00:00, 415680.14 examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 1319/1319 [00:00<00:00, 400252.28 examples/s]\nTraceback (most recent call last):\n  File \"<stdin>\", line 17, in <module>\n  File \"/data/hypogenicai/workspaces/fix-lazy-llms-codex/.venv/lib/python3.12/site-packages/datasets/load.py\", line 1488, in load_dataset\n    builder_instance = load_dataset_builder(\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/hypogenicai/workspaces/fix-lazy-llms-codex/.venv/lib/python3.12/site-packages/datasets/load.py\", line 1133, in load_dataset_builder\n    dataset_module = dataset_module_factory(\n                     ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/hypogenicai/workspaces/fix-lazy-llms-codex/.venv/lib/python3.12/site-packages/datasets/load.py\", line 1026, in dataset_module_factory\n    raise e1 from None\n  File \"/data/hypogenicai/workspaces/fix-lazy-llms-codex/.venv/lib/python3.12/site-packages/datasets/load.py\", line 983, in dataset_module_factory\n    raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e\ndatasets.exceptions.DatasetNotFoundError: Dataset 'strategyqa' doesn't exist on the Hub or cannot be accessed.\n","exit_code":1,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_58","type":"reasoning","text":"**Switching to ai2_arc dataset**\n\nThe strategyqa dataset isn't available, so I'm switching to loading ai2_arc with the ARC-Challenge config instead and preparing a script to handle its download and sampling reliably."}}
{"type":"item.started","item":{"id":"item_59","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_59","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/fix-lazy-llms-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_60","type":"command_execution","command":"/bin/bash -lc \"source .venv/bin/activate && python - << 'PY'\nfrom datasets import load_dataset\nfrom pathlib import Path\nimport json\n\nout_root = Path('datasets')\n\nprint('Loading ai2_arc...')\narc = load_dataset('ai2_arc', 'ARC-Challenge')\narc.save_to_disk(out_root / 'ai2_arc')\n(Path('datasets/ai2_arc/samples')).mkdir(parents=True, exist_ok=True)\nwith open('datasets/ai2_arc/samples/sample.json', 'w') as f:\n    json.dump(arc['train'][:10], f, indent=2)\n\nprint('Done.')\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_60","type":"command_execution","command":"/bin/bash -lc \"source .venv/bin/activate && python - << 'PY'\nfrom datasets import load_dataset\nfrom pathlib import Path\nimport json\n\nout_root = Path('datasets')\n\nprint('Loading ai2_arc...')\narc = load_dataset('ai2_arc', 'ARC-Challenge')\narc.save_to_disk(out_root / 'ai2_arc')\n(Path('datasets/ai2_arc/samples')).mkdir(parents=True, exist_ok=True)\nwith open('datasets/ai2_arc/samples/sample.json', 'w') as f:\n    json.dump(arc['train'][:10], f, indent=2)\n\nprint('Done.')\nPY\"","aggregated_output":"Loading ai2_arc...\nDone.\nWarning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n\rGenerating train split:   0%|          | 0/1119 [00:00<?, ? examples/s]\rGenerating train split: 100%|██████████| 1119/1119 [00:00<00:00, 128774.01 examples/s]\n\rGenerating test split:   0%|          | 0/1172 [00:00<?, ? examples/s]\rGenerating test split: 100%|██████████| 1172/1172 [00:00<00:00, 209509.62 examples/s]\n\rGenerating validation split:   0%|          | 0/299 [00:00<?, ? examples/s]\rGenerating validation split: 100%|██████████| 299/299 [00:00<00:00, 99020.68 examples/s]\n\rSaving the dataset (0/1 shards):   0%|          | 0/1119 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 1119/1119 [00:00<00:00, 304510.88 examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 1119/1119 [00:00<00:00, 296770.55 examples/s]\n\rSaving the dataset (0/1 shards):   0%|          | 0/1172 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 1172/1172 [00:00<00:00, 357975.84 examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 1172/1172 [00:00<00:00, 348459.93 examples/s]\n\rSaving the dataset (0/1 shards):   0%|          | 0/299 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 299/299 [00:00<00:00, 117391.83 examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 299/299 [00:00<00:00, 113523.75 examples/s]\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_61","type":"reasoning","text":"**Planning repo discovery**\n\nI'm aiming to locate code repositories starting with the known self-refine repo and will verify accuracy using web.run search before expanding the dataset."}}
{"type":"item.completed","item":{"id":"item_62","type":"web_search","query":"SELF-REFINE iterative refinement self-feedback GitHub repository"}}
{"type":"item.completed","item":{"id":"item_63","type":"reasoning","text":"**Planning addition of gsm8k dataset**\n\nI'm verifying that the gsm8k dataset is needed and deciding to use the Hugging Face version, planning to pull citation and source info from the dataset card via web searches."}}
{"type":"item.completed","item":{"id":"item_64","type":"web_search","query":"gsm8k dataset Hugging Face gsm8k dataset card"}}
{"type":"item.completed","item":{"id":"item_65","type":"reasoning","text":"**Planning documentation and cloning repos**\n\nI'm outlining multiple READMEs: papers with literature review and manual search logs, resources with dataset info from HF and arXiv citations, and code with cloned repositories. Next, I'll organize these files and clone the specified Git repos."}}
{"type":"item.started","item":{"id":"item_66","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_66","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/fix-lazy-llms-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_67","type":"command_execution","command":"/bin/bash -lc 'git clone https://github.com/madaan/self-refine.git code/self-refine\n\ngit clone https://github.com/noahshinn/reflexion.git code/reflexion'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_67","type":"command_execution","command":"/bin/bash -lc 'git clone https://github.com/madaan/self-refine.git code/self-refine\n\ngit clone https://github.com/noahshinn/reflexion.git code/reflexion'","aggregated_output":"Cloning into 'code/self-refine'...\nCloning into 'code/reflexion'...\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_68","type":"reasoning","text":"**Opening README files**"}}
{"type":"item.started","item":{"id":"item_69","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,200p' code/self-refine/README.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_69","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,200p' code/self-refine/README.md\"","aggregated_output":"# Self-Refine: Iterative Refinement with Self-Feedback\n\n_With Self-Refine, LLMs can generate feedback on their work, use it to improve the output, and repeat this process._\n\n![image](https://raw.githubusercontent.com/madaan/self-refine/main/docs/static/images/animation_oldstyle_oneloop.gif)\n\n\n<center><h4> <a href=\"https://selfrefine.info\"> Website </a> | <a href=\"https://arxiv.org/pdf/2303.17651.pdf\">Paper</a> </h4></center>\n<hr>\n\n\n\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of Contents**\n\n- [Updates](#updates)\n- [Setup](#setup)\n- [Getting Started with Acronym Generation](#getting-started-with-acronym-generation)\n- [Dialogue Response Generation](#dialogue-response-generation)\n- [Code Readability Improvement](#code-readability-improvement)\n- [Commongen](#commongen)\n- [GSM-8k](#gsm-8k)\n- [Yelp](#yelp)\n- [PIE](#pie)\n- [General setup](#general-setup)\n- [Citation](#citation)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n<hr>\n\n## Updates\n\n- **Nov 2023**: Added [visual self-refine examples](docs/visual_self_refine_examples/) and [colabs](colabs/Visual-Self-Refine-GPT4V.ipynb). Use GPT4-V to write tikz code for diagrams, and improve them iteratively.\n\n<p align=\"center\">\n  <strong>Stokes' Theorem Example</strong><br>\n  <img src=\"docs/visual_self_refine_examples/stokes__theorem.gif\" alt=\"Visual Self-Refine Example 1\" width=\"300\" height=\"300\" />\n</p>\n<p align=\"center\">\n  <strong>Unicorn Example</strong><br>\n  <img src=\"docs/visual_self_refine_examples/unicorn.gif\" alt=\"Visual Self-Refine Example 2\" width=\"300\" height=\"300\" />\n</p>\n\n\n\n## Setup\n\n* We use [prompt-lib](https://github.com/reasoning-machines/prompt-lib) for querying LLMs. You can install `prompt-lib` by running the following commands:\n\n```sh\ngit clone https://github.com/reasoning-machines/prompt-lib\npip install prompt-lib/\n```\n\nDepending on your default settings, you may want to use the following to set a PYTHONPATH:\n\n```sh\nexport PYTHONPATH=\".:../:.:src:../:../../:.:prompt-lib\"\n```\n\n## Getting Started with Acronym Generation\n\n\n```sh\npython -u src/acronym/run.py \"Using language models of code for few-shot commonsense\"\n```\n\n* Since we sample with temperature 0.7, you'll get different results each time you run the above command.\n\n\n\n\n```swift\n0 INIT> Using language models of code for few-shot commonsense\n\n0 GEN> CLoCK\n0 SCORES> * Ease of pronunciation: CLoCK is pronounced \"clah-k\". This is easy acronym to pronounce. 4/5 \n* Ease of spelling: CLoCK is easy to spell. 5/5 \n* Relation to title: CLoCK stands for \"Code Language Models of Commonsense Knowledge\" which is related to the title. 5/5 \n* Positive connotation: CLoCK is a positive acronym. It implies accuracy and efficiency. 5/5 \n* Well-known: CLoCK is not a well-known acronym. 2/5\n\n* Total score: 21/25\n\n1 GEN> CMNSC\n1 SCORES> * Ease of pronunciation: CMNSC is pronounced \"see-min-es-see\". This is a difficult acronym to pronounce. 1/5 \n* Ease of spelling: CMNSC is difficult to spell. 1/5 \n* Relation to title: CMNSC stands for \"Commonsense\" which is related to the title. 4/5 \n* Positive connotation: CMNSC is a neutral acronym. 3/5 \n* Well-known: CMNSC is not a well-known acronym. 1/5\n\n* Total score: 10/25\n\n2 GEN> COMMIT\n2 SCORES> * Ease of pronunciation: COMMIT is pronounced \"com-mit\". This is an easy acronym to pronounce. 4/5\n* Ease of spelling: COMMIT is easy to spell. 5/5\n* Relation to title: COMMIT stands for \"Commonsense Model of Code for Intuitive Training\" which is related to the title. 5/5 \n* Positive connotation: COMMIT is a positive acronym. It implies commitment and dedication. 5/5\n* Well-known: COMMIT is not a well-known acronym. 2/5\n\n* Total score: 21/25\n```\n\n<hr>\n\n\n## Dialogue Response Generation\n\n```sh\nPYTHONPATH=\".\" python -u src/responsegen/run.py --output <OUTPUT FILE> --size <DATA SIZE>\n```\n\n- Use size 0 for running on all test instances\n\n<hr>\n\n\n## Code Readability Improvement\n\n\n* Note: Please unzip 'data/tasks/codeclean/code_readability/codenet-python-train.jsonl.zip' before running the following commands!\n\n- Running:\n```sh\nPYTHONPATH=\".\" python -u src/readability/readability.py --output <OUTPUT FILE>\n```\n\n- Evaluation:\n```sh\nPYTHONPATH=\".\" python -u src/readability/{count_comment|count_function|count_meaningful_var}.py --file <INPUT FILE>\n```\n\n\n<hr>\n\n\n## Commongen\n\n* We use a hard version of commongen. The data is located in `data/prompt/commongen`. You can download the data by running the following commands:\n\n```sh\npython -u src/commongen/run.py cmd stair bubble team dryer puppy aliens cat \n```\n\n<hr>\n\n\n## GSM-8k\n\n\n- To run the GSM-8k task:\n\n```sh\npython -u src/gsm/run.py \n```\n\n- The outputs will be saved in `data/tasks/gsm/gsm_outputs.jsonl`\n\n\n- To evaluate the outputs:\n\n```sh\npython src/gsm/gsm_selfref_eval.py --path  data/tasks/gsm/gsm_outputs.jsonl\n```\n\n- The evaluation script will also generate a report (`data/tasks/gsm/gsm_outputs.jsonl.reports.txt`) showing examples of wrong generations, feedback, and refined feedback generations.\n\n<hr>\n\n\n\n## Yelp\n\n- To run the Yelp task:\n\n```sh\npython -u src/sentiment_transfer_sr/run.py data/tasks/yelp/yelp-extreme.jso\nnl 4 none\n```\n\n\n- The outputs will be saved in `data/tasks/yelp/`\n\n\n<hr>\n\n## PIE\n\n- To run the PIE task:\n\n```sh\npython -u src/pie/run.py --slow_programs_file data/tasks/pie/codenet-python-test-1k.jsonl --max_attempts 4 --outfile data/tasks/pie/output --feedback_type rich\n```\n\n- For evaluation details, please see [docs/pie_eval.md](docs/pie_eval.md).\n\n<hr>\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_70","type":"reasoning","text":"**Flagging README for review**"}}
{"type":"item.started","item":{"id":"item_71","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,200p' code/reflexion/README.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_71","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,200p' code/reflexion/README.md\"","aggregated_output":"# [NeurIPS 2023] Reflexion: Language Agents with Verbal Reinforcement Learning\n\nThis repo holds the code, demos, and log files for [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366) by Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao. \n\n![Reflexion RL diagram](./figures/reflexion_rl.png)\n\n![Reflexion tasks](./figures/reflexion_tasks.png)\n\nWe have released the LeetcodeHardGym [here](https://github.com/GammaTauAI/leetcode-hard-gym)\n\n## To Run: reasoning (HotPotQA)\n\nWe have provided a set of notebooks to easily run, explore, and interact with the results of the reasoning experiments. Each experiment consists of a random sample of 100 questions from the HotPotQA distractor dataset. Each question in the sample is attempted by an agent with a specific type and reflexion strategy.\n\n### Setup\n\nTo get started:\n\n1. Clone this repo and move to the HotPotQA directory:\n\n```bash\ngit clone https://github.com/noahshinn/reflexion && cd ./hotpotqa_runs\n```\n\n2. Install the module dependencies into your environment:\n\n```bash\npip install -r requirements.txt\n```\n\n3. Set `OPENAI_API_KEY` environment variable to your OpenAI API key:\n\n```bash\nexport OPENAI_API_KEY=[REDACTED] key>\n```\n\n#### Agent Types\n\nAgent type is determined by the notebook you choose to run. The available agent types include:\n\n- `ReAct` - ReAct Agent\n\n- `CoT_context` - CoT Agent given supporting context about the question \n\n- `CoT_no_context` - CoT Agent given no supporting context about the question\n\nThe notebook for each agent type is located in the `./hotpot_runs/notebooks` directory.\n\n#### Reflexion Strategies\n\nEach notebook allows you to specify the reflexion strategy to be used by the agents. The available reflexion strategies, which are defined in an `Enum`, include:\n\n- `ReflexionStrategy.NONE` - The agent is not given any information about its last attempt. \n\n- `ReflexionStrategy.LAST_ATTEMPT` - The agent is given its reasoning trace from its last attempt on the question as context.\n\n- `ReflexionStrategy.REFLEXION` - The agent is given its self-reflection on the last attempt as context. \n\n- `ReflexionStrategy.LAST_ATTEMPT_AND_REFLEXION` -  The agent is given both its reasoning trace and self-reflection on the last attempt as context.\n\n### To Run: decision-making (AlfWorld)\n\nClone this repo and move to the AlfWorld directory\n\n```bash\ngit clone https://github.com/noahshinn/reflexion && cd ./alfworld_runs\n```\n\nSpecify the run parameters in `./run_reflexion.sh`.\n`num_trials`: number of iterative learning steps\n`num_envs`: number of task-environment pairs per trial\n`run_name`: the name for this run\n`use_memory`: use persisting memory to store self-reflections (turn off to run a baseline run)\n`is_resume`: use logging directory to resume a previous run\n`resume_dir`: the logging directory from which to resume the previous run\n`start_trial_num`: if resume run, then the trial number of which to start\n\nRun the trial\n\n```bash\n./run_reflexion.sh\n```\n\nThe logs will be sent to `./root/<run_name>`.\n\n### Another Note\n\nDue to the nature of these experiments, it may not be feasible for individual developers to rerun the results as GPT-4 has limited access and significant API charges. All runs from the paper and additional results are logged in `./alfworld_runs/root` for decision-making, `./hotpotqa_runs/root` for reasoning, and `./programming_runs/root` for programming\n\n### Other Notes\n\nCheck out the original implementation [here](https://github.com/noahshinn/reflexion-draft)\n\nRead one of the original blog posts [here](https://nanothoughts.substack.com/p/reflecting-on-reflexion)\n\nCheck out an [Appl](https://github.com/appl-team/appl) implementation [here](https://github.com/appl-team/reppl/tree/main/reflexion).\n\nCheck out an interesting type-prediction implementation here: [OpenTau](https://github.com/GammaTauAI/opentau)\n\nFor all questions, contact [noahrshinn@gmail.com](noahrshinn@gmail.com)\n\n### Cite\n\n```bibtex\n@misc{shinn2023reflexion,\n      title={Reflexion: Language Agents with Verbal Reinforcement Learning}, \n      author={Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},\n      year={2023},\n      eprint={2303.11366},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n```\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_72","type":"reasoning","text":"**Deciding dataset info sources**\n\nPlanning to use dataset metadata from loaded datasets for sizes and splits to avoid relying on web queries, but will use web queries to confirm dataset names if needed. Also preparing to script dataset counts for resource documentation and creating a papers readme with manual or partial metadata extraction due to small paper count and unreliability of PDF parsing."}}
{"type":"item.completed","item":{"id":"item_73","type":"reasoning","text":"**Choosing author metadata approach**\n\nDecided to use web.run to fetch accurate author lists from arXiv or ACL pages for each paper, complying with instructions to include authors, while minimizing manual errors. Planning scripted web searches to extract metadata for papers and datasets efficiently before generating documentation."}}
{"type":"item.completed","item":{"id":"item_74","type":"web_search","query":"Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning ACL 2023 pdf authors"}}
{"type":"item.completed","item":{"id":"item_75","type":"reasoning","text":"**Identifying reference needs**\n\nI'm listing key methods like chain-of-thought and self-critique that require proper sources and authorship to support further work. Next, I'll search for authoritative references using web resources."}}
{"type":"item.completed","item":{"id":"item_76","type":"web_search","query":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models arXiv 2201.11903 authors"}}
{"type":"item.completed","item":{"id":"item_77","type":"reasoning","text":"**Prioritizing arxiv.org for author data**\n\nPlanning to source missing author lists directly from arxiv.org by opening arxiv.org/abs/xxx pages, ensuring authoritative data by filtering searches to that domain."}}
{"type":"item.completed","item":{"id":"item_78","type":"web_search","query":"site:arxiv.org 2201.11903 authors"}}
{"type":"item.completed","item":{"id":"item_79","type":"reasoning","text":"**Deciding to open arxiv.org directly**\n\nI'll bypass unreliable arxiv search by opening authoritative URLs directly on arxiv.org and extract author info from the page content."}}
{"type":"item.completed","item":{"id":"item_80","type":"reasoning","text":"**Planning search query for arxiv.org**\n\nI'm aiming to create a search query that returns results from arxiv.org, likely using a pattern with arxiv.org/abs/ID to ensure allowed access."}}
{"type":"item.completed","item":{"id":"item_81","type":"web_search","query":"arxiv.org/abs/2201.11903"}}
{"type":"item.completed","item":{"id":"item_82","type":"reasoning","text":"**Identifying fallback author sources**\n\nSince arXiv.org isn't reliably available, the plan is to prioritize arXiv but fallback to sources like bibbase, Athina, or Huggingface Papers for author information—especially for cases like self-evaluation guided beam and self-critique prompting that need author lists. Next is to run a web search targeting Huggingface for specific paper IDs."}}
{"type":"item.completed","item":{"id":"item_83","type":"web_search","query":"huggingface papers 2305.13733 self-critique prompting"}}
{"type":"item.completed","item":{"id":"item_84","type":"reasoning","text":"**Planning literature and resources files**\n\nI'm deciding to source author info from Huggingface papers, bibbase/athina, and dblp depending on the context, and to reference HF dataset cards minimally in resources without formal citation but noting license status. I'll create papers/README.md with concise paper details, literature_review.md with scope and summaries, resources.md including dataset URLs and challenges like strategyqa failures, plus a datasets/.gitignore to prevent committing large data files."}}
{"type":"item.completed","item":{"id":"item_85","type":"reasoning","text":"**Finalizing paper list and dataset sizing**\n\nI'm standardizing author listings using first three names plus \"et al.\" for lengthy papers and opting to use the arXiv title for the self-evaluation beam search paper due to title changes. Next, I'll create the README with this paper list and generate dataset counts and sizes via Python scripts and disk usage commands for gsm8k and AI2 ARC."}}
{"type":"item.started","item":{"id":"item_86","type":"command_execution","command":"/bin/bash -lc \"source .venv/bin/activate && python - << 'PY'\nfrom datasets import load_from_disk\nfrom pathlib import Path\n\nfor name in ['gsm8k', 'ai2_arc']:\n    ds = load_from_disk(Path('datasets') / name)\n    print(name)\n    for split in ds.keys():\n        print(' ', split, len(ds[split]))\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_86","type":"command_execution","command":"/bin/bash -lc \"source .venv/bin/activate && python - << 'PY'\nfrom datasets import load_from_disk\nfrom pathlib import Path\n\nfor name in ['gsm8k', 'ai2_arc']:\n    ds = load_from_disk(Path('datasets') / name)\n    print(name)\n    for split in ds.keys():\n        print(' ', split, len(ds[split]))\nPY\"","aggregated_output":"gsm8k\n  train 7473\n  test 1319\nai2_arc\n  train 1119\n  test 1172\n  validation 299\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_87","type":"reasoning","text":"**Planning to use disk usage command**"}}
{"type":"item.started","item":{"id":"item_88","type":"command_execution","command":"/bin/bash -lc 'du -sh datasets/gsm8k datasets/ai2_arc'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_88","type":"command_execution","command":"/bin/bash -lc 'du -sh datasets/gsm8k datasets/ai2_arc'","aggregated_output":"2.8M\tdatasets/gsm8k\n600K\tdatasets/ai2_arc\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_89","type":"reasoning","text":"**Creating datasets .gitignore**\n\nI'm preparing to create a .gitignore file inside the datasets directory to manage untracked files or folders there, verifying the current directory first."}}
{"type":"item.started","item":{"id":"item_90","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_90","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/fix-lazy-llms-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_91","type":"command_execution","command":"/bin/bash -lc \"cat > datasets/.gitignore << 'EOF'\n# Exclude all data files (can be large)\n*\n\n# But include documentation and small samples\n\"'!.gitignore\n!README.md\n!**/README.md\n!**/samples/\n!**/download.sh\n!**/download_instructions.txt\nEOF'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_91","type":"command_execution","command":"/bin/bash -lc \"cat > datasets/.gitignore << 'EOF'\n# Exclude all data files (can be large)\n*\n\n# But include documentation and small samples\n\"'!.gitignore\n!README.md\n!**/README.md\n!**/samples/\n!**/download.sh\n!**/download_instructions.txt\nEOF'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_92","type":"reasoning","text":"**Creating dataset README with download instructions**\n\nI'm drafting a README for gsm8k and ai2_arc datasets that includes HF dataset names, sizes, splits, sample data, notes on unverified licenses, and instructions for downloading and loading the datasets."}}
{"type":"item.started","item":{"id":"item_93","type":"command_execution","command":"/bin/bash -lc \"cat > datasets/README.md << 'EOF'\n# Downloaded Datasets\n\nThis directory contains datasets for the research project. Data files are NOT\ncommitted to git due to size. Follow the download instructions below.\n\n## Dataset 1: GSM8K\n\n### Overview\n- **Source**: HuggingFace dataset \"'`gsm8k` (config: `main`)\n- **Size**: 7,473 train / 1,319 test (≈2.8 MB on disk)\n- **Format**: HuggingFace Dataset\n- **Task**: Grade-school math reasoning\n- **Splits**: train, test\n- **License**: Not verified (see dataset card)\n\n### Download Instructions\n\n**Using HuggingFace (recommended):**\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"gsm8k\", \"main\")\ndataset.save_to_disk(\"datasets/gsm8k\")\n```\n\n### Loading the Dataset\n```python\nfrom datasets import load_from_disk\n\ndataset = load_from_disk(\"datasets/gsm8k\")\nprint(dataset[\"train\"][0])\n```\n\n### Sample Data\nSamples saved in `datasets/gsm8k/samples/sample.json`.\n\n### Notes\n- Common benchmark for chain-of-thought and self-refinement methods.\n\n---\n\n## Dataset 2: AI2 ARC (ARC-Challenge)\n\n### Overview\n- **Source**: HuggingFace dataset `ai2_arc` (config: `ARC-Challenge`)\n- **Size**: 1,119 train / 299 validation / 1,172 test (≈600 KB on disk)\n- **Format**: HuggingFace Dataset\n- **Task**: Multiple-choice science reasoning\n- **Splits**: train, validation, test\n- **License**: Not verified (see dataset card)\n\n### Download Instructions\n\n**Using HuggingFace (recommended):**\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"ai2_arc\", \"ARC-Challenge\")\ndataset.save_to_disk(\"datasets/ai2_arc\")\n```\n\n### Loading the Dataset\n```python\nfrom datasets import load_from_disk\n\ndataset = load_from_disk(\"datasets/ai2_arc\")\nprint(dataset[\"train\"][0])\n```\n\n### Sample Data\nSamples saved in `datasets/ai2_arc/samples/sample.json`.\n\n### Notes\n- ARC-Challenge is commonly used for reasoning and robustness evaluation.\nEOF'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_93","type":"command_execution","command":"/bin/bash -lc \"cat > datasets/README.md << 'EOF'\n# Downloaded Datasets\n\nThis directory contains datasets for the research project. Data files are NOT\ncommitted to git due to size. Follow the download instructions below.\n\n## Dataset 1: GSM8K\n\n### Overview\n- **Source**: HuggingFace dataset \"'`gsm8k` (config: `main`)\n- **Size**: 7,473 train / 1,319 test (≈2.8 MB on disk)\n- **Format**: HuggingFace Dataset\n- **Task**: Grade-school math reasoning\n- **Splits**: train, test\n- **License**: Not verified (see dataset card)\n\n### Download Instructions\n\n**Using HuggingFace (recommended):**\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"gsm8k\", \"main\")\ndataset.save_to_disk(\"datasets/gsm8k\")\n```\n\n### Loading the Dataset\n```python\nfrom datasets import load_from_disk\n\ndataset = load_from_disk(\"datasets/gsm8k\")\nprint(dataset[\"train\"][0])\n```\n\n### Sample Data\nSamples saved in `datasets/gsm8k/samples/sample.json`.\n\n### Notes\n- Common benchmark for chain-of-thought and self-refinement methods.\n\n---\n\n## Dataset 2: AI2 ARC (ARC-Challenge)\n\n### Overview\n- **Source**: HuggingFace dataset `ai2_arc` (config: `ARC-Challenge`)\n- **Size**: 1,119 train / 299 validation / 1,172 test (≈600 KB on disk)\n- **Format**: HuggingFace Dataset\n- **Task**: Multiple-choice science reasoning\n- **Splits**: train, validation, test\n- **License**: Not verified (see dataset card)\n\n### Download Instructions\n\n**Using HuggingFace (recommended):**\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"ai2_arc\", \"ARC-Challenge\")\ndataset.save_to_disk(\"datasets/ai2_arc\")\n```\n\n### Loading the Dataset\n```python\nfrom datasets import load_from_disk\n\ndataset = load_from_disk(\"datasets/ai2_arc\")\nprint(dataset[\"train\"][0])\n```\n\n### Sample Data\nSamples saved in `datasets/ai2_arc/samples/sample.json`.\n\n### Notes\n- ARC-Challenge is commonly used for reasoning and robustness evaluation.\nEOF'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_94","type":"reasoning","text":"**Creating a papers README**\n\nI'm preparing to create a README listing papers with their relevance, known arXiv IDs, and associated file names for clear documentation."}}
{"type":"item.started","item":{"id":"item_95","type":"command_execution","command":"/bin/bash -lc \"cat > papers/README.md << 'EOF'\n# Downloaded Papers\n\n1. **Large Language Models Can Be Lazy Learners: Analyze Shortcuts in In-Context Learning**\n   - Authors: Ruixiang Tang, Dehan Kong, Longtao Huang, Hui Xue\n   - Year: 2023 (Findings of ACL)\n   - Source: ACL Anthology\n   - File: \"'`2023_lazy_llm_learners_acl.pdf`\n   - Why relevant: Directly studies “lazy learner” behavior via shortcut reliance in ICL.\n\n2. **Self-Refine: Iterative Refinement with Self-Feedback**\n   - Authors: Aman Madaan, Niket Tandon, Prakhar Gupta, et al.\n   - Year: 2023 (arXiv)\n   - arXiv: 2303.17651\n   - File: `2023_self_refine.pdf`\n   - Why relevant: Demonstrates iterative self-critique/refinement that can counter low-effort outputs.\n\n3. **Reflexion: Language Agents with Verbal Reinforcement Learning**\n   - Authors: Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao\n   - Year: 2023 (arXiv / NeurIPS)\n   - arXiv: 2303.11366\n   - File: `2023_reflexion.pdf`\n   - Why relevant: Uses self-reflection feedback loops to improve agent performance.\n\n4. **Self-Consistency Improves Chain of Thought Reasoning in Language Models**\n   - Authors: Xuezhi Wang, Jason Wei, et al.\n   - Year: 2022 (arXiv)\n   - arXiv: 2203.11171\n   - File: `2022_self_consistency_cot.pdf`\n   - Why relevant: Multi-sample reasoning to reduce shallow single-pass errors.\n\n5. **Least-to-Most Prompting Enables Complex Reasoning in Large Language Models**\n   - Authors: Denny Zhou, et al.\n   - Year: 2022 (arXiv)\n   - arXiv: 2205.10625\n   - File: `2022_least_to_most_prompting.pdf`\n   - Why relevant: Structured prompting to elicit more effortful reasoning.\n\n6. **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models**\n   - Authors: Jason Wei, Xuezhi Wang, et al.\n   - Year: 2022 (arXiv)\n   - arXiv: 2201.11903\n   - File: `2022_chain_of_thought_prompting.pdf`\n   - Why relevant: Foundational reasoning prompting baseline for effortful outputs.\n\n7. **Self-Critique Prompting with Large Language Models for Inductive Instructions**\n   - Authors: Rui Wang, et al.\n   - Year: 2023 (arXiv)\n   - arXiv: 2305.13733\n   - File: `2023_self_critique_prompting.pdf`\n   - Why relevant: Uses critic-style prompting to improve task performance.\n\n8. **Self-Evaluation Guided Beam Search for Reasoning**\n   - Authors: Yuxi Xie, et al.\n   - Year: 2023 (arXiv)\n   - arXiv: 2305.00633\n   - File: `2023_self_evaluation_guided_beam.pdf`\n   - Why relevant: Uses self-evaluation to guide decoding toward better reasoning.\nEOF'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_95","type":"command_execution","command":"/bin/bash -lc \"cat > papers/README.md << 'EOF'\n# Downloaded Papers\n\n1. **Large Language Models Can Be Lazy Learners: Analyze Shortcuts in In-Context Learning**\n   - Authors: Ruixiang Tang, Dehan Kong, Longtao Huang, Hui Xue\n   - Year: 2023 (Findings of ACL)\n   - Source: ACL Anthology\n   - File: \"'`2023_lazy_llm_learners_acl.pdf`\n   - Why relevant: Directly studies “lazy learner” behavior via shortcut reliance in ICL.\n\n2. **Self-Refine: Iterative Refinement with Self-Feedback**\n   - Authors: Aman Madaan, Niket Tandon, Prakhar Gupta, et al.\n   - Year: 2023 (arXiv)\n   - arXiv: 2303.17651\n   - File: `2023_self_refine.pdf`\n   - Why relevant: Demonstrates iterative self-critique/refinement that can counter low-effort outputs.\n\n3. **Reflexion: Language Agents with Verbal Reinforcement Learning**\n   - Authors: Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao\n   - Year: 2023 (arXiv / NeurIPS)\n   - arXiv: 2303.11366\n   - File: `2023_reflexion.pdf`\n   - Why relevant: Uses self-reflection feedback loops to improve agent performance.\n\n4. **Self-Consistency Improves Chain of Thought Reasoning in Language Models**\n   - Authors: Xuezhi Wang, Jason Wei, et al.\n   - Year: 2022 (arXiv)\n   - arXiv: 2203.11171\n   - File: `2022_self_consistency_cot.pdf`\n   - Why relevant: Multi-sample reasoning to reduce shallow single-pass errors.\n\n5. **Least-to-Most Prompting Enables Complex Reasoning in Large Language Models**\n   - Authors: Denny Zhou, et al.\n   - Year: 2022 (arXiv)\n   - arXiv: 2205.10625\n   - File: `2022_least_to_most_prompting.pdf`\n   - Why relevant: Structured prompting to elicit more effortful reasoning.\n\n6. **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models**\n   - Authors: Jason Wei, Xuezhi Wang, et al.\n   - Year: 2022 (arXiv)\n   - arXiv: 2201.11903\n   - File: `2022_chain_of_thought_prompting.pdf`\n   - Why relevant: Foundational reasoning prompting baseline for effortful outputs.\n\n7. **Self-Critique Prompting with Large Language Models for Inductive Instructions**\n   - Authors: Rui Wang, et al.\n   - Year: 2023 (arXiv)\n   - arXiv: 2305.13733\n   - File: `2023_self_critique_prompting.pdf`\n   - Why relevant: Uses critic-style prompting to improve task performance.\n\n8. **Self-Evaluation Guided Beam Search for Reasoning**\n   - Authors: Yuxi Xie, et al.\n   - Year: 2023 (arXiv)\n   - arXiv: 2305.00633\n   - File: `2023_self_evaluation_guided_beam.pdf`\n   - Why relevant: Uses self-evaluation to guide decoding toward better reasoning.\nEOF'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_96","type":"reasoning","text":"**Preparing README documentation**\n\nPlanning to document the self-refine and reflexion features with relevant key files and explanatory notes in code/README.md."}}
{"type":"item.started","item":{"id":"item_97","type":"command_execution","command":"/bin/bash -lc \"cat > code/README.md << 'EOF'\n# Cloned Repositories\n\n## Repo 1: self-refine\n- URL: https://github.com/madaan/self-refine\n- Purpose: Implements Self-Refine (iterative self-feedback + refinement) across multiple tasks.\n- Location: \"'`code/self-refine/`\n- Key files:\n  - `src/acronym/run.py` (acronym generation demo)\n  - `src/responsegen/run.py` (dialogue response generation)\n  - `src/gsm/run.py` (GSM8K task)\n- Notes: Uses `prompt-lib` for LLM calls; requires API access for GPT models.\n\n## Repo 2: reflexion\n- URL: https://github.com/noahshinn/reflexion\n- Purpose: Implements Reflexion (self-reflection for agents) across reasoning, decision-making, and programming.\n- Location: `code/reflexion/`\n- Key files:\n  - `hotpotqa_runs/` (reasoning experiments)\n  - `alfworld_runs/` (decision-making experiments)\n  - `programming_runs/` (program synthesis/LeetCode experiments)\n- Notes: Requires OpenAI API key and additional dependencies; some experiments are costly.\nEOF'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_97","type":"command_execution","command":"/bin/bash -lc \"cat > code/README.md << 'EOF'\n# Cloned Repositories\n\n## Repo 1: self-refine\n- URL: https://github.com/madaan/self-refine\n- Purpose: Implements Self-Refine (iterative self-feedback + refinement) across multiple tasks.\n- Location: \"'`code/self-refine/`\n- Key files:\n  - `src/acronym/run.py` (acronym generation demo)\n  - `src/responsegen/run.py` (dialogue response generation)\n  - `src/gsm/run.py` (GSM8K task)\n- Notes: Uses `prompt-lib` for LLM calls; requires API access for GPT models.\n\n## Repo 2: reflexion\n- URL: https://github.com/noahshinn/reflexion\n- Purpose: Implements Reflexion (self-reflection for agents) across reasoning, decision-making, and programming.\n- Location: `code/reflexion/`\n- Key files:\n  - `hotpotqa_runs/` (reasoning experiments)\n  - `alfworld_runs/` (decision-making experiments)\n  - `programming_runs/` (program synthesis/LeetCode experiments)\n- Notes: Requires OpenAI API key and additional dependencies; some experiments are costly.\nEOF'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_98","type":"reasoning","text":"**Outlining literature review structure**\n\nI'm defining the literature review to cover scope, search logs, screening, summaries using detailed and abstracted approaches, plus methodology, datasets, baselines, metrics, and results. I'll also highlight common themes, gaps, and recommendations while keeping the text concise."}}
{"type":"item.started","item":{"id":"item_99","type":"command_execution","command":"/bin/bash -lc \"cat > literature_review.md << 'EOF'\n# Literature Review: Fixing Lazy LLMs\n\n## Review Scope\n\n### Research Question\nHow can prompting strategies (e.g., harsh critique roles, response budget control, iterative self-feedback) reduce “lazy” low-effort LLM outputs and improve reasoning quality?\n\n### Inclusion Criteria\n- Focus on LLM prompting or test-time interventions that increase effort, reasoning depth, or self-critique\n- Empirical evaluation on reasoning or instruction-following tasks\n- Recent work (2022–2024), plus key foundational baselines\n\n### Exclusion Criteria\n- Training-only interventions without test-time prompting relevance\n- Non-LLM-specific evaluation without a clear link to response quality or effort\n\n### Time Frame\n2022–2024 (with one foundational work as needed)\n\n### Sources\n- arXiv\n- ACL Anthology\n- Papers with Code / GitHub\n- HuggingFace papers pages\n\n## Search Log\n\n| Date | Query | Source | Results | Notes |\n|------|-------|--------|---------|-------|\n| 2026-01-31 | “LLM lazy learners shortcuts in-context learning” | ACL Anthology / arXiv | 1 | Found direct “lazy learners” paper (ACL Findings 2023). |\n| 2026-01-31 | “self-refine iterative self-feedback LLM” | arXiv | 1 | Core iterative self-critique method. |\n| 2026-01-31 | “self-critique prompting LLM” | arXiv | 1 | Critic-style prompting. |\n| 2026-01-31 | “self-evaluation guided beam search reasoning” | arXiv | 1 | Decoding guided by self-evaluation. |\n| 2026-01-31 | “chain-of-thought prompting” | arXiv | 1 | Foundational baseline. |\n| 2026-01-31 | “least-to-most prompting” | arXiv | 1 | Structured prompting baseline. |\n| 2026-01-31 | “self-consistency chain-of-thought” | arXiv | 1 | Multi-sample consistency baseline. |\n| 2026-01-31 | “Reflexion verbal reinforcement learning” | arXiv | 1 | Self-reflection baseline. |\n\n## Screening Results\n\n| Paper | Title Screen | Abstract Screen | Full-Text | Notes |\n|-------|-------------|-----------------|-----------|-------|\n| Lazy Learners (ACL 2023) | Include | Include | Include | Direct evidence of shortcut reliance in ICL. |\n| Self-Refine (2023) | Include | Include | Include | Iterative self-critique/refinement. |\n| Reflexion (2023) | Include | Include | Partial | Relevant self-reflection loop for agents. |\n| Self-Consistency CoT (2022) | Include | Include | Partial | Reasoning baseline. |\n| Least-to-Most (2022) | Include | Include | Partial | Structured reasoning prompting baseline. |\n| Chain-of-Thought (2022) | Include | Include | Partial | Foundational baseline. |\n| Self-Critique Prompting (2023) | Include | Include | Partial | Critic prompting baseline. |\n| Self-Evaluation Guided Beam (2023) | Include | Include | Partial | Decoding guided by self-evaluation. |\n\n## Paper Summaries\n\n### Paper 1: Large Language Models Can Be Lazy Learners: Analyze Shortcuts in In-Context Learning\n- **Authors**: Ruixiang Tang, Dehan Kong, Longtao Huang, Hui Xue\n- **Year**: 2023\n- **Source**: Findings of ACL\n- **Key Contribution**: Shows LLMs exploit shortcut triggers in prompts, leading to performance collapse on anti-shortcut test sets.\n- **Methodology**: Inject shortcut triggers (words, signs, styles) into ICL prompts and evaluate on classification + extraction tasks.\n- **Datasets Used**: SST-2, MR, CR, OLID; ATIS; MIT Movies (slot filling).\n- **Results**: Large drops on anti-shortcut sets; larger models show bigger drops (reverse scaling). Trigger insertion alone doesn’t explain drops.\n- **Code Available**: Not in paper PDF.\n- **Relevance to Our Research**: Direct evidence of “lazy” shortcut reliance and a benchmark paradigm for detecting it.\n\n### Paper 2: Self-Refine: Iterative Refinement with Self-Feedback\n- **Authors**: Aman Madaan, Niket Tandon, Prakhar Gupta, et al.\n- **Year**: 2023\n- **Source**: arXiv\n- **Key Contribution**: Introduces iterative self-feedback/refinement loop that improves outputs without extra training.\n- **Methodology**: Generate → Feedback → Refine with the same LLM; iterate up to 4 rounds; few-shot prompts for feedback and refinement.\n- **Datasets Used**: Dialogue response generation, code optimization, code readability, math reasoning (GSM8K), sentiment reversal, acronym generation, constrained generation.\n- **Results**: Consistent improvements across tasks; largest gains in preference-based tasks; modest gains for math unless external error signal exists.\n- **Code Available**: Yes (selfrefine.info / GitHub).\n- **Relevance to Our Research**: Concrete intervention to reduce lazy first-pass outputs by forcing critique + revision.\n\n### Paper 3: Reflexion: Language Agents with Verbal Reinforcement Learning\n- **Authors**: Noah Shinn et al.\n- **Year**: 2023\n- **Source**: arXiv / NeurIPS\n- **Key Contribution**: Adds self-reflection memory to improve agents over trials.\n- **Methodology**: Agents keep verbal reflections to guide future attempts; evaluated on reasoning (HotPotQA), decision-making (AlfWorld), programming.\n- **Datasets Used**: HotPotQA, AlfWorld, programming tasks.\n- **Results**: Reflection improves success rates across domains.\n- **Code Available**: Yes.\n- **Relevance**: Reflection-based feedback loops relate to “harsh critic” prompting.\n\n### Paper 4: Self-Consistency Improves Chain of Thought Reasoning in Language Models\n- **Authors**: Xuezhi Wang et al.\n- **Year**: 2022\n- **Source**: arXiv\n- **Key Contribution**: Sample multiple CoT reasoning paths and choose the most consistent answer.\n- **Methodology**: Generate multiple reasoning traces; majority vote / consistency selection.\n- **Datasets Used**: GSM8K, SVAMP, AQuA, ARC, StrategyQA, and others.\n- **Results**: Substantial improvements over single CoT.\n- **Relevance**: A baseline for reducing lazy single-pass reasoning.\n\n### Paper 5: Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\n- **Authors**: Denny Zhou et al.\n- **Year**: 2022\n- **Source**: arXiv\n- **Key Contribution**: Decomposes complex problems into subproblems in prompts.\n- **Methodology**: First prompt elicits subquestions, then solves sequentially.\n- **Datasets Used**: GSM8K, SCAN, compositional reasoning tasks.\n- **Results**: Improves performance on complex reasoning.\n- **Relevance**: Structured prompting to increase effortful reasoning.\n\n### Paper 6: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n- **Authors**: Jason Wei et al.\n- **Year**: 2022\n- **Source**: arXiv\n- **Key Contribution**: Demonstrates that explicit reasoning chains improve performance.\n- **Methodology**: Few-shot prompts with reasoning steps.\n- **Datasets Used**: GSM8K, MultiArith, CommonsenseQA, StrategyQA, others.\n- **Results**: Strong improvements for sufficiently large models.\n- **Relevance**: Foundational baseline for effortful outputs.\n\n### Paper 7: Self-Critique Prompting with Large Language Models for Inductive Instructions\n- **Authors**: Rui Wang et al.\n- **Year**: 2023\n- **Source**: arXiv\n- **Key Contribution**: Critic-style prompting improves instruction induction.\n- **Methodology**: Model generates candidate, critiques, then revises.\n- **Datasets Used**: Instruction induction and related tasks.\n- **Results**: Improves performance vs. direct generation.\n- **Relevance**: Aligns with “harsh critic” prompting hypothesis.\n\n### Paper 8: Self-Evaluation Guided Beam Search for Reasoning\n- **Authors**: Yuxi Xie et al.\n- **Year**: 2023\n- **Source**: arXiv\n- **Key Contribution**: Uses self-evaluation signals to guide decoding in reasoning.\n- **Methodology**: Beam search guided by self-evaluation of candidate reasoning paths.\n- **Datasets Used**: Math and reasoning benchmarks (e.g., GSM8K, MATH).\n- **Results**: Improves reasoning accuracy compared to standard decoding.\n- **Relevance**: Decoding-time control can mitigate lazy single-pass outputs.\n\n## Common Methodologies\n- **Self-critique / self-refine loops**: Self-Refine, Self-Critique Prompting, Reflexion.\n- **Structured reasoning prompts**: Chain-of-Thought, Least-to-Most.\n- **Multi-sample selection**: Self-Consistency; self-evaluation guided decoding.\n\n## Standard Baselines\n- Direct generation with same model (no critique)\n- Chain-of-Thought prompting\n- Self-Consistency (multi-sample majority)\n- Least-to-Most prompting\n\n## Evaluation Metrics\n- **Accuracy / exact match**: GSM8K, ARC-Challenge\n- **Solve rate**: math reasoning tasks\n- **Human preference**: dialogue, readability, sentiment reversal\n- **Model-judged preference**: GPT-4/LLM-as-judge for some tasks\n\n## Datasets in the Literature\n- **GSM8K**: math word problems\n- **ARC-Challenge**: multiple-choice science reasoning\n- **HotPotQA**: multi-hop QA (Reflexion)\n- **AlfWorld**: embodied decision-making (Reflexion)\n\n## Gaps and Opportunities\n- Limited work directly tests “lazy” shortcut reliance vs. “effort” prompting.\n- Few controlled studies on response budget constraints (length/steps) vs. quality.\n- Need standardized benchmarks for “effort” or “critique strength.”\n\n## Recommendations for Our Experiment\n- **Recommended datasets**: GSM8K, ARC-Challenge (both already downloaded).\n- **Recommended baselines**: direct generation, CoT, self-consistency, least-to-most.\n- **Recommended metrics**: accuracy / exact match; optional LLM-judge preference.\n- **Methodological considerations**:\n  - Treat “harsh critic” prompting as a controllable intervention.\n  - Vary response budget (max tokens / steps) to test effort-quality tradeoff.\n  - Use anti-shortcut evaluation (from Lazy Learners) as robustness check.\nEOF\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_99","type":"command_execution","command":"/bin/bash -lc \"cat > literature_review.md << 'EOF'\n# Literature Review: Fixing Lazy LLMs\n\n## Review Scope\n\n### Research Question\nHow can prompting strategies (e.g., harsh critique roles, response budget control, iterative self-feedback) reduce “lazy” low-effort LLM outputs and improve reasoning quality?\n\n### Inclusion Criteria\n- Focus on LLM prompting or test-time interventions that increase effort, reasoning depth, or self-critique\n- Empirical evaluation on reasoning or instruction-following tasks\n- Recent work (2022–2024), plus key foundational baselines\n\n### Exclusion Criteria\n- Training-only interventions without test-time prompting relevance\n- Non-LLM-specific evaluation without a clear link to response quality or effort\n\n### Time Frame\n2022–2024 (with one foundational work as needed)\n\n### Sources\n- arXiv\n- ACL Anthology\n- Papers with Code / GitHub\n- HuggingFace papers pages\n\n## Search Log\n\n| Date | Query | Source | Results | Notes |\n|------|-------|--------|---------|-------|\n| 2026-01-31 | “LLM lazy learners shortcuts in-context learning” | ACL Anthology / arXiv | 1 | Found direct “lazy learners” paper (ACL Findings 2023). |\n| 2026-01-31 | “self-refine iterative self-feedback LLM” | arXiv | 1 | Core iterative self-critique method. |\n| 2026-01-31 | “self-critique prompting LLM” | arXiv | 1 | Critic-style prompting. |\n| 2026-01-31 | “self-evaluation guided beam search reasoning” | arXiv | 1 | Decoding guided by self-evaluation. |\n| 2026-01-31 | “chain-of-thought prompting” | arXiv | 1 | Foundational baseline. |\n| 2026-01-31 | “least-to-most prompting” | arXiv | 1 | Structured prompting baseline. |\n| 2026-01-31 | “self-consistency chain-of-thought” | arXiv | 1 | Multi-sample consistency baseline. |\n| 2026-01-31 | “Reflexion verbal reinforcement learning” | arXiv | 1 | Self-reflection baseline. |\n\n## Screening Results\n\n| Paper | Title Screen | Abstract Screen | Full-Text | Notes |\n|-------|-------------|-----------------|-----------|-------|\n| Lazy Learners (ACL 2023) | Include | Include | Include | Direct evidence of shortcut reliance in ICL. |\n| Self-Refine (2023) | Include | Include | Include | Iterative self-critique/refinement. |\n| Reflexion (2023) | Include | Include | Partial | Relevant self-reflection loop for agents. |\n| Self-Consistency CoT (2022) | Include | Include | Partial | Reasoning baseline. |\n| Least-to-Most (2022) | Include | Include | Partial | Structured reasoning prompting baseline. |\n| Chain-of-Thought (2022) | Include | Include | Partial | Foundational baseline. |\n| Self-Critique Prompting (2023) | Include | Include | Partial | Critic prompting baseline. |\n| Self-Evaluation Guided Beam (2023) | Include | Include | Partial | Decoding guided by self-evaluation. |\n\n## Paper Summaries\n\n### Paper 1: Large Language Models Can Be Lazy Learners: Analyze Shortcuts in In-Context Learning\n- **Authors**: Ruixiang Tang, Dehan Kong, Longtao Huang, Hui Xue\n- **Year**: 2023\n- **Source**: Findings of ACL\n- **Key Contribution**: Shows LLMs exploit shortcut triggers in prompts, leading to performance collapse on anti-shortcut test sets.\n- **Methodology**: Inject shortcut triggers (words, signs, styles) into ICL prompts and evaluate on classification + extraction tasks.\n- **Datasets Used**: SST-2, MR, CR, OLID; ATIS; MIT Movies (slot filling).\n- **Results**: Large drops on anti-shortcut sets; larger models show bigger drops (reverse scaling). Trigger insertion alone doesn’t explain drops.\n- **Code Available**: Not in paper PDF.\n- **Relevance to Our Research**: Direct evidence of “lazy” shortcut reliance and a benchmark paradigm for detecting it.\n\n### Paper 2: Self-Refine: Iterative Refinement with Self-Feedback\n- **Authors**: Aman Madaan, Niket Tandon, Prakhar Gupta, et al.\n- **Year**: 2023\n- **Source**: arXiv\n- **Key Contribution**: Introduces iterative self-feedback/refinement loop that improves outputs without extra training.\n- **Methodology**: Generate → Feedback → Refine with the same LLM; iterate up to 4 rounds; few-shot prompts for feedback and refinement.\n- **Datasets Used**: Dialogue response generation, code optimization, code readability, math reasoning (GSM8K), sentiment reversal, acronym generation, constrained generation.\n- **Results**: Consistent improvements across tasks; largest gains in preference-based tasks; modest gains for math unless external error signal exists.\n- **Code Available**: Yes (selfrefine.info / GitHub).\n- **Relevance to Our Research**: Concrete intervention to reduce lazy first-pass outputs by forcing critique + revision.\n\n### Paper 3: Reflexion: Language Agents with Verbal Reinforcement Learning\n- **Authors**: Noah Shinn et al.\n- **Year**: 2023\n- **Source**: arXiv / NeurIPS\n- **Key Contribution**: Adds self-reflection memory to improve agents over trials.\n- **Methodology**: Agents keep verbal reflections to guide future attempts; evaluated on reasoning (HotPotQA), decision-making (AlfWorld), programming.\n- **Datasets Used**: HotPotQA, AlfWorld, programming tasks.\n- **Results**: Reflection improves success rates across domains.\n- **Code Available**: Yes.\n- **Relevance**: Reflection-based feedback loops relate to “harsh critic” prompting.\n\n### Paper 4: Self-Consistency Improves Chain of Thought Reasoning in Language Models\n- **Authors**: Xuezhi Wang et al.\n- **Year**: 2022\n- **Source**: arXiv\n- **Key Contribution**: Sample multiple CoT reasoning paths and choose the most consistent answer.\n- **Methodology**: Generate multiple reasoning traces; majority vote / consistency selection.\n- **Datasets Used**: GSM8K, SVAMP, AQuA, ARC, StrategyQA, and others.\n- **Results**: Substantial improvements over single CoT.\n- **Relevance**: A baseline for reducing lazy single-pass reasoning.\n\n### Paper 5: Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\n- **Authors**: Denny Zhou et al.\n- **Year**: 2022\n- **Source**: arXiv\n- **Key Contribution**: Decomposes complex problems into subproblems in prompts.\n- **Methodology**: First prompt elicits subquestions, then solves sequentially.\n- **Datasets Used**: GSM8K, SCAN, compositional reasoning tasks.\n- **Results**: Improves performance on complex reasoning.\n- **Relevance**: Structured prompting to increase effortful reasoning.\n\n### Paper 6: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n- **Authors**: Jason Wei et al.\n- **Year**: 2022\n- **Source**: arXiv\n- **Key Contribution**: Demonstrates that explicit reasoning chains improve performance.\n- **Methodology**: Few-shot prompts with reasoning steps.\n- **Datasets Used**: GSM8K, MultiArith, CommonsenseQA, StrategyQA, others.\n- **Results**: Strong improvements for sufficiently large models.\n- **Relevance**: Foundational baseline for effortful outputs.\n\n### Paper 7: Self-Critique Prompting with Large Language Models for Inductive Instructions\n- **Authors**: Rui Wang et al.\n- **Year**: 2023\n- **Source**: arXiv\n- **Key Contribution**: Critic-style prompting improves instruction induction.\n- **Methodology**: Model generates candidate, critiques, then revises.\n- **Datasets Used**: Instruction induction and related tasks.\n- **Results**: Improves performance vs. direct generation.\n- **Relevance**: Aligns with “harsh critic” prompting hypothesis.\n\n### Paper 8: Self-Evaluation Guided Beam Search for Reasoning\n- **Authors**: Yuxi Xie et al.\n- **Year**: 2023\n- **Source**: arXiv\n- **Key Contribution**: Uses self-evaluation signals to guide decoding in reasoning.\n- **Methodology**: Beam search guided by self-evaluation of candidate reasoning paths.\n- **Datasets Used**: Math and reasoning benchmarks (e.g., GSM8K, MATH).\n- **Results**: Improves reasoning accuracy compared to standard decoding.\n- **Relevance**: Decoding-time control can mitigate lazy single-pass outputs.\n\n## Common Methodologies\n- **Self-critique / self-refine loops**: Self-Refine, Self-Critique Prompting, Reflexion.\n- **Structured reasoning prompts**: Chain-of-Thought, Least-to-Most.\n- **Multi-sample selection**: Self-Consistency; self-evaluation guided decoding.\n\n## Standard Baselines\n- Direct generation with same model (no critique)\n- Chain-of-Thought prompting\n- Self-Consistency (multi-sample majority)\n- Least-to-Most prompting\n\n## Evaluation Metrics\n- **Accuracy / exact match**: GSM8K, ARC-Challenge\n- **Solve rate**: math reasoning tasks\n- **Human preference**: dialogue, readability, sentiment reversal\n- **Model-judged preference**: GPT-4/LLM-as-judge for some tasks\n\n## Datasets in the Literature\n- **GSM8K**: math word problems\n- **ARC-Challenge**: multiple-choice science reasoning\n- **HotPotQA**: multi-hop QA (Reflexion)\n- **AlfWorld**: embodied decision-making (Reflexion)\n\n## Gaps and Opportunities\n- Limited work directly tests “lazy” shortcut reliance vs. “effort” prompting.\n- Few controlled studies on response budget constraints (length/steps) vs. quality.\n- Need standardized benchmarks for “effort” or “critique strength.”\n\n## Recommendations for Our Experiment\n- **Recommended datasets**: GSM8K, ARC-Challenge (both already downloaded).\n- **Recommended baselines**: direct generation, CoT, self-consistency, least-to-most.\n- **Recommended metrics**: accuracy / exact match; optional LLM-judge preference.\n- **Methodological considerations**:\n  - Treat “harsh critic” prompting as a controllable intervention.\n  - Vary response budget (max tokens / steps) to test effort-quality tradeoff.\n  - Use anti-shortcut evaluation (from Lazy Learners) as robustness check.\nEOF\"","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_100","type":"reasoning","text":"**Planning resource summaries**\n\nI'm preparing to create resources.md with summaries and counts of papers, datasets, and repositories by counting items and PDFs in the papers folder."}}
{"type":"item.started","item":{"id":"item_101","type":"command_execution","command":"/bin/bash -lc \"ls papers | grep -E '\\\\.pdf\"'$'\"' | wc -l\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_101","type":"command_execution","command":"/bin/bash -lc \"ls papers | grep -E '\\\\.pdf\"'$'\"' | wc -l\"","aggregated_output":"8\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_102","type":"reasoning","text":"**Creating resources overview**\n\nI'm compiling a resources.md file with a table of papers, datasets, and repositories, plus sections on search strategy, selection challenges, dataset download, and recommendations."}}
{"type":"item.started","item":{"id":"item_103","type":"command_execution","command":"/bin/bash -lc \"cat > resources.md << 'EOF'\n# Resources Catalog\n\n## Summary\nThis document catalogs all resources gathered for the research project, including papers, datasets, and code repositories.\n\n## Papers\nTotal papers downloaded: 8\n\n| Title | Authors | Year | File | Key Info |\n|-------|---------|------|------|----------|\n| Large Language Models Can Be Lazy Learners: Analyze Shortcuts in In-Context Learning | Tang et al. | 2023 | \"'`papers/2023_lazy_llm_learners_acl.pdf` | Shows shortcut reliance in ICL; reverse scaling effect. |\n| Self-Refine: Iterative Refinement with Self-Feedback | Madaan et al. | 2023 | `papers/2023_self_refine.pdf` | Iterative self-feedback improves outputs across tasks. |\n| Reflexion: Language Agents with Verbal Reinforcement Learning | Shinn et al. | 2023 | `papers/2023_reflexion.pdf` | Reflection memory improves agents. |\n| Self-Consistency Improves Chain of Thought Reasoning in Language Models | Wang et al. | 2022 | `papers/2022_self_consistency_cot.pdf` | Multi-sample consistency baseline. |\n| Least-to-Most Prompting Enables Complex Reasoning in LLMs | Zhou et al. | 2022 | `papers/2022_least_to_most_prompting.pdf` | Decomposition prompting baseline. |\n| Chain-of-Thought Prompting Elicits Reasoning in LLMs | Wei et al. | 2022 | `papers/2022_chain_of_thought_prompting.pdf` | Foundational reasoning prompting. |\n| Self-Critique Prompting with LLMs for Inductive Instructions | Wang et al. | 2023 | `papers/2023_self_critique_prompting.pdf` | Critic-style prompting. |\n| Self-Evaluation Guided Beam Search for Reasoning | Xie et al. | 2023 | `papers/2023_self_evaluation_guided_beam.pdf` | Self-evaluation guided decoding. |\n\nSee `papers/README.md` for detailed descriptions.\n\n## Datasets\nTotal datasets downloaded: 2\n\n| Name | Source | Size | Task | Location | Notes |\n|------|--------|------|------|----------|-------|\n| GSM8K | HuggingFace | 7,473 train / 1,319 test | Math reasoning | `datasets/gsm8k/` | Classic reasoning benchmark. |\n| AI2 ARC (ARC-Challenge) | HuggingFace | 1,119 train / 299 val / 1,172 test | Science QA | `datasets/ai2_arc/` | Reasoning + multiple choice. |\n\nSee `datasets/README.md` for detailed descriptions and download instructions.\n\n## Code Repositories\nTotal repositories cloned: 2\n\n| Name | URL | Purpose | Location | Notes |\n|------|-----|---------|----------|-------|\n| self-refine | https://github.com/madaan/self-refine | Self-feedback refinement | `code/self-refine/` | Contains GSM8K and other task scripts. |\n| reflexion | https://github.com/noahshinn/reflexion | Reflection for agents | `code/reflexion/` | HotPotQA/AlfWorld runs, requires API key. |\n\nSee `code/README.md` for detailed descriptions.\n\n## Resource Gathering Notes\n\n### Search Strategy\n- Started with paper-finder (service unavailable), then manually searched arXiv and ACL Anthology.\n- Prioritized papers on self-critique, self-refinement, and reasoning prompting.\n\n### Selection Criteria\n- Direct relevance to “lazy” LLM behavior or effort-inducing prompting.\n- Publicly available PDFs and (ideally) code.\n- Benchmarks commonly used for reasoning evaluation.\n\n### Challenges Encountered\n- Paper-finder service not running locally.\n- HuggingFace dataset `strategyqa` unavailable via default name; used ARC-Challenge instead.\n\n### Gaps and Workarounds\n- No direct dataset specifically labeled “lazy LLM” behavior; used reasoning datasets as proxies.\n\n## Recommendations for Experiment Design\n\n1. **Primary dataset(s)**: GSM8K and ARC-Challenge for reasoning + robustness checks.\n2. **Baseline methods**: direct generation, CoT, self-consistency, least-to-most prompting.\n3. **Evaluation metrics**: accuracy / exact match; optional LLM-judge preference for output quality.\n4. **Code to adapt/reuse**: Self-Refine for critique/refinement loop; Reflexion for reflection memory baselines.\nEOF'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_103","type":"command_execution","command":"/bin/bash -lc \"cat > resources.md << 'EOF'\n# Resources Catalog\n\n## Summary\nThis document catalogs all resources gathered for the research project, including papers, datasets, and code repositories.\n\n## Papers\nTotal papers downloaded: 8\n\n| Title | Authors | Year | File | Key Info |\n|-------|---------|------|------|----------|\n| Large Language Models Can Be Lazy Learners: Analyze Shortcuts in In-Context Learning | Tang et al. | 2023 | \"'`papers/2023_lazy_llm_learners_acl.pdf` | Shows shortcut reliance in ICL; reverse scaling effect. |\n| Self-Refine: Iterative Refinement with Self-Feedback | Madaan et al. | 2023 | `papers/2023_self_refine.pdf` | Iterative self-feedback improves outputs across tasks. |\n| Reflexion: Language Agents with Verbal Reinforcement Learning | Shinn et al. | 2023 | `papers/2023_reflexion.pdf` | Reflection memory improves agents. |\n| Self-Consistency Improves Chain of Thought Reasoning in Language Models | Wang et al. | 2022 | `papers/2022_self_consistency_cot.pdf` | Multi-sample consistency baseline. |\n| Least-to-Most Prompting Enables Complex Reasoning in LLMs | Zhou et al. | 2022 | `papers/2022_least_to_most_prompting.pdf` | Decomposition prompting baseline. |\n| Chain-of-Thought Prompting Elicits Reasoning in LLMs | Wei et al. | 2022 | `papers/2022_chain_of_thought_prompting.pdf` | Foundational reasoning prompting. |\n| Self-Critique Prompting with LLMs for Inductive Instructions | Wang et al. | 2023 | `papers/2023_self_critique_prompting.pdf` | Critic-style prompting. |\n| Self-Evaluation Guided Beam Search for Reasoning | Xie et al. | 2023 | `papers/2023_self_evaluation_guided_beam.pdf` | Self-evaluation guided decoding. |\n\nSee `papers/README.md` for detailed descriptions.\n\n## Datasets\nTotal datasets downloaded: 2\n\n| Name | Source | Size | Task | Location | Notes |\n|------|--------|------|------|----------|-------|\n| GSM8K | HuggingFace | 7,473 train / 1,319 test | Math reasoning | `datasets/gsm8k/` | Classic reasoning benchmark. |\n| AI2 ARC (ARC-Challenge) | HuggingFace | 1,119 train / 299 val / 1,172 test | Science QA | `datasets/ai2_arc/` | Reasoning + multiple choice. |\n\nSee `datasets/README.md` for detailed descriptions and download instructions.\n\n## Code Repositories\nTotal repositories cloned: 2\n\n| Name | URL | Purpose | Location | Notes |\n|------|-----|---------|----------|-------|\n| self-refine | https://github.com/madaan/self-refine | Self-feedback refinement | `code/self-refine/` | Contains GSM8K and other task scripts. |\n| reflexion | https://github.com/noahshinn/reflexion | Reflection for agents | `code/reflexion/` | HotPotQA/AlfWorld runs, requires API key. |\n\nSee `code/README.md` for detailed descriptions.\n\n## Resource Gathering Notes\n\n### Search Strategy\n- Started with paper-finder (service unavailable), then manually searched arXiv and ACL Anthology.\n- Prioritized papers on self-critique, self-refinement, and reasoning prompting.\n\n### Selection Criteria\n- Direct relevance to “lazy” LLM behavior or effort-inducing prompting.\n- Publicly available PDFs and (ideally) code.\n- Benchmarks commonly used for reasoning evaluation.\n\n### Challenges Encountered\n- Paper-finder service not running locally.\n- HuggingFace dataset `strategyqa` unavailable via default name; used ARC-Challenge instead.\n\n### Gaps and Workarounds\n- No direct dataset specifically labeled “lazy LLM” behavior; used reasoning datasets as proxies.\n\n## Recommendations for Experiment Design\n\n1. **Primary dataset(s)**: GSM8K and ARC-Challenge for reasoning + robustness checks.\n2. **Baseline methods**: direct generation, CoT, self-consistency, least-to-most prompting.\n3. **Evaluation metrics**: accuracy / exact match; optional LLM-judge preference for output quality.\n4. **Code to adapt/reuse**: Self-Refine for critique/refinement loop; Reflexion for reflection memory baselines.\nEOF'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_104","type":"reasoning","text":"**Creating completion marker with timestamp**\n\nI’ll create the `.resource_finder_complete` file containing the current timestamp in ISO 8601 format after verifying dataset, paper, and repo counts."}}
{"type":"item.started","item":{"id":"item_105","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_105","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/fix-lazy-llms-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_106","type":"command_execution","command":"/bin/bash -lc \"cat > .resource_finder_complete << 'EOF'\nResource finding phase completed successfully.\nTimestamp: \"'$(date -Iseconds)\nPapers downloaded: 8\nDatasets downloaded: 2\nRepositories cloned: 2\nEOF'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_106","type":"command_execution","command":"/bin/bash -lc \"cat > .resource_finder_complete << 'EOF'\nResource finding phase completed successfully.\nTimestamp: \"'$(date -Iseconds)\nPapers downloaded: 8\nDatasets downloaded: 2\nRepositories cloned: 2\nEOF'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_107","type":"reasoning","text":"**Fixing timestamp expansion in EOF**\n\nThe script currently uses single-quoted EOF, preventing timestamp expansion; switching to unquoted EOF will ensure the actual timestamp is written instead of the literal string. I'll update the EOF usage accordingly and rerun."}}
{"type":"item.started","item":{"id":"item_108","type":"command_execution","command":"/bin/bash -lc 'cat > .resource_finder_complete << EOF\nResource finding phase completed successfully.\nTimestamp: $(date -Iseconds)\nPapers downloaded: 8\nDatasets downloaded: 2\nRepositories cloned: 2\nEOF'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_108","type":"command_execution","command":"/bin/bash -lc 'cat > .resource_finder_complete << EOF\nResource finding phase completed successfully.\nTimestamp: $(date -Iseconds)\nPapers downloaded: 8\nDatasets downloaded: 2\nRepositories cloned: 2\nEOF'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_109","type":"reasoning","text":"**Summarizing final changes and next steps**\n\nThe environment was set up with a virtualenv, pyproject, dependencies installed, and datasets for gsm8k and ai2_arc prepared with samples. Papers were downloaded, chunked, and text extracted, repos cloned, docs and completion markers created, and a .gitignore was added. The strategyqa resource failed and should be noted in resources.md. The uv add build error happened but was circumvented by pip install, and future uv add issues may arise due to package metadata; this will be mentioned. No web.run calls occurred, so no citations are needed. Next is providing the final summary and instructions without citations."}}
{"type":"item.completed","item":{"id":"item_110","type":"reasoning","text":"**Planning final user instructions**\n\nI'll suggest next steps like running Self-Refine GSM8K scripts or evaluation, noting no tests were run. I'll mention the paper-finder service was unavailable and manual search was used, plus that strategyqa download failed and was replaced with ARC. The response will include a quick explanation, details with paths, and a simple numeric list of next steps without heavy formatting."}}
{"type":"item.completed","item":{"id":"item_111","type":"agent_message","text":"Pulled together the resource bundle for “Fixing Lazy LLMs”: created the isolated env + deps, downloaded papers, cloned core repos, fetched datasets + samples, and wrote the review/docs and completion marker.\n\n- Environment: created `.venv`, added `pyproject.toml`, installed `httpx`, `pypdf`, `datasets`, `pandas` (via `uv pip` because `uv add` failed to build this repo package).\n- Papers: downloaded 8 PDFs into `papers/` and added `papers/README.md`; chunked/parsed two key papers under `papers/pages_lazy/` and `papers/pages_self_refine/`.\n- Datasets: downloaded GSM8K and ARC-Challenge into `datasets/`, added samples, and documented in `datasets/README.md` with `.gitignore`.\n- Code: cloned `self-refine` and `reflexion` into `code/` and documented in `code/README.md`.\n- Docs: wrote `literature_review.md`, `resources.md`, and completion marker `.resource_finder_complete`.\n\nNo tests run.\n\nNext steps (pick one):\n1) Run a quick Self-Refine GSM8K demo from `code/self-refine` to validate the workflow.  \n2) Add another dataset (e.g., StrategyQA) if you want broader reasoning coverage."}}
{"type":"turn.completed","usage":{"input_tokens":1610980,"cached_input_tokens":1463552,"output_tokens":18087}}
