\begin{thebibliography}{8}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe,
  Alon, Dziri, Prabhumoye, Yang, Gupta, Majumder, Hermann, Welleck,
  Yazdanbakhsh, and Clark]{madaan2023selfrefine}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
  Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank
  Gupta, Bodhisattwa~Prasad Majumder, Katherine Hermann, Sean Welleck, Amir
  Yazdanbakhsh, and Peter Clark.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{arXiv preprint arXiv:2303.17651}, 2023.

\bibitem[Shinn et~al.(2023)Shinn, Cassano, Berman, Gopinath, Narasimhan, and
  Yao]{shinn2023reflexion}
Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik
  Narasimhan, and Shunyu Yao.
\newblock Reflexion: Language agents with verbal reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2303.11366}, 2023.

\bibitem[Tang et~al.(2023)Tang, Kong, Huang, and Xue]{tang2023lazy}
Ruixiang Tang, Dehan Kong, Longtao Huang, and Hui Xue.
\newblock Large language models can be lazy learners: Analyze shortcuts in
  in-context learning.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL 2023}, pages 4645--4657, 2023.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Wang, Mi, Xue, Chen, Wong, and
  Xu]{wang2023dualcritique}
Rui Wang, Hongru Wang, Fei Mi, Boyang Xue, Yi~Chen, Kam-Fai Wong, and Ruifeng
  Xu.
\newblock Enhancing large language models against inductive instructions with
  dual-critique prompting.
\newblock \emph{arXiv preprint arXiv:2305.13733}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Wei, Schuurmans, Le, Chi, Narang,
  Chowdhery, and Zhou]{wang2022selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc~V. Le, Ed~H. Chi, Sharan Narang,
  Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023{\natexlab{b}}.
\newblock arXiv:2203.11171.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le,
  and Zhou]{wei2022cot}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,
  Ed~H. Chi, Quoc~V. Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{arXiv preprint arXiv:2201.11903}, 2022.

\bibitem[Xie et~al.(2023)Xie, Kawaguchi, Zhao, Zhao, Kan, He, and
  Xie]{xie2023selfeval}
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James~Xu Zhao, Min-Yen Kan, Junxian He,
  and Michael~Qizhe Xie.
\newblock Self-evaluation guided beam search for reasoning.
\newblock \emph{arXiv preprint arXiv:2305.00633}, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Sch\"arli, Hou, Wei, Scales, Wang, Schuurmans,
  Cui, Bousquet, Le, and Chi]{zhou2022least}
Denny Zhou, Nathanael Sch\"arli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi Wang,
  Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc~V. Le, and Ed~H. Chi.
\newblock Least-to-most prompting enables complex reasoning in large language
  models.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.
\newblock arXiv:2205.10625.

\end{thebibliography}
