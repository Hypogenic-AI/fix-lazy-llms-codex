\begin{abstract}
Large language models (LLMs) often produce short, shallow answers on reasoning tasks, and prompting is the cheapest lever to reduce this behavior. We examine whether harsher self-critique prompts and response-budget control improve reasoning accuracy compared to standard prompts. Using \gsm and \arc test splits, we evaluate six prompt conditions on a fixed \gptfourone model with deterministic decoding and matched examples (n=50 per dataset). We measure accuracy, response length, and paired bootstrap confidence intervals versus a direct baseline. \cotprompt improves \gsm accuracy from 0.52 to 0.74 (+0.22, 95\% CI [0.08, 0.36]) and slightly improves \arc (0.88 to 0.90, +0.02, CI [-0.04, 0.08]). In contrast, \harshcritic reduces accuracy on both datasets (\gsm: 0.40; \arc: 0.32), and \harshbudget performs worst (\gsm: 0.14; \arc: 0.24). Rude and polite framing yield near-zero effects. These results show that harsh self-critique with budget control is not a reliable fix for lazy outputs; standard \cotprompt remains the most robust low-effort improvement in this setting.
\end{abstract}
